<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://sarthakv7.github.io/my_folio/feed.xml" rel="self" type="application/atom+xml" /><link href="https://sarthakv7.github.io/my_folio/" rel="alternate" type="text/html" /><updated>2023-12-20T10:34:58+00:00</updated><id>https://sarthakv7.github.io/my_folio/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Transformers (State-of-the-art Natural Language Processing).</title><link href="https://sarthakv7.github.io/my_folio/blog/2020/Transformers/" rel="alternate" type="text/html" title="Transformers (State-of-the-art Natural Language Processing)." /><published>2020-08-01T11:10:16+00:00</published><updated>2020-08-01T11:10:16+00:00</updated><id>https://sarthakv7.github.io/my_folio/blog/2020/Transformers</id><content type="html" xml:base="https://sarthakv7.github.io/my_folio/blog/2020/Transformers/"><![CDATA[<h4 id="part-13-of-transformers-vs-google-quest-qa-labeling-kaggle-top-5">Part 1/3 of Transformers vs Google QUEST Q&amp;A Labeling (Kaggle top 5%).</h4>

<p><img src="" alt="" /></p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2560/1*EDjB0L5LW9dc-7eAKjMCdA.jpeg" />
    </div>
</div>

<p><img src="" alt="" />
<em>This is a 3 part series where we will be going through Transformers, BERT, and a hands-on Kaggle challenge — <a href="https://www.kaggle.com/c/google-quest-challenge/">Google QUEST Q&amp;A Labeling</a> to see Transformers in action (top 4.4% on the leaderboard).
In this part (1/3) we will be looking at how Transformers became state-of-the-art in various modern natural language processing tasks and their working.</em></p>

<p>The <strong>Transformer</strong> is a deep learning model proposed in the paper <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> by researchers at Google and the University of Toronto in 2017, used primarily in the field of natural language processing (NLP).</p>

<p>Like recurrent neural networks (RNNs), Transformers are designed to handle sequential data, such as natural language, for tasks such as translation and text summarization. However, unlike RNNs, Transformers do not require that the sequential data be processed in the order. For example, if the input data is a natural language sentence, the Transformer does not need to process the beginning of it before the end. Due to this feature, the Transformer allows for much more parallelization than RNNs and therefore reduced training times.</p>

<p>Transformers were designed around the concept of attention mechanism which was designed to help memorize long source sentences in neural machine translation.</p>

<p>Sounds cool right?
Let’s take a look under the hood and see how things work.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3624/1*woSNC3z6SMv9E3FFkwfb2Q.png" />
    </div>
</div>

<p><img src="" alt="" />
Transformers are based on an encoder-decoder architecture that comprises of <strong>encoders</strong> which consists of a set of encoding layers that processes the input iteratively one layer after another and <strong>decoders</strong> that consists of a set of decoding layers that does the same thing to the output of the encoder.</p>

<p>So, when we pass a sentence into a transformer, it is embedded and passed into a stack of encoders. The output from the final encoder is then passed into each decoder block in the decoder stack. The decoder stack then generates the output.</p>

<p>All the encoder blocks in the transformer are identical and similarly, all the decoder blocks in the transformer are identical.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3008/1*HAArsaBKNQ0Sbof5X4e70w.png" />
    </div>
</div>

<p><img src="" alt="" />
This was a very high-level representation of a transformer and it wouldn’t probably make much sense when understanding how transformers are so efficient in modern NLP tasks.
Don’t worry, to make things clearer, we will go through the internals of an encoder and decoder cell now…</p>

<p><img src="" alt="" /></p>
<blockquote>
  <h1 id="encoder">Encoder</h1>
</blockquote>

<p>The encoder has 2 parts, self-attention, and a feed-forward neural network.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2964/1*aWB4kG6TWQhHan4Q7UgfCA.png" />
    </div>
</div>

<p><img src="" alt="" />
The encoder’s inputs first flow through a self-attention layer — a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. Basically for each input word ‘x’ the self-attention layer generates a vector <strong>Z</strong> such that it takes all the input words (x1, x2, x3, …, xn) into the picture before generating <strong>Z</strong>. I’ll come to why it takes all the input word’s embedding into the picture and how it generates <strong>Z</strong> later in this blog but for now, just remember these brief high-level summarizations of the subcomponents of an encoder.</p>

<p>The outputs of the self-attention layer are fed to a feed-forward neural network. The feed-forward neural network generates an output for each input <strong>Z</strong> and the output from the feed-forward neural network is passed into the next encoder block’s self-attention layer and so on.</p>

<p>Now that we have an idea of what all is inside an encoder, let’s understand the tensor operations happening inside each component.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>First comes the input:</strong></p>
</blockquote>

<p>We know that transformers are used for NLP tasks so the data we deal with is usually a corpus of sentences, but since machine learning algorithms are all about matrix operations, we first need to convert the human-readable sentences into a machine-readable format (numbers).
To convert the sentences into numbers, we use ‘word embeddings’. This step is simple, each word in a sentence is represented as an n-dimensional vector (n is usually 512) and for transformers, we typically use GloVe embedding representation of words.
There is also something called <strong><em>positional encoding</em></strong> that is applied to these embedding but I’ll come to it later.
Once we have the embedding for each input word, we pass these embedding simultaneously to the self-attention layer.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>The training parameters of self-attention layer:</strong></p>
</blockquote>

<p>Different layers have different learning parameters eg. a <em>Dense</em> layer has <em>weights</em> and <em>bias</em>, a <em>Convolutional</em> layer has <em>kernels</em> as the learning parameters similarly in the self-attention layer, we have 4 learning parameters:</p>
<ul>
  <li>Query matrix: <strong>Wq</strong></li>
  <li>Key matrix: <strong>Wk</strong></li>
  <li>Value matrix: <strong>Wv</strong></li>
  <li>Output matrix: <strong>Wo</strong> (this is not the output matrix but a trainable parameter that generates the final output of the self-attention layer <strong>Z</strong>).</li>
</ul>

<p>The first 3 trainable parameters have a special purpose, they are used for generating 3 new parameters:</p>
<ul>
  <li>Query: <strong>Q</strong></li>
  <li>Key: <strong>K</strong></li>
  <li>Value: <strong>V</strong>
which are later used for generating output <strong>Z</strong> from input <strong>x</strong>, let’s see how-</li>
</ul>

<p>Some points to keep in mind are:</p>
<ul>
  <li>The input tensor <strong>x</strong> has <em>n-rows</em> and <em>m-columns</em> where <em>n</em> is the number of input words and <em>m</em> is the vector size of each word i.e. 512.</li>
  <li>The output tensors <strong><em>Q, K, V,</em></strong> and <strong><em>Z</em></strong> have <em>n-rows</em> and <em>dk-columns</em> where n is the number of input words and <em>dk</em> is 64. The values of <em>m</em> and <em>dk</em> are no random values but were found to work the best by researchers who came up with this architecture.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*MU1Gq47CuU-pxzPjSqPLNQ.png" />
    </div>
</div>

<p>After calculating the 3 parameters <em>Q, K, V</em> as mentioned above, the self-attention layer then calculates <strong><em>scores,</em></strong> a vector for each of the input words.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Dot-product attention:</strong></p>
</blockquote>

<p>The next step in the self-attention layer is to calculate the value of the vector score corresponding to each input word. This score calculation is one of the most crucial steps that bring the attention mechanism to life (well… not literally). The vector <strong><em>score</em></strong> has a size of <strong><em>n</em></strong> where <strong><em>n</em></strong> is the number of input words and each element of this vector is a number that tells how much does the word that it corresponds to contributes to the current word.
Let’s consider an example to get the intuition-
<strong>“The animal didn’t cross the street because it was too tired”</strong>
In the above sentence, the word <strong><em>it</em></strong> refers to the animal and not the road. For us, this is pretty simple to grasp but not for a machine with no attention, because we know how grammar works and we’ve developed a sense that it will be referring to <strong><em>animal</em></strong> more than words like <strong><em>cross</em></strong> or <strong><em>road.</em></strong>
This sense of grammar comes to transformers after training but the fact that for a given word, it considers all the words in the input and then has the ability to select the one that it thinks contributes the most is what the attention mechanism is about.
For the above sentence, the score vector generated for the word <strong><em>it</em></strong> will have 11 numbers, each corresponding to a word in the input sentence. For a well-trained model, this score vector will have larger numbers at positions 2 and 8 because the words at 2(animal) and 8(it) contribute the most to <strong><em>it.</em></strong> It may look something like:
[2, <strong>60</strong>, 4, 5, 3, 8, 5, <strong>90</strong>, 7, 6, 3]
Notice that the values at positions 2 and 8 are greater than the values at other positions.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2972/1*RyHXeWIxrIKgfTOMnhpCGw.png" />
    </div>
</div>

<p><img src="" alt="" />
Let’s see how these scores are generated in the self-attention layer.
Till now, for each word, we have <strong><em>Q, K, V</em></strong> vectors. To generate the score vector, we use something called the <em>dot-product attention</em> where we take a dot product between the <strong><em>Q</em></strong> and the <strong><em>K</em></strong> vectors to generate the score. The value of <strong><em>Q</em></strong> is corresponding to the query of the word for which we are calculating the score, in the above example, the word was <strong><em>it</em></strong> whereas there are <strong><em>n</em></strong> values of <strong><em>K,</em></strong> each corresponding to the key vector of the input words.
So, if we want to generate the scores for the word it:</p>

<ol>
  <li>
    <p>We take the query vector of it: <strong><em>Q</em></strong></p>
  </li>
  <li>
    <p>We take the key vectors of the input sentences: <strong><em>K1, K2, K3, …, Kn.</em></strong></p>
  </li>
  <li>
    <p>We take a dot product between <strong><em>Q</em></strong> and <strong><em>K</em></strong>’s and obtain <strong><em>n</em></strong> scores.</p>
  </li>
</ol>

<p>After calculating the scores, we kind of <strong>normalize</strong> the scores by dividing them by squared root of (<strong><em>dk</em></strong>) which was the column-dimension of vectors <strong><em>Q, K, V.</em></strong>
This step was mandatory because the creators of the transformer found that normalizing the scores by sqrt. of <strong><em>dk</em></strong> gives better results.</p>

<p>After normalizing the score vectors, we encode them using <strong>softmax function</strong> such that the output is proportional to the original scores but all the values sum up to 1.</p>

<p>Once we have the ‘softmaxed’ scores ready, we simply multiply each <strong>score</strong> element with the value vector <strong><em>V</em></strong> corresponding to it, such that we get <strong>n</strong> value vectors <strong><em>V</em></strong> after this operation: [<strong><em>V1, V2, V3, …, Vn</em></strong>].
Now to obtain the output <strong><em>Z</em></strong> of the self-attention, we simply add all the n value vectors.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2848/1*ajzNAllhsgqYL9wELeqFwA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2592/1*yto49xFf4eForVb3BVcx0w.png" />
    </div>
</div>

<p>The above diagrams illustrate the steps of the self-attention layer.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Multi-head Attention:</strong></p>
</blockquote>

<p>Now that we know how an attention-head works, and how amazing it is there is a catch to it. A single attention-head can sometimes miss some of the words in input that contribute most to the spotlight word, like in the example before, sometimes the attention head may fail to pay attention to the word <strong>animal</strong> while predicting the word <strong>it</strong> and this may cause problems.
To tackle this issue, instead of just a single attention-head, we use multiple attention-heads, each working in a similar manner. This idea helps us to reduce the error or miscalculation by any single attention head.
This is also referred to as <strong><em>multi-head attention</em></strong>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2584/1*eDQ0m0hIxIArPQ34Y6D8bg.png" />
    </div>
</div>

<p><img src="" alt="" />
In the transformers, multi-head attention typically uses 8 attention heads.
Now notice that the output of a single attention-head was of 64 dimensions, but if we use multi-head attention, we will get 8 such 64-dimensional vectors as output.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3288/1*iaN2Ua6B0vT894J-gPl-ZA.png" />
    </div>
</div>

<p><img src="" alt="" />
Turns out there is a final trainable parameter Output matrix <strong>Wo</strong> that I mentioned before that comes into play here.
In the final layer of the self-attention, all the output <strong>[Z0, Z1, Z2,…, Z7]</strong> are concatenated and multiplied with <strong><em>Wo</em></strong> such that the final output <strong><em>Z</em></strong> is of a dimension 64.</p>

<p>Below is the diagram to show all the steps discussed above:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3704/1*7pP54xnYsDIo9AAtitLFNw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Positional encoding:</strong></p>
</blockquote>

<p>Remember in <em>first comes the input</em> section I mentioned positional encoding, let’s see what are they and how they help. The problem with our current awesome transformer is that it does not take the position of the input words into account. Unlike RNN where we had timesteps to denote which word comes before and after, in transformers since the words are fed simultaneously, we need some kind of positional encoding that defines which word comes after which.
Positional encoding comes to our rescue as it gives the input embedding a sense of position, we first generate the position embeddings for each of the input words and these position embeddings are then added to the word embeddings of the respective words to generate <em>embeddings with a time signal.</em></p>

<p>There were many proposed method for generating the positional embeddings like one-hot encoded vectors or binary encoding but what the researchers found to work the best was using the equations below to generate the embeddings:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2932/1*NzB_4u7XDl6zi9SBRufJTA.png" />
    </div>
</div>

<p><img src="" alt="" />
When we plot the <em>128-dimensional positional encoding for a sentence with a maximum length of 50, it looks something like:</em></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3044/1*9fs9HBQWiNH2H54UGR-cIA.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Residual connections:</strong></p>
</blockquote>

<p>Finally, there is one more improvisation added to the encoders known as residual connections or skip connections which allow the output from the previous layer to bypass layers in between.
It helps in deep networks where there are many hidden layers and if any layer in between is not of much use or is not learning much, skip connections help in bypassing that layer.
Another thing to note is that when the residual connections are added and the resultant is normalized.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4332/1*C56hUkVSWamz2J7BeGGxeQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<blockquote>
  <h1 id="decoder">Decoder</h1>
</blockquote>

<p>A decoder is very similar to the encoder. Like encoder, it also has the self-attention and feed-forward network but it also has an additional block known as <em>Encoder-Decoder Attention</em> sandwiched between the two.
The <em>Encoder-Decoder Attention</em> layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.
The remaining 2 layers work exactly the same as those in the encoder cell.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3452/1*l4iIupatrLqKgaJR7BgdBg.png" />
    </div>
</div>

<p><img src="" alt="" />
The input to the decoder stack is sequential unlike the simultaneous input in encoder stack, meaning the first output word is passed into the decoder as an input using which it generates the second output now this output is again passed as an input to the decoder and using that it generates the third output and so on…</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2876/1*8jTqC2HMbBVVgVM63UOWyQ.gif" />
    </div>
</div>

<p><img src="" alt="" />
The output of the decoders is passed into a linear layer with softmax activation using which, the correct word is predicted.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4660/1*ERDmCo4ihoB7Vntth88DOw.png" />
    </div>
</div>

<p><img src="" alt="" />
Once the transformer predicts a word using forward propagation, the prediction is compared with the actual label using a loss function like cross-entropy and then all the trainable parameters are updated using back-propagation.
Well, this is one simplified way of understanding how learning happens in transformers. There are more variations like taking the complete output sentence for calculating the loss. To know more you can check out <a href="http://jalammar.github.io/illustrated-transformer/">this amazing blog</a> on Transformer by Jay Alammar.</p>

<p><em>With this, we have come to the end of this blog. Hope the read was pleasant.
I would like to thank all the creators for creating the awesome content I referred to for writing this blog.</em></p>

<p><em>Reference links:</em></p>

<ul>
  <li>
    <p><a href="https://www.appliedaicourse.com/">Applied AI Course.</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>
  </li>
  <li>
    <p><a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a></p>
  </li>
  <li>
    <p><a href="http://primo.ai/index.php?title=Transformer">http://primo.ai/index.php?title=Transformer</a></p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)</a></p>
  </li>
  <li>
    <p><a href="https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04">https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04</a></p>
  </li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Final note</strong></p>
</blockquote>

<p>Thank you for reading the blog. I hope it was useful for some of you aspiring to do projects or learn some new concepts in NLP.</p>

<p>In <a href="https://towardsdatascience.com/understanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef?source=friends_link&amp;sk=f48ce58edfdf395fe5d86436d8102a61">part 2/3</a> we will go through BERT (Bidirectional Encoder Representations from Transformers).</p>

<p>In <a href="https://towardsdatascience.com/hands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb?source=friends_link&amp;sk=159fd259d7ae346bb4cfa07dc5180938">part 3/3</a> we will go through a hands-on Kaggle challenge — <a href="https://www.kaggle.com/c/google-quest-challenge/">Google QUEST Q&amp;A Labeling</a> to see Transformers in action (top 4.4% on the leaderboard).</p>

<p>Find me on LinkedIn: <a href="http://www.linkedin.com/in/sarthak-vajpayee">www.linkedin.com/in/sarthak-vajpayee</a></p>

<p>Peace! ☮</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Part 1/3 of Transformers vs Google QUEST Q&A Labeling (Kaggle top 5%).]]></summary></entry><entry><title type="html">Understanding BERT — (Bidirectional Encoder Representations from Transformers).</title><link href="https://sarthakv7.github.io/my_folio/blog/2020/Understanding-BERT/" rel="alternate" type="text/html" title="Understanding BERT — (Bidirectional Encoder Representations from Transformers)." /><published>2020-08-01T11:10:16+00:00</published><updated>2020-08-01T11:10:16+00:00</updated><id>https://sarthakv7.github.io/my_folio/blog/2020/%20Understanding%20BERT</id><content type="html" xml:base="https://sarthakv7.github.io/my_folio/blog/2020/Understanding-BERT/"><![CDATA[<h4 id="part-23-of-transformers-vs-google-quest-qa-labeling-kaggle-top-5">Part 2/3 of Transformers vs Google QUEST Q&amp;A Labeling (Kaggle top 5%).</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2058/1*i4CrNfG02A_sBFdU7MWbdg.gif" />
    </div>
</div>

<p><img src="" alt="" />
<em>This is a 3 part series where we will be going through Transformers, BERT, and a hands-on Kaggle challenge — <a href="https://www.kaggle.com/c/google-quest-challenge/">Google QUEST Q&amp;A Labeling</a> to see Transformers in action (top 4.4% on the leaderboard).
In this part (2/3) we will be looking at BERT (Bidirectional Encoder Representations from Transformers) *and how it *became state-of-the-art in various modern natural language processing tasks.
Since the architecture of BERT is based on Transformers, you might want to check the internals of a Transformer which you can find in Part 1/3.</em></p>

<p>We are aware of the fact that how transfer learning has revolutionized the field of computer vision in the past few years. Pretrained networks like VGG, YOLO, UNET, RESNET, and many more have shown a groundbreaking performance in different areas of computer vision.
We have also seen a similar approach in the field of NLP like Word2Vector, GloVe but BERT takes it to a whole another level.</p>

<p>BERT was a <a href="https://arxiv.org/pdf/1810.04805.pdf">paper</a> published by researchers at Google AI Language in late 2018 and since then it has become a state of the art in many NLP tasks. BERT takes a different approach, it considers all the words of the input sentence simultaneously and then uses an attention mechanism to develop a contextual meaning of the words.
This approach works well for many NLP tasks as shown in the ELMo (Embeddings from Language Models) <a href="https://arxiv.org/pdf/1802.05365.pdf">paper</a> recently.</p>

<p>In this blog, we will cover BERT by going through 4 sub-topics-</p>

<ul>
  <li>
    <p><strong><em>Difference between BERT and previous embedding techniques.</em></strong></p>
  </li>
  <li>
    <p><strong><em>Taking a look under the hood:</em></strong> The internal architecture of BERT.</p>
  </li>
  <li>
    <p><strong><em>How the pre-trained BERT was trained:</em></strong> Different training methods that made BERT so efficient.</p>
  </li>
  <li>
    <p><strong><em>The input and output:</em></strong> How to use a BERT</p>
  </li>
</ul>

<p><strong><em>How is BERT different from other embedding generating algorithms like Word2Vector or GloVe?</em></strong></p>

<p>The main differences between BERT and W2V or GloVe are:</p>

<ol>
  <li>
    <p>W2V and GloVe word embeddings are context-independent. These models output just one vector (embedding) for each word, <em>combining all the different senses of the word into one vector</em>. For example in a given sentence:
“the game will lead to a <strong>tie</strong> if both the guys <strong>tie</strong> their final <strong>tie</strong> at the same time.”
Word2Vector or GloVe will fail to capture that all the 3 words <strong>tie</strong> in the sentence have different meanings and would simply return the same embedding for all the 3 words.
Whereas BERT is context-dependent, which means each of the 3 words would have different embeddings because BERT pays attention to the neighboring words before generating the embeddings.</p>
  </li>
  <li>
    <p>Because W2V and GloVe are context-independent, we do not require the model which was used to train the vectors every time to generate the embeddings. We can simply train the vectors on a corpus of words once and then generate a table or database holding the words and their respective trained vectors.
Whereas in the case of BERT, since it is context-dependent, we need the pre-trained model every time while generating the embeddings or performing any NLP task.</p>
  </li>
</ol>

<p>Now that we have got to know BERT a bit, let’s understand how it works.</p>
<blockquote>
  <p><strong>Taking a look under the hood</strong></p>
</blockquote>

<p>The architecture of BERT is derived from transformers. Inside a BERT there are several stacked encoder cells, similar to what we saw in transformers. Remember that inside a transformer how the encoder cells were used to read the input sentence and the decoder cells were used to predict the output sentence (word by word) but in the case of BERT, since we only need a model that reads the input sentence and generates some features that can be used for various NLP tasks, only the encoder part of the transformer is used.
The bi-directional part in BERT comes from the fact that it reads all the input words simultaneously.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2456/1*OUm4YbMOJHF-bL1m1cdndQ.png" />
    </div>
</div>

<p><img src="" alt="" />
Like I said before, the encoder cells are similar to what we saw in the transformer. There are <strong>self-attention heads</strong> and then a <strong>feed-forward neural network.</strong> The attention heads and feed-forward neural networks are also parameters that define different kinds of BERT models.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3924/1*B-Kd1JHDms479Id2uCW22A.png" />
    </div>
</div>

<p><img src="" alt="" />
If we look at BERT base and BERT large which are the two BERT architectures, both BERT base, and BERT large take an input of 512-dimensions.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2520/1*6anJjc_T-fjphXD-xe4JNA.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>BERT base — 12 layers (transformer blocks), 12 attention heads, 110 million parameters, and has an output size of 768-dimensions.</p>
  </li>
  <li>
    <p>BERT Large — 24 layers (transformer blocks), 16 attention heads, 340 million parameters, and has an output size of 1024-dimensions.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2652/0*BX2O8E-NW5hYdqAr.png" />
    </div>
</div>

<p><img src="" alt="" />
On <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD v1.1</a>, BERT achieves a 93.16% F1 score, surpassing even the human-level score of 91.2%: BERT also improves the state-of-the-art by 7.6% absolute on the very challenging GLUE benchmark, a set of 9 diverse Natural Language Understanding (NLU) tasks.</p>

<p>*The paper calls encoder cells as transformer blocks.</p>
<blockquote>
  <p><strong>Training the model</strong></p>
</blockquote>

<p>BERT is available as a pre-trained model. It was pre-trained on a large corpus of unlabelled text including the entire Wikipedia(that’s 2,500 million words) and book corpus (800 million words). Let’s see the 2 training methods that were used to train BERT.</p>

<ul>
  <li><strong>Masked Language Model (MLM):</strong>
In this approach, the model is fed with a sentence such that 15% of the words in the sentence are masked.
The challenge for BERT is to predict the masked words correctly given the context of unmasked words.
Some points to keep in mind are:</li>
  <li>The output from the final encoder block is not directly used for predictions, instead, a fully connected layer with GELU activation is added in between. The output from this layer is then converted to vocab and a softmax function is applied for predicting the masked word.</li>
  <li>The loss function only considers the predicted values for the masked words which make the learning more context-based.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2436/1*g44AA6blE3bn27vaWBjomQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li><strong>Next Sentence Prediction (NSP):</strong>
In this approach, the model is fed with 2 sentences.
The challenge for BERT is to predict the order of the 2 sentences.
For example, suppose the two sentences are:
<em>“I have a pen”</em> and <em>“The pen is red”</em>.
While training, BERT is expected to return 1 if the first sentence comes after the second sentence and 0 if the second sentence comes after the first sentence.</li>
</ul>

<p>While training a BERT model, both of the approaches discussed above are used simultaneously.</p>
<blockquote>
  <p><strong>The Input and Output</strong></p>
</blockquote>

<p><strong><em>Input:</em></strong></p>

<p>Having learned about the architecture and the training process of BERT, now let’s understand how to generate the output using BERT given some input text.</p>

<p><em>Special tokens:</em> There are some special tokens or keywords that are used while generating the input for BERT. The main ones are <strong>[CLS]</strong> and <strong>[SEP]</strong>.
<strong>[CLS]</strong> is used as a very first token added at the beginning of the input sentence. <strong>[SEP]</strong> is used as a separator between different sentences when multiple input sentences are passed.
Let’s consider an example: Suppose we want to pass the two sentences <em>“I have a pen”</em> and *“The pen is red” *to BERT. The tokenizer will first tokenize these sentences as:
<strong>[‘[CLS]’, ‘I’, ‘have’, ‘a’, ‘pen’, ‘[SEP]’, ‘the’, ‘pen’, ‘is’, ‘red’, ‘[SEP]’]</strong>
and then convert into numerical tokens.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2524/1*Pe1d17zhxNyV6gu9OHjdkQ.png" />
    </div>
</div>

<p><img src="" alt="" />
BERT takes 3 types of input:</p>

<ol>
  <li>
    <p><strong>*Token Embeddings</strong>: *The token embeddings are numerical representations of words in the input sentence. There is also something called sub-word tokenization that BERT uses to first breakdown larger or complex words into simple words and then convert them into tokens. For example, in the above diagram look how the word ‘playing’ was broken into ‘play’ and ‘##ing’ before generating the token embeddings. This tweak in tokenization works wonders as it utilized the sub-word context of a complex word instead of just treating it like a new word.</p>
  </li>
  <li>
    <p><strong><em>Segment Embeddings:</em></strong> The segment embeddings are used to help BERT distinguish between the different sentences in a single input. The elements of this embedding vector are all the same for the words from the same sentence and the value changes if the sentence is different.
Let’s consider an example: Suppose we want to pass the two sentences <em>“I have a pen”</em> and <em>“The pen is red”</em> to BERT. The tokenizer will first tokenize these sentences as:
<strong>[‘[CLS]’, ‘I’, ‘have’, ‘a’, ‘pen’, ‘[SEP]’, ‘the’, ‘pen’, ‘is’, ‘red’, ‘[SEP]’]</strong>
And the segment embeddings for these will look like:
<strong>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]</strong>
Notice how all the elements corresponding to the word in the first sentence have the same element <strong>0</strong> whereas all the elements corresponding to the word in the second sentence have the same element <strong>1</strong>.</p>
  </li>
  <li>
    <p><strong><em>Mask tokens:</em></strong> The mask tokens that help BERT to understand what all input words are relevant and what all are just there for padding.
Since BERT takes a 512-dimensional input, and suppose we have an input of 10 words only. To make the tokenized words compatible with the input size, we will add padding of size 512–10=502 at the end. Along with the padding, we will generate a mask token of size 512 in which the index corresponding to the relevant words will have <strong>1</strong>s and the index corresponding to padding will have <strong>0</strong>s.</p>
  </li>
  <li>
    <p><strong><em>Position Embeddings:</em></strong> Finally there are Position Embeddings that are generated internally in BERT and that provide the input data a sense of order. It is the same as what we discussed in Transformers.</p>
  </li>
</ol>

<p><strong><em>Output:</em></strong></p>

<p>Remember for each word in the input, the BERT base internally creates a 768-dimensional output but for tasks like classification, we do not actually require the output for all the embeddings. So by default, BERT considers only the output corresponding to the first token <strong>[CLS]</strong> and drops the output vectors corresponding to all the other tokens.</p>

<p>This works pretty neatly for classification tasks like spam detection where for a given an input text, we need to predict if it is spam or not.
We generate token_embeddings, segmentation_embeddings, and mask_tokens for the input sentence, pass them into BERT which generates a 768-dimensional output. Finally, we take this output and pass it into a feed-forward network like a Dense layer with 2 nodes and softmax as the activation function.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3068/1*mk3nnec9aatuEZCBq2O24g.png" />
    </div>
</div>

<p><img src="" alt="" />
The catch with the above output is that it does not work as good for tasks where we need to focus more on the semantic meaning of the sentence like machine translation.
For tasks like these, it is advised to use the pooled or averaged output of the hidden states of encoders. This is sometimes also referred to as <em>feature extraction</em>.</p>

<p>After much experimentation on which vector works best as a contextualized embedding, the paper mentions 6 choices.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3600/1*7EkcumT97b-VQzwZeaAKYg.png" />
    </div>
</div>

<p><img src="" alt="" />
Turns out that for most of the tasks, concatenation of the hidden states from the last four encoders seems to work the best.</p>

<p>To know more about the input parameters and the values returned my BERT you can check out the official documentation here: <a href="https://huggingface.co/transformers/model_doc/bert.html">https://huggingface.co/transformers/model_doc/bert.html</a></p>
<blockquote>
  <p><strong>Usability</strong></p>
</blockquote>

<p>Finally, let’s take a look at what all tasks BERT can perform as per the paper.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2772/1*VftUICOqy21xd6kRZY_k-w.png" />
    </div>
</div>

<p><img src="" alt="" />
(a). To classify a pair of sentences eg. a question-answer pair is relevant or not.</p>

<p>(b). To classify a single sentence eg. detect if the input sentence is a spam or not.</p>

<p>(c). To generate an answer to the given question title and paragraph.</p>

<p>(d). Single sentence tagging tasks such as named entity recognition, a tag must be predicted for every word in the input.</p>

<p><em>That was all to BERT in this blog, Hope the read was pleasant.
I would like to thank all the creators for creating the awesome content I referred to for writing this blog.</em></p>

<p><em>Reference links:</em></p>

<ul>
  <li>
    <p><a href="https://www.appliedaicourse.com/">Applied AI Course.</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>
  </li>
  <li>
    <p><a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270</a></p>
  </li>
  <li>
    <p><a href="http://jalammar.github.io/illustrated-bert/">http://jalammar.github.io/illustrated-bert/</a></p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=BhlOGGzC0Q0">https://www.youtube.com/watch?v=BhlOGGzC0Q0</a></p>
  </li>
  <li>
    <p><a href="https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad">https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad</a></p>
  </li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Final note</strong></p>
</blockquote>

<p>Thank you for reading the blog. I hope it was useful for some of you aspiring to do projects or learn some new concepts in NLP.</p>

<p>In <a href="https://towardsdatascience.com/transformers-state-of-the-art-natural-language-processing-1d84c4c7462b?source=friends_link&amp;sk=4ba3eb424ff59ce765c749819c6b5892">part 1/3</a> we covered how Transformers became state-of-the-art in various modern natural language processing tasks and their working.</p>

<p>In <a href="https://towardsdatascience.com/hands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb?source=friends_link&amp;sk=159fd259d7ae346bb4cfa07dc5180938">part 3/3</a> we will go through a hands-on Kaggle challenge — <a href="https://www.kaggle.com/c/google-quest-challenge/">Google QUEST Q&amp;A Labeling</a> to see Transformers in action (top 4.4% on the leaderboard).</p>

<p>Find me on LinkedIn: <a href="http://www.linkedin.com/in/sarthak-vajpayee">www.linkedin.com/in/sarthak-vajpayee</a></p>

<p>Peace! ☮</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Part 2/3 of Transformers vs Google QUEST Q&A Labeling (Kaggle top 5%).]]></summary></entry><entry><title type="html">Hands-on Transformers (Kaggle Google QUEST Q&amp;amp;A Labeling).</title><link href="https://sarthakv7.github.io/my_folio/blog/2020/Hands-on-Transformers/" rel="alternate" type="text/html" title="Hands-on Transformers (Kaggle Google QUEST Q&amp;amp;A Labeling)." /><published>2020-08-01T11:10:16+00:00</published><updated>2020-08-01T11:10:16+00:00</updated><id>https://sarthakv7.github.io/my_folio/blog/2020/Hands-on%20Transformers</id><content type="html" xml:base="https://sarthakv7.github.io/my_folio/blog/2020/Hands-on-Transformers/"><![CDATA[<h4 id="part-33-of-transformers-vs-google-quest-qa-labeling-kaggle-top-5">Part 3/3 of Transformers vs Google QUEST Q&amp;A Labeling (Kaggle top 5%).</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2400/1*hOohtF4J9u1updZhHNh9ow.jpeg" />
    </div>
</div>

<p><em>This is a 3 part series where we will be going through Transformers, BERT, and a hands-on Kaggle challenge — <a href="https://www.kaggle.com/c/google-quest-challenge/">Google QUEST Q&amp;A Labeling</a> to see Transformers in action (top 4.4% on the leaderboard).
In this part (3/3) we will be looking at a hands-on project from Google on Kaggle.
Since this is an NLP challenge, I’ve used transformers in this project. I have not covered transformers in much detail in this part but if you wish you could check out the part 1/3 of this series where I’ve discussed transformers in detail.</em></p>

<p><img src="" alt="" /></p>
<h2 id="birds-eye-view-of-the-blog">Bird’s eye view of the blog:</h2>

<p>To make the reading easy, I’ve divided the blog into different sub-topics-</p>

<ul>
  <li>
    <p>Problem statement and evaluation metrics.</p>
  </li>
  <li>
    <p>About the data.</p>
  </li>
  <li>
    <p>Exploratory Data Analysis (EDA).</p>
  </li>
  <li>
    <p>Modeling (includes data preprocessing).</p>
  </li>
  <li>
    <p>Post-modeling analysis.</p>
  </li>
</ul>

<p><img src="" alt="" /></p>
<h2 id="problem-statement-and-evaluation-metrics">Problem statement and Evaluation metrics:</h2>

<p>Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.</p>

<p>Humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context. Questions can take many forms — some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong.</p>

<p><img src="" alt="" /></p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2664/1*MKNxCI-qlr_YAhcKowSHkQ.png" />
    </div>
</div>

<p><img src="" alt="" />
Unfortunately, it’s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That’s why the <a href="https://crowdsource.google.com/">CrowdSource</a> team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.</p>

<p>In this competition, we’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a “common-sense” fashion. The raters received minimal guidance and training and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task.</p>

<p>Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&amp;A systems will get built, hopefully contributing to them becoming more human-like.</p>

<p><strong>Evaluation metric:</strong> Submissions are evaluated on the mean column-wise <a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient">Spearman’s correlation coefficient</a>. The Spearman’s rank correlation is computed for each target column, and the mean of these values is calculated for the submission score.</p>

<p><img src="" alt="" /></p>
<h2 id="about-the-data">About the data:</h2>

<p>The data for this competition includes questions and answers from various StackExchange properties. Our task is to predict the target values of 30 labels for each question-answer pair.
The list of 30 target labels is the same as the column names in the sample_submission.csv file. Target labels with the prefix question_ relate to the question_title and/or question_body features in the data. Target labels with the prefix answer_ relate to the answer feature.
Each row contains a single question and a single answer to that question, along with additional features. The training data contains rows with some duplicated questions (but with different answers). The test data does not contain any duplicated questions.
Target labels can have continuous values in the range [0,1]. Therefore, predictions must also be in that range.
The files provided are:</p>

<ul>
  <li>
    <p>train.csv — the training data (target labels are the last 30 columns)</p>
  </li>
  <li>
    <p>test.csv — the test set (you must predict 30 labels for each test set row)</p>
  </li>
  <li>
    <p>sample_submission.csv — a sample submission file in the correct format; column names are the 30 target labels</p>
  </li>
</ul>

<p>You can check out the dataset using <a href="https://www.kaggle.com/c/google-quest-challenge/data">this</a> link.</p>

<p><img src="" alt="" /></p>
<h2 id="exploratory-data-analysis-eda"><strong>Exploratory Data Analysis (EDA)</strong></h2>

<p><strong><em>Check-out the notebook with in-depth EDA + Data Scraping (<a href="https://www.kaggle.com/sarthakvajpayee/top-4-4-in-depth-eda-feature-scraping?scriptVersionId=40263047">Kaggle link</a>).</em></strong></p>

<p>The training data contains 6079 listings and each listing has 41 columns. Out of these 41 columns, the first 11 columns/features have to be used as the input and the last 30 columns/features are the target predictions.
Let’s take a look at the input and target labels:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2456/1*q5AIFvq5vvoWICyjktZtAg.png" />
    </div>
</div>

<p>The output features are all of the float types between 0 and 1.</p>

<p>Let’s explore the input labels one by one.</p>

<p><img src="" alt="" /></p>
<h3 id="qa_id">qa_id</h3>

<p>Question answer ID represents the id of a particular data point in the given dataset. Each data point has a unique qa_id. This feature is not to be used for training and will be used later while submitting the output to Kaggle.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/5036/1*AaPKUzHR6jKegZ7ku_INeg.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<h3 id="question_title">question_title</h3>

<p>This is a string data type feature that holds the title of the question asked.
For the analysis of question_title, I’ll be plotting a histogram of the number of words in this feature.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3448/1*Bio022v9rxBg8mhGhLNOjA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2760/1*h2VPrcBPqpLrTVOAXO8jMw.png" />
    </div>
</div>

<p>From the analysis, it is evident that:</p>
<ul>
  <li>Most of the question_title features have a word length of around 9.</li>
  <li>The minimum question length is 2.</li>
  <li>The maximum question length is 28.</li>
  <li>50% of question_title have lengths between 6 and 11.</li>
  <li>25% of question_title have lengths between 2 and 6.</li>
  <li>25% of question_title have lengths between 11 and 28.</li>
</ul>

<p><img src="" alt="" /></p>
<h3 id="question_body">question_body</h3>

<p>This is again a string data type feature that holds the detailed text of the question asked.
For the analysis of question_body, I’ll be plotting a histogram of the number of words in this feature.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3016/1*vck8a5DcxH6JSQIwvW-COA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2788/1*J_p2iMsZ8wLfxFlC7acrfQ.png" />
    </div>
</div>

<p><img src="" alt="" />
From the analysis, it is evident that:</p>
<ul>
  <li>Most of the question_body features have a word length of around 93.</li>
  <li>The minimum question length is 1.</li>
  <li>The maximum question length is 4666.</li>
  <li>50% of question_title have lengths between 55 and 165.</li>
  <li>25% of question_title have lengths between 1 and 55.</li>
  <li>25% of question_title have lengths between 165 and 4666.</li>
</ul>

<p>The distribution looks like a power-law distribution, it can be converted to a gaussian distribution using log and then used as an engineered feature.</p>

<p><img src="" alt="" /></p>
<h3 id="question_user_name">question_user_name</h3>

<p>This is a string data type feature that denotes the name of the user who asked the question.
For the analysis of question_answer, I’ll be plotting a histogram of the number of words in this feature.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3612/1*y2lMiIxsJae7PqDLCZLruw.png" />
    </div>
</div>

<p><img src="" alt="" />
I did not find this feature of much use therefore I won’t be using this for modeling.</p>

<p><img src="" alt="" /></p>
<h3 id="question_user_page">question_user_page</h3>

<p>This is a string data type feature that holds the URL to the profile page of the user who asked the question.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4196/1*AEIgkWtuReLYpi4vO1jczQ.png" />
    </div>
</div>

<p><img src="" alt="" />
On the profile page, I noticed 4 useful features that could be used and should possibly contribute to good predictions. The features are:</p>
<ul>
  <li>Reputation: Denotes the reputation of the user.</li>
  <li>gold_score: The number of gold medals awarded.</li>
  <li>silver_score: The number of silver medals awarded.</li>
  <li>bronze_score: The number of bronze medals awarded.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td><td class="code"><pre> <span class="c1"># code for scraping the data. Since all of the urls are of stackoverflow, they have the same html hie rarchy.
</span><span class="kn">from</span> <span class="n">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="k">def</span> <span class="nf">get_user_rating</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">get</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="nf">urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">).</span><span class="nf">read</span><span class="p">()</span>
    <span class="n">src</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">get</span><span class="p">,</span> <span class="sh">'</span><span class="s">html.parser</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">reputation</span><span class="p">,</span> <span class="n">gold</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">silver</span><span class="p">,</span> <span class="n">bronze</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">template</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">"</span><span class="s">div</span><span class="sh">"</span><span class="p">,</span> <span class="n">class_</span> <span class="o">=</span> <span class="sh">'</span><span class="s">grid--cell fl-shrink0 ws2 overflow-hidden</span><span class="sh">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">reputation</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">template</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">div</span><span class="sh">'</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span><span class="sh">'</span><span class="s">grid--cell fs-title fc-dark</span><span class="sh">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">)))</span>
    <span class="n">gold</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">template</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">div</span><span class="sh">'</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span><span class="sh">'</span><span class="s">grid ai-center s-badge s-badge__gold</span><span class="sh">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">)))</span>
    <span class="n">silver</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">template</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">div</span><span class="sh">'</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span><span class="sh">'</span><span class="s">grid ai-center s-badge s-badge__silver</span><span class="sh">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">)))</span>
    <span class="n">bronze</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">template</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">div</span><span class="sh">'</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span><span class="sh">'</span><span class="s">grid ai-center s-badge s-badge__bronze</span><span class="sh">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">)))</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="n">reputation</span><span class="p">,</span> <span class="n">gold</span><span class="p">,</span> <span class="n">silver</span><span class="p">,</span> <span class="n">bronze</span><span class="p">]</span>
  <span class="k">except</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span> <span class="c1"># return np.nan if the code runs into some error like page not found
</span>
  <span class="k">return</span> <span class="n">output</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="sh">'</span><span class="s">answer_user_page</span><span class="sh">'</span><span class="p">]):</span>
  <span class="n">data</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">get_user_rating</span><span class="p">(</span><span class="n">url</span><span class="p">))</span>
<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">reputation</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">gold</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">silver</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">bronze</span><span class="sh">'</span><span class="p">]</span>
<span class="n">scraped</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">lens</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="n">scraped</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">scraped_score.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p><img src="" alt="" /></p>
<h3 id="answer">answer</h3>

<p>This is again a string data type feature that holds the detailed text of the answer to the question.
For the analysis of <em>answer</em>, I’ll be plotting a histogram of the number of words in this feature.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3312/1*hoFuOPl3B9cNz1TSeHLzrA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2728/1*SPMSqpz78qsCvfOmyD5icg.png" />
    </div>
</div>

<p><img src="" alt="" />
From the analysis, it is evident that:</p>
<ul>
  <li>Most of the question_body features have a word length of around 143.</li>
  <li>The minimum question length is 2.</li>
  <li>The maximum question length is 8158.</li>
  <li>50% of question_title have lengths between 48 and 170.</li>
  <li>25% of question_title have lengths between 2 and 48.</li>
  <li>25% of question_title have lengths between 170 and 8158.</li>
</ul>

<p>This distribution also looks like a power-law distribution, it can also be converted to a gaussian distribution using log and then used as an engineered feature.</p>

<p><img src="" alt="" /></p>
<h3 id="answer_user_name">answer_user_name</h3>

<p>This is a string data type feature that denotes the name of the user who answered the question.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3024/1*8am6v-oUIg_XKA1d6LpzBQ.png" />
    </div>
</div>

<p><img src="" alt="" />
I did not find this feature of much use therefore I won’t be using this for modeling.</p>

<p><img src="" alt="" /></p>
<h3 id="answer_user_page">answer_user_page</h3>

<p>This is a string data type feature similar to the feature “question_user_page” that holds the URL to the profile page of the user who asked the question.</p>

<p>I also used the URL in this feature to scrape the external data from the user’s profile page, similar to what I did for the feature ‘question_user_page’.</p>

<p><img src="" alt="" /></p>
<h3 id="url">url</h3>

<p>This feature holds the URL of the question and answers page on StackExchange or StackOverflow. Below I’ve printed the first 10 <em>url</em> data-points from train.csv</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3656/1*0_RcKZAy01R7OxZ7BaVsCQ.png" />
    </div>
</div>

<p><img src="" alt="" />
One thing to notice is that this feature lands us on the question-answer page, and that page may usually contain a lot more data like comments, upvotes, other answers, etc. which can be used for generating more features if the model does not perform well due to fewer data in train.csv
Let’s see the data is present and what additional data can be scraped from the question-answer page.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3200/1*HikC8L5q8zm8GFasu4gM1Q.png" />
    </div>
</div>

<p><img src="" alt="" />
In the snapshot attached above, <em>Post 1</em> and <em>Post 2</em> contain the answers, upvotes, and comments for the question asked in decreasing order of upvotes. The post with a green tick is the one containing the answer provided in the train.csv file.</p>

<p>Each question may have more than one answer. We can scrape these answers and use them as additional data.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3096/1*f_qUPbojaSFLJ38d_LPzhA.png" />
    </div>
</div>

<p><img src="" alt="" />
The snapshot above defines the anatomy of a post. We can scrape useful features like <em>upvotes</em> and <em>comments</em> and use them as additional data.</p>

<p>Below is the code for scraping the data from the URL page.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre></td><td class="code"><pre><span class="c1"># Here is the code. Since all of the urls are of stackoverflow, they have the same html hierarchy.
</span>  <span class="k">def</span> <span class="nf">get_answers</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="n">get</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="nf">urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">).</span><span class="nf">read</span><span class="p">()</span>
      <span class="n">src</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">get</span><span class="p">,</span> <span class="sh">'</span><span class="s">html.parser</span><span class="sh">'</span><span class="p">)</span>
      <span class="n">upvotes</span><span class="p">,</span> <span class="n">posts</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
      <span class="n">correct_ans</span><span class="p">,</span> <span class="n">comments</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
      <span class="n">new_features</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">post_layout</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">"</span><span class="s">div</span><span class="sh">"</span><span class="p">,</span> <span class="n">class_</span> <span class="o">=</span> <span class="sh">'</span><span class="s">post-layout</span><span class="sh">'</span><span class="p">)</span>
      <span class="n">l</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">post_layout</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">post_layout</span><span class="p">[:</span><span class="n">l</span><span class="p">]:</span>
        <span class="n">posts</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">div</span><span class="sh">'</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span><span class="sh">'</span><span class="s">post-text</span><span class="sh">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span><span class="p">.</span><span class="nf">strip</span><span class="p">())</span>
        <span class="n">upvotes</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">"</span><span class="s">div</span><span class="sh">"</span><span class="p">,</span> <span class="n">class_</span> <span class="o">=</span> <span class="sh">'</span><span class="s">js-vote-count grid--cell fc-black-500 fs-title g rid fd-column ai-center</span><span class="sh">'</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">data-value</span><span class="sh">'</span><span class="p">)))</span>
        <span class="n">correct_ans</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">"</span><span class="s">div</span><span class="sh">"</span><span class="p">,</span> <span class="n">class_</span> <span class="o">=</span> <span class="sh">'</span><span class="s">js-accepted-answer-indicator grid--cell fc-g reen-500 ta-center py4</span><span class="sh">'</span><span class="p">)))</span>
        <span class="n">comments</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">i</span><span class="p">.</span><span class="n">text</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">p</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">span</span><span class="sh">'</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span><span class="sh">'</span><span class="s">comment-copy</span><span class="sh">'</span><span class="p">)]))</span>

      <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">correct_ans</span><span class="p">)</span>
      <span class="n">new_features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">upvotes</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>
      <span class="n">new_features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">comments</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>
      <span class="k">del</span> <span class="n">posts</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">l</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">k</span><span class="o">=</span><span class="n">l</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">k</span><span class="o">=</span><span class="mi">3</span>
      <span class="k">for</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">posts</span><span class="p">[:</span><span class="n">k</span><span class="p">],</span> <span class="n">comments</span><span class="p">[:</span><span class="n">k</span><span class="p">]):</span>
        <span class="n">new_features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">new_features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">posts</span><span class="p">[:</span><span class="mi">3</span><span class="o">-</span><span class="n">k</span><span class="p">],</span> <span class="n">comments</span><span class="p">[:</span><span class="mi">3</span><span class="o">-</span><span class="n">k</span><span class="p">]):</span>
        <span class="n">new_features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span>
        <span class="n">new_features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span>

      <span class="k">return</span> <span class="n">new_features</span>

    <span class="k">except</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">]</span><span class="o">*</span><span class="mi">8</span> <span class="c1"># return np.nan if the code runs into some error like page not found
</span>
<span class="c1"># collecting the data
</span><span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="sh">'</span><span class="s">url</span><span class="sh">'</span><span class="p">]):</span>
  <span class="n">data</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">get_answers</span><span class="p">(</span><span class="n">url</span><span class="p">))</span>

<span class="c1"># Saving as dataframe
</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">upvotes</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">comments_0</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">answer_1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">comment_1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">answer_2</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">comment_2</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">answer_3</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">comment_3</span><span class="sh">'</span><span class="p">]</span>
<span class="n">scraped</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">lens</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="n">scraped</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">scraped_posts.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>There are 8 new features that I’ve scraped-</p>
<ul>
  <li>upvotes: The number of upvotes on the provided answer.</li>
  <li>comments_0: Comments to the provided answer.</li>
  <li>answer_1: Most voted answer apart from the one provided.</li>
  <li>comment_1: Top comment to answer_1.</li>
  <li>answer_2: Second most voted answer.</li>
  <li>comment_2: Top comment to answer_2.</li>
  <li>answer_3: Third most voted answer.</li>
  <li>comment_3: Top comment to answer_3.</li>
</ul>

<p><img src="" alt="" /></p>
<h3 id="category">category</h3>

<p>This is a categorical feature that tells the categories of question and answers pairs. Below I’ve printed the first 10 <em>category</em> data-points from train.csv</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2724/1*ZVyREoM-KJIonAMNurlWZQ.png" />
    </div>
</div>

<p>Below is the code for plotting a Pie chart of category.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2976/1*sQeMNBXBG_9wjgL5RwMuKA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2740/1*raqsI9DfgAi-X9DEteDG-g.png" />
    </div>
</div>

<p><img src="" alt="" />
The chart tells us that most of the points belong to the category *TECHNOLOGY *and least belong to *LIFE_ARTS *(709 out of 6079).</p>

<p><img src="" alt="" /></p>
<h3 id="host">host</h3>

<p>This feature holds the host or domain of the question and answers page on StackExchange or StackOverflow. Below I’ve printed the first 10 <em>host</em> data-points from train.csv</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3052/1*Z2qQIIdk3tS1TTPUjbjesQ.png" />
    </div>
</div>

<p><img src="" alt="" />
Below is the code for plotting a bar graph of unique hosts.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2952/1*MEYLevz8Mb4jEDi45l3CHQ.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2720/1*AC4MzE30X4w-jOdZUKDWPw.png" />
    </div>
</div>

<p><img src="" alt="" />
It seems there are not many but just 63 different subdomains present in the training data. Most of the data points are from StackOverflow.com whereas least from meta.math.stackexchange.com</p>

<p><img src="" alt="" /></p>
<h3 id="target-values">Target values</h3>

<p>Let’s analyze the target values that we need to predict. But first, for the sake of a better interpretation, please check out the full dataset on kaggle using <a href="https://www.kaggle.com/c/google-quest-challenge/data?select=train.csv">this link</a>.</p>

<p>Below is the code block displaying the statistical description of the target values. These are only the first 6 features out of all the 30 features.
The values of all the features are of type float and are between 0 and 1.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4624/1*vIH-FKw4qnebXL4HnYJuSw.png" />
    </div>
</div>

<p><img src="" alt="" />
Notice the second code block which displays the unique values present in the dataset. There are just 25 unique values between 0 and 1. This could be useful later while fine-tuning the code.</p>

<p>Finally, let’s check the distribution of the target features and their correlation.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2740/1*K33iaHeCjBKwQiAE6aW2Yw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2850/1*a6rTLOBC2RuXYBDZExY9Hw.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2836/1*olpEN_FeEg1ViqLw_jJV7w.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2190/1*rJGjCI31Tu4yM3bi6vfeAw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<h2 id="modeling">Modeling</h2>

<p><img src="" alt="" /></p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2850/1*QdslcbSWBOdPju1zpqycgQ.jpeg" />
    </div>
</div>

<p><img src="" alt="" />
Now that we know our data better through EDA, let’s begin with modeling. Below are the subtopics that we’ll go through in this section-</p>

<ul>
  <li>
    <p><strong>Overview of the architecture:</strong> Quick rundown of the ensemble architecture and it’s different components.</p>
  </li>
  <li>
    <p><strong>Base learners:</strong> Overview of the base learners used in the ensemble.</p>
  </li>
  <li>
    <p><strong>Preparing the data:</strong> Data cleaning and preparation for modeling.</p>
  </li>
  <li>
    <p><strong>Ensembling:</strong> Creating models for training, and predicting. Pipelining the data preparation, model training, and model prediction steps.</p>
  </li>
  <li>
    <p><strong>Getting the scores from Kaggle:</strong> Submitting the predicted target values for test data on Kaggle and generating a leaderboard score to see how well the ensemble did.</p>
  </li>
</ul>

<p>I tried various deep neural network architectures with GRU, Conv1D, Dense layers, and with different features for the competition but, an ensemble of 8 transformers (as shown above) seems to work the best.
In this part, we will be focusing on the final architecture of the ensemble used and for the other baseline models that I experimented with, you can check out my github repo.</p>
<blockquote>
  <p><strong>Overview of the architecture:</strong></p>
</blockquote>

<p>Remember our task was for a given <strong><em>question_title, question_body,</em></strong> and <strong><em>answer</em></strong>, we had to predict 30 target labels.
Now out of these 30 target labels, the first 21 are related to the <strong><em>question_title</em></strong> and <strong><em>question_body</em></strong> and have no connection to the <strong><em>answer</em></strong> whereas the last 9 target labels are related to the <strong><em>answer</em></strong> only but out of these 9, some of them also take <strong><em>question_title</em></strong> and <strong><em>question_body</em></strong> into the picture.
Eg. features like <em>answer_relevance</em> and <em>answer_satisfaction</em> can only be rated by looking at both the question and answer.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2868/1*JsrdmWw2Jwa9uNN8c5LdNw.png" />
    </div>
</div>

<p><img src="" alt="" />
With some experimentation, I found that the base-learner (BERT_base) performs exceptionally well in predicting the first 21 target features (related to questions only) but does not perform that well in predicting the last 9 target features. Taking note of this, I constructed 3 dedicated base-learners and 2 different datasets to train them.</p>

<ol>
  <li>
    <p>The first base-learner was dedicated to predicting the question-related features (first 21) only. The dataset used for training this model consisted of features <strong><em>question_title</em></strong> and <strong><em>question_body</em></strong> only.</p>
  </li>
  <li>
    <p>The second base-learner was dedicated to predicting the answer-related features (last 9) only. The dataset used for training this model consisted of features <strong><em>question_title</em></strong>, <strong><em>question_body,</em></strong> and <strong><em>answer</em></strong>.</p>
  </li>
  <li>
    <p>The third base-learner was dedicated to predicting all the 30 features. The dataset used for training this model again consisted of features <strong><em>question_title</em></strong>, <strong><em>question_body,</em></strong> and <strong><em>answer</em></strong>.</p>
  </li>
</ol>

<p>To make the architecture even more robust, I used 3 different types of base learners — <strong>BERT, RoBERTa, and XLNet.</strong>
We will be going through these different transformer models later in this blog.</p>

<p>In the ensemble diagram above, we can see —</p>

<ul>
  <li>
    <p>The 2 datasets consisting of <strong>[question_title + question_body]</strong> and <strong>[question_title + question_body + answer]</strong> being used separately to train different base learners.</p>
  </li>
  <li>
    <p>Then we can see the 3 different base learners <strong>(BERT, RoBERTa, and XLNet)</strong> dedicated to predicting the <strong>question-related features only</strong> (first 21) colored in blue, using the dataset <strong>[question_title + question_body]</strong></p>
  </li>
  <li>
    <p>Next, we can see the 3 different base learners <strong>(BERT, RoBERTa, and XLNet)</strong> dedicated to predicting the <strong>answer-related features only</strong> (last 9) colored in green, using the dataset <strong>[question_title + question_body + answer].</strong></p>
  </li>
  <li>
    <p>Finally, we can see the 2 different base learners <strong>(BERT, and RoBERTa)</strong> dedicated to predicting <strong>all the 30 features</strong> colored in red, using the dataset <strong>[question_title + question_body + answer].</strong></p>
  </li>
</ul>

<p>In the next step, the predicted data from models dedicated to predicting the <strong>question-related features only</strong> (denoted as <strong><em>bert_pred_q, roberta_pred_q, xlnet_pred_q</em></strong>) and the predicted data from models dedicated to predicting the <strong>answer-related features only</strong> (denoted as <strong><em>bert_pred_a, roberta_pred_a, xlnet_pred_a</em></strong>) is collected and concatenated column-wise which leads to a predicted data with all the 30 features. These concatenated features are denoted as <strong><em>xlnet_concat, roberta_concat,</em></strong> and <strong><em>bert_concat.</em></strong></p>

<p>Similarly, the predicted data from models dedicated to predicting <strong>all the 30 features</strong> (denoted as <strong><em>bert_qa, roberta_qa</em></strong>) is collected. Notice that I’ve not used the XLNet model here for predicting all the 30 features because the scores were not up to the mark.</p>

<p>Finally, after collecting all the different predicted data — <strong>[xlnet_concat, roberta_concat, bert_concat, bert_qa, and roberta_qa],</strong> the final value is calculated by taking the average of all the different predicted values.</p>
<blockquote>
  <p><strong>Base learners</strong></p>
</blockquote>

<p>Now we will take a look at the 3 different transformer models that were used as base learners.</p>

<p><strong>1. bert_base_uncased:</strong></p>

<p><a href="https://arxiv.org/abs/1810.04805">Bert</a> was proposed by Google AI in late 2018 and since then it has become state-of-the-art for a wide spectrum of NLP tasks.
It uses an architecture derived from transformers pre-trained over a lot of unlabeled text data to learn a language representation that can be used to fine-tune for specific machine learning tasks. BERT outperformed the NLP state-of-the-art on several challenging tasks. This performance of BERT can be ascribed to the transformer’s encoder architecture, unconventional training methodology like the Masked Language Model (MLM), and Next Sentence Prediction (NSP) and the humungous amount of text data (all of Wikipedia and book corpus) that it is trained on. BERT comes in different sizes but for this challenge, I’ve used <em>bert_base_uncased.</em></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*nbFb82C1avPQB6aH.png" />
    </div>
</div>

<p>The architecture of <em>bert_base_uncased</em> consists of 12 encoder cells with 8 attention heads in each encoder cell.
It takes an input of size 512 and returns 2 values by default, the output corresponding to the first input token [CLS] which has a dimension of 786 and another output corresponding to all the 512 input tokens which have a dimension of (512, 768) aka pooled_output.
But apart from these, we can also access the hidden states returned by each of the 12 encoder cells by passing <strong><em>output_hidden_states=True</em></strong> as one of the parameters.
BERT accepts several sets of input, for this challenge, the input I’ll be using will be of 3 types:</p>

<ul>
  <li>
    <p><strong><em>input_ids</em></strong>: The token embeddings are numerical representations of words in the input sentence. There is also something called sub-word tokenization that BERT uses to first breakdown larger or complex words into simple words and then convert them into tokens. For example, in the above diagram look how the word ‘playing’ was broken into ‘play’ and ‘##ing’ before generating the token embeddings. This tweak in tokenization works wonders as it utilized the sub-word context of a complex word instead of just treating it like a new word.</p>
  </li>
  <li>
    <p><strong><em>attention_mask</em></strong>: The segment embeddings are used to help BERT distinguish between the different sentences in a single input. The elements of this embedding vector are all the same for the words from the same sentence and the value changes if the sentence is different.
Let’s consider an example: Suppose we want to pass the two sentences <em>“I have a pen”</em> and <em>“The pen is red”</em> to BERT. The tokenizer will first tokenize these sentences as:
<strong>[‘[CLS]’, ‘I’, ‘have’, ‘a’, ‘pen’, ‘[SEP]’, ‘the’, ‘pen’, ‘is’, ‘red’, ‘[SEP]’]</strong>
And the segment embeddings for these will look like:
<strong>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1].</strong>
Notice how all the elements corresponding to the word in the first sentence have the same element <strong>0</strong> whereas all the elements corresponding to the word in the second sentence have the same element <strong>1</strong>.</p>
  </li>
  <li>
    <p><strong><em>token_type_ids:</em></strong> The mask tokens that help BERT to understand what all input words are relevant and what all are just there for padding.
Since BERT takes a 512-dimensional input, and suppose we have an input of 10 words only. To make the tokenized words compatible with the input size, we will add padding of size 512–10=502 at the end. Along with the padding, we will generate a mask token of size 512 in which the index corresponding to the relevant words will have <strong>1</strong>s and the index corresponding to padding will have <strong>0</strong>s.</p>
  </li>
</ul>

<p><strong>2. XLNet_base_cased:</strong></p>

<p><a href="https://arxiv.org/abs/1906.08237">XLNet</a> was proposed by Google AI Brain team and researchers at CMU in mid-2019. Its architecture is larger than BERT and uses an improved methodology for training. It is trained on larger data and shows better performance than BERT in many language tasks. The conceptual difference between BERT and XLNet is that while training <strong>BERT</strong>, the words are predicted in an order such that the previous predicted word contributes to the prediction of the next word whereas, <strong>XLNet</strong> learns to predict the words in an arbitrary order but in an autoregressive manner (not necessarily left-to-right).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*V2CD2JYdmFiaPTIa" />
    </div>
</div>

<p><img src="" alt="" />
This helps the model to learn bidirectional relationships and therefore better handles dependencies and relations between words.
In addition to the training methodology, XLNet uses Transformer XL based architecture and 2 main key ideas: <em>relative positional embeddings</em> and the <em>recurrence mechanism</em> which showed good performance even in the absence of permutation-based training.
XLNet was trained with over 130 GB of textual data and 512 TPU chips running for 2.5 days, both of which are much larger than BERT.</p>

<p>For XLNet, I’ll be using only <strong>input_ids</strong> and <strong>attention_mask</strong> as input.</p>

<p><img src="" alt="" />
<strong>3. RoBERTa_base:</strong></p>

<p>RoBERTa was proposed by Facebook in mid-2019. It is a robustly optimized method for pretraining natural language processing (NLP) systems that improve on BERT’s self-supervised method.
RoBERTa builds on BERT’s language masking strategy, wherein the system learns to predict intentionally hidden sections of text within otherwise unannotated language examples. RoBERTa modifies key hyperparameters in BERT, including removing BERT’s Next Sentence Prediction (NSP) objective, and training with much larger mini-batches and learning rates. This allows RoBERTa to improve on the masked language modeling objective compared with BERT and leads to better downstream task performance. RoBERTa was also trained on more data than BERT and for a longer amount of time. The dataset used was from existing unannotated NLP data sets as well as CC-News, a novel set drawn from public news articles.</p>

<p>For RoBERTa_base, I’ll be using only <strong>input_ids</strong> and <strong>attention_mask</strong> as input.</p>

<p><strong><em>Finally here is the comparison of BERT, XLNet, and RoBERTa:</em></strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2436/1*lNiXASsDWI86aMKZihMC1Q.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://miro.medium.com/max/1050/0*EfEZgjlXlGl0sXjG.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Preparing the data</strong></p>
</blockquote>

<p>Now that we have gained some idea about the architecture let’s see how to prepare the data for the base learners.</p>

<ul>
  <li>As a preprocessing step, I have just treated the HTML syntax present in the features. I used html.unescape() to extract the text from HTML DOM elements.
In the code snippet below, the function <strong>get_data()</strong> reads the train and test data and applies the preprocessing to the features <strong><em>question_title, question_body,</em></strong> and <strong><em>answer.</em></strong></li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">get_data</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">getting test and train data...</span><span class="sh">'</span><span class="p">)</span>
    <span class="c1"># reading the data into dataframe using pandas
</span>    <span class="n">path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">../input/google-quest-challenge/</span><span class="sh">'</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">+</span><span class="sh">'</span><span class="s">train.csv</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">+</span><span class="sh">'</span><span class="s">test.csv</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">submission</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">+</span><span class="sh">'</span><span class="s">sample_submission.csv</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Selecting data for training and testing
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">train</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">11</span><span class="p">:]]</span> <span class="c1"># storing the target values in y
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">train</span><span class="p">[[</span><span class="sh">'</span><span class="s">question_title</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">question_body</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="p">]]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">test</span><span class="p">[[</span><span class="sh">'</span><span class="s">question_title</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">question_body</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="p">]]</span>

    <span class="c1"># Cleaning the data
</span>    <span class="n">X</span><span class="p">.</span><span class="n">question_body</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">question_body</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">html</span><span class="p">.</span><span class="n">unescape</span><span class="p">)</span>
    <span class="n">X</span><span class="p">.</span><span class="n">question_title</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">question_title</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">html</span><span class="p">.</span><span class="n">unescape</span><span class="p">)</span>
    <span class="n">X</span><span class="p">.</span><span class="n">answer</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">answer</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">html</span><span class="p">.</span><span class="n">unescape</span><span class="p">)</span>

    <span class="n">X_test</span><span class="p">.</span><span class="n">question_body</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="n">question_body</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">html</span><span class="p">.</span><span class="n">unescape</span><span class="p">)</span>
    <span class="n">X_test</span><span class="p">.</span><span class="n">question_title</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="n">question_title</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">html</span><span class="p">.</span><span class="n">unescape</span><span class="p">)</span>
    <span class="n">X_test</span><span class="p">.</span><span class="n">answer</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="n">answer</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">html</span><span class="p">.</span><span class="n">unescape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<ul>
  <li>The next step was to create <strong><em>input_ids, attention_masks,</em></strong> and <strong><em>token_type_ids</em></strong> from the input sentence.
In the code snippet below, the function <strong>get_tokenizer()</strong> collects pre-trained tokenizer for the different base_learners.
The second function <strong>fix_length()</strong> goes through the generated question tokens and answer tokens and makes sure that the maximum number of tokens is 512. The steps for fixing the number of tokens are as follows:</li>
  <li>If the input sentence has the number of tokens &gt; 512, the sentence is trimmed down to 512.</li>
  <li>To trim the number of tokens, 256 tokens from the beginning and 256 tokens from the end are kept and the remaining tokens are dropped.</li>
  <li>For example, suppose an answer has 700 tokens, to trim this down to 512, 256 tokens from the beginning are taken and 256 tokens from the end are taken and concatenated to make 512 tokens. The remaining [700-(256+256) = 288] tokens that are in the middle of the answer are dropped.</li>
  <li>The logic makes sense because in a large text, the beginning part usually describes what the text is all about and the end part describes the conclusion of the text.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
</pre></td><td class="code"><pre> <span class="k">def</span> <span class="nf">get_tokenizer</span><span class="p">(</span><span class="n">model_name</span><span class="p">):</span>
     <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">getting tokenizer for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s">...</span><span class="sh">'</span><span class="p">)</span>
     <span class="k">if</span> <span class="n">model_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">xlnet-base-cased</span><span class="sh">'</span><span class="p">:</span>
         <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">XLNetTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">xlnet-base-cased</span><span class="sh">'</span><span class="p">)</span>
     <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">roberta-base</span><span class="sh">'</span><span class="p">:</span>
         <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RobertaTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">roberta-base</span><span class="sh">'</span><span class="p">)</span>
     <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">bert-base-uncased</span><span class="sh">'</span><span class="p">:</span>
         <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">bert-base-uncased</span><span class="sh">'</span><span class="p">)</span>

     <span class="k">return</span> <span class="n">tokenizer</span>

 <span class="k">def</span> <span class="nf">fix_length</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">max_sequence_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">q_max_len</span><span class="o">=</span><span class="mi">254</span><span class="p">,</span> <span class="n">a_max_len</span><span class="o">=</span><span class="mi">254</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="sh">'</span><span class="s">questions</span><span class="sh">'</span><span class="p">):</span>
     <span class="k">if</span> <span class="n">model_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">questions</span><span class="sh">'</span><span class="p">:</span>
         <span class="n">length</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
         <span class="k">if</span> <span class="n">length</span> <span class="o">&gt;</span> <span class="n">max_sequence_length</span><span class="p">:</span>
             <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[:</span><span class="n">max_sequence_length</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
         <span class="k">return</span> <span class="n">tokens</span>

     <span class="k">else</span><span class="p">:</span>
         <span class="n">question_tokens</span><span class="p">,</span> <span class="n">answer_tokens</span> <span class="o">=</span> <span class="n">tokens</span>
         <span class="n">q_len</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">question_tokens</span><span class="p">)</span>
         <span class="n">a_len</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">answer_tokens</span><span class="p">)</span>
         <span class="k">if</span> <span class="n">q_len</span> <span class="o">+</span> <span class="n">a_len</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">&gt;</span> <span class="n">max_sequence_length</span><span class="p">:</span>
             <span class="k">if</span> <span class="n">a_max_len</span> <span class="o">&lt;=</span> <span class="n">a_len</span> <span class="ow">and</span> <span class="n">q_max_len</span> <span class="o">&lt;=</span> <span class="n">q_len</span><span class="p">:</span>
                 <span class="n">q_new_len_head</span> <span class="o">=</span> <span class="n">q_max_len</span><span class="o">//</span><span class="mi">2</span>
                 <span class="n">question_tokens</span> <span class="o">=</span> <span class="n">question_tokens</span><span class="p">[:</span><span class="n">q_new_len_head</span><span class="p">]</span> <span class="o">+</span> <span class="n">question_tokens</span><span class="p">[</span><span class="o">-</span><span class="n">q_new_len_head</span><span class="p">:]</span>
                 <span class="n">a_new_len_head</span> <span class="o">=</span> <span class="n">a_max_len</span><span class="o">//</span><span class="mi">2</span>
                 <span class="n">answer_tokens</span> <span class="o">=</span> <span class="n">answer_tokens</span><span class="p">[:</span><span class="n">a_new_len_head</span><span class="p">]</span> <span class="o">+</span> <span class="n">answer_tokens</span><span class="p">[</span><span class="o">-</span><span class="n">a_new_len_head</span><span class="p">:]</span>
             <span class="k">elif</span> <span class="n">q_len</span> <span class="o">&lt;=</span> <span class="n">a_len</span> <span class="ow">and</span> <span class="n">q_len</span> <span class="o">&lt;</span> <span class="n">q_max_len</span><span class="p">:</span>
                 <span class="n">a_max_len</span> <span class="o">=</span> <span class="n">a_max_len</span> <span class="o">+</span> <span class="p">(</span><span class="n">q_max_len</span> <span class="o">-</span> <span class="n">q_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                 <span class="n">a_new_len_head</span> <span class="o">=</span> <span class="n">a_max_len</span><span class="o">//</span><span class="mi">2</span>
                 <span class="n">answer_tokens</span> <span class="o">=</span> <span class="n">answer_tokens</span><span class="p">[:</span><span class="n">a_new_len_head</span><span class="p">]</span> <span class="o">+</span> <span class="n">answer_tokens</span><span class="p">[</span><span class="o">-</span><span class="n">a_new_len_head</span><span class="p">:]</span>
             <span class="k">elif</span> <span class="n">a_len</span> <span class="o">&lt;</span> <span class="n">q_len</span><span class="p">:</span>
                 <span class="n">q_max_len</span> <span class="o">=</span> <span class="n">q_max_len</span> <span class="o">+</span> <span class="p">(</span><span class="n">a_max_len</span> <span class="o">-</span> <span class="n">a_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                 <span class="n">q_new_len_head</span> <span class="o">=</span> <span class="n">q_max_len</span><span class="o">//</span><span class="mi">2</span>
                 <span class="n">question_tokens</span> <span class="o">=</span> <span class="n">question_tokens</span><span class="p">[:</span><span class="n">q_new_len_head</span><span class="p">]</span> <span class="o">+</span> <span class="n">question_tokens</span><span class="p">[</span><span class="o">-</span><span class="n">q_new_len_head</span><span class="p">:]</span>

     <span class="k">return</span> <span class="n">question_tokens</span><span class="p">,</span> <span class="n">answer_tokens</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Next is the code block for generating the <strong>input_ids, attention_masks,</strong> and <strong>token_type_ids.</strong> I’ve used a condition that checks if the function needs to return the generated data for base learners relying on the dataset <strong>[question_title + question_body]</strong> or the dataset <strong>[question_title + question_body + answer].</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="code"><pre> <span class="k">def</span> <span class="nf">transformer_inputs</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">question</span><span class="p">,</span> <span class="n">answer</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="sh">'</span><span class="s">questions</span><span class="sh">'</span><span class="p">,</span> <span class="n">MAX_SEQUENCE_LENGTH</span> <span class="o">=</span> <span class="mi">512</span><span class="p">):</span>

     <span class="k">if</span> <span class="n">model_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">questions</span><span class="sh">'</span><span class="p">:</span>
         <span class="n">question</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s"> [SEP] </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="sh">"</span>
         <span class="n">question_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
         <span class="n">question_tokens</span> <span class="o">=</span> <span class="nf">fix_length</span><span class="p">(</span><span class="n">question_tokens</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="n">model_type</span><span class="p">)</span>
         <span class="n">ids_q</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">convert_tokens_to_ids</span><span class="p">([</span><span class="sh">"</span><span class="s">[CLS]</span><span class="sh">"</span><span class="p">]</span> <span class="o">+</span> <span class="n">question_tokens</span><span class="p">)</span>
         <span class="n">padded_ids</span> <span class="o">=</span> <span class="p">(</span><span class="n">ids_q</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids_q</span><span class="p">)))[:</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">]</span>
         <span class="n">token_type_ids</span> <span class="o">=</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">)[:</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">]</span>
         <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids_q</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids_q</span><span class="p">)))[:</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">]</span>

         <span class="k">return</span> <span class="n">padded_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">,</span> <span class="n">attention_mask</span>

     <span class="k">else</span><span class="p">:</span>
         <span class="n">question</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s"> [SEP] </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="sh">"</span>
         <span class="n">question_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
         <span class="n">answer_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>
         <span class="n">question_tokens</span><span class="p">,</span> <span class="n">answer_tokens</span> <span class="o">=</span> <span class="nf">fix_length</span><span class="p">(</span><span class="n">tokens</span><span class="o">=</span><span class="p">(</span><span class="n">question_tokens</span><span class="p">,</span> <span class="n">answer_tokens</span><span class="p">),</span> <span class="n">model_type</span><span class="o">=</span><span class="n">model_type</span><span class="p">)</span>
         <span class="n">ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">convert_tokens_to_ids</span><span class="p">([</span><span class="sh">"</span><span class="s">[CLS]</span><span class="sh">"</span><span class="p">]</span> <span class="o">+</span> <span class="n">question_tokens</span> <span class="o">+</span> <span class="p">[</span><span class="sh">"</span><span class="s">[SEP]</span><span class="sh">"</span><span class="p">]</span> <span class="o">+</span> <span class="n">answer_tokens</span> <span class="o">+</span> <span class="p">[</span><span class="sh">"</span><span class="s">[SEP]</span><span class="sh">"</span><span class="p">])</span>
         <span class="n">padded_ids</span> <span class="o">=</span> <span class="n">ids</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids</span><span class="p">))</span>
         <span class="n">token_type_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">question_tokens</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">answer_tokens</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids</span><span class="p">))</span>
         <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">MAX_SEQUENCE_LENGTH</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">ids</span><span class="p">))</span>

         <span class="k">return</span> <span class="n">padded_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">,</span> <span class="n">attention_mask</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Finally, here is the function that makes use of the function initialized above and generates <strong>input_ids, attention_masks,</strong> and <strong>token_type_ids</strong> for each of the instances in the provided data.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre><span class="c1"># function for creating the input_ids, masks and segments for the transformer input
</span><span class="k">def</span> <span class="nf">input_data</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="sh">'</span><span class="s">questions</span><span class="sh">'</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">generating </span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s"> input for transformer...</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">input_ids</span><span class="p">,</span> <span class="n">input_token_type_ids</span><span class="p">,</span> <span class="n">input_attention_masks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">title</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">answer</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">question_title</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">question_body</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">answer</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">)):</span>
        <span class="n">ids</span><span class="p">,</span> <span class="n">type_ids</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="nf">transformer_inputs</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">answer</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="n">model_type</span><span class="p">)</span>
        <span class="n">input_ids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
        <span class="n">input_token_type_ids</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">type_ids</span><span class="p">)</span>
        <span class="n">input_attention_masks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>

    <span class="nf">return </span><span class="p">(</span>
        <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">input_attention_masks</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">input_token_type_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">))</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>To make the model training easy, I also created a class that generates train and cross-validation data based on the fold while using KFlod CV with the help of the functions specified above.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
</pre></td><td class="code"><pre> <span class="k">class</span> <span class="nc">data_generator</span><span class="p">:</span>
   <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">type_</span><span class="p">):</span>
       <span class="c1"># test data
</span>       <span class="n">tokens</span><span class="p">,</span> <span class="n">masks</span><span class="p">,</span> <span class="n">segments</span> <span class="o">=</span> <span class="nf">input_data</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">type_</span><span class="p">)</span>
       <span class="n">self</span><span class="p">.</span><span class="n">test_data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">input_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">tokens</span><span class="p">,</span>
                         <span class="sh">'</span><span class="s">input_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">masks</span><span class="p">,</span>
                         <span class="sh">'</span><span class="s">input_segment</span><span class="sh">'</span><span class="p">:</span> <span class="n">segments</span><span class="p">}</span>

       <span class="c1"># Train data
</span>       <span class="n">self</span><span class="p">.</span><span class="n">tokens</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">masks</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">segments</span> <span class="o">=</span> <span class="nf">input_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">type_</span><span class="p">)</span>
   <span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">tr</span><span class="p">,</span> <span class="n">cv</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">xlnet-base-cased</span><span class="sh">'</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="sh">'</span><span class="s">questions</span><span class="sh">'</span><span class="p">):</span>
       <span class="k">if</span> <span class="n">name</span><span class="o">!=</span><span class="sh">'</span><span class="s">xlnet-base-cased</span><span class="sh">'</span><span class="p">:</span>
           <span class="n">train_data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">input_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">tokens</span><span class="p">[</span><span class="n">tr</span><span class="p">],</span>
                         <span class="sh">'</span><span class="s">input_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">masks</span><span class="p">[</span><span class="n">tr</span><span class="p">],</span>
                         <span class="sh">'</span><span class="s">input_segment</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">segments</span><span class="p">[</span><span class="n">tr</span><span class="p">]}</span>

           <span class="n">cv_data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">input_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">tokens</span><span class="p">[</span><span class="n">cv</span><span class="p">],</span>
                     <span class="sh">'</span><span class="s">input_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">masks</span><span class="p">[</span><span class="n">cv</span><span class="p">],</span>
                     <span class="sh">'</span><span class="s">input_segment</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">segments</span><span class="p">[</span><span class="n">cv</span><span class="p">]}</span>
       <span class="k">else</span><span class="p">:</span>
           <span class="n">train_data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">input_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">tokens</span><span class="p">[</span><span class="n">tr</span><span class="p">],</span>
                         <span class="sh">'</span><span class="s">input_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">masks</span><span class="p">[</span><span class="n">tr</span><span class="p">]}</span>

           <span class="n">cv_data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">input_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">tokens</span><span class="p">[</span><span class="n">cv</span><span class="p">],</span>
                     <span class="sh">'</span><span class="s">input_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">masks</span><span class="p">[</span><span class="n">cv</span><span class="p">]}</span>

       <span class="k">if</span> <span class="n">model_type</span><span class="o">==</span><span class="sh">'</span><span class="s">questions</span><span class="sh">'</span><span class="p">:</span>
           <span class="n">y_tr</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="n">tr</span><span class="p">,</span> <span class="mi">21</span><span class="p">:]</span>
           <span class="n">y_cv</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="n">cv</span><span class="p">,</span> <span class="mi">21</span><span class="p">:]</span>

       <span class="k">elif</span> <span class="n">model_type</span><span class="o">==</span><span class="sh">'</span><span class="s">answers</span><span class="sh">'</span><span class="p">:</span>
           <span class="n">y_tr</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="n">tr</span><span class="p">,</span> <span class="mi">21</span><span class="p">:]</span>
           <span class="n">y_cv</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="n">cv</span><span class="p">,</span> <span class="mi">21</span><span class="p">:]</span>

       <span class="k">else</span><span class="p">:</span>
           <span class="n">y_tr</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="n">tr</span><span class="p">]</span>
           <span class="n">y_cv</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="n">cv</span><span class="p">]</span>  

       <span class="k">return</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">cv_data</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_cv</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<blockquote>
  <p><strong>Ensembling</strong></p>
</blockquote>

<p>After data preprocessing, let’s create the model architecture starting with base learners.</p>

<p>The code below takes the model name as input, collects the pre-trained model, and its configuration information according to the input name and creates the base learner model. Notice that <strong>output_hidden_states=True</strong> is passed after adding the config data.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre> <span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
     <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">xlnet-base-cased</span><span class="sh">'</span><span class="p">:</span>
         <span class="n">config</span> <span class="o">=</span> <span class="n">XLNetConfig</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">xlnet-base-cased</span><span class="sh">'</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
         <span class="n">model</span> <span class="o">=</span> <span class="n">TFXLNetModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">xlnet-base-cased</span><span class="sh">'</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
     <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">roberta-base</span><span class="sh">'</span><span class="p">:</span>
         <span class="n">config</span> <span class="o">=</span> <span class="n">RobertaConfig</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">roberta-base</span><span class="sh">'</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
         <span class="n">model</span> <span class="o">=</span> <span class="n">TFRobertaModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">roberta-base</span><span class="sh">'</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
     <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">bert-base-uncased</span><span class="sh">'</span><span class="p">:</span>
         <span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">bert-base-uncased</span><span class="sh">'</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
         <span class="n">model</span> <span class="o">=</span> <span class="n">TFBertModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">bert-base-uncased</span><span class="sh">'</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
     <span class="k">return</span> <span class="n">model</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>The next code block is to create the ensemble architecture. The function accepts 2 parameters name that expects the name of the model that we want to train and model_type that expects the type of model we want to train. The model type can be <strong>bert-base-uncased, roberta-base</strong> or <strong>xlnet-base-cased</strong> whereas the model type can be <strong>questions, answers,</strong> or <strong>question_answers.</strong>
The function <strong>create_model()</strong> takes the model_name and model_type and generates a model that can be trained on the specified data accordingly.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
</pre></td><td class="code"><pre> <span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">xlnet-base-cased</span><span class="sh">'</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="sh">'</span><span class="s">questions</span><span class="sh">'</span><span class="p">):</span>
     <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">creating model </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">...</span><span class="sh">'</span><span class="p">)</span>
     <span class="c1"># Creating the model
</span>     <span class="n">K</span><span class="p">.</span><span class="nf">clear_session</span><span class="p">()</span>
     <span class="n">max_seq_length</span> <span class="o">=</span> <span class="mi">512</span>

     <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">input_tokens</span><span class="sh">"</span><span class="p">)</span>
     <span class="n">input_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">input_mask</span><span class="sh">"</span><span class="p">)</span>
     <span class="n">input_segment</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">input_segment</span><span class="sh">"</span><span class="p">)</span>

     <span class="n">model</span> <span class="o">=</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
     <span class="nf">if </span><span class="p">(</span><span class="n">name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">xlnet-base-cased</span><span class="sh">'</span><span class="p">):</span>
       <span class="n">sequence_output</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="nf">model</span><span class="p">([</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">])</span>
     <span class="nf">elif </span><span class="p">(</span><span class="n">name</span><span class="o">==</span><span class="sh">'</span><span class="s">roberta-base</span><span class="sh">'</span> <span class="ow">and</span> <span class="n">model_type</span><span class="o">!=</span><span class="sh">'</span><span class="s">questions</span><span class="sh">'</span><span class="p">):</span>
       <span class="n">sequence_output</span><span class="p">,</span> <span class="n">pooler_output</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="nf">model</span><span class="p">([</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">])</span>
     <span class="k">else</span><span class="p">:</span>
       <span class="n">sequence_output</span><span class="p">,</span> <span class="n">pooler_output</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="nf">model</span><span class="p">([</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">input_segment</span><span class="p">])</span>

     <span class="c1"># Last 4 hidden layers of transformer
</span>     <span class="n">h12</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">768</span><span class="p">))</span>
     <span class="n">h11</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">768</span><span class="p">))</span>
     <span class="n">h10</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">768</span><span class="p">))</span>
     <span class="n">h09</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">768</span><span class="p">))</span>
     <span class="n">concat_hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Concatenate</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)([</span><span class="n">h12</span><span class="p">,</span> <span class="n">h11</span><span class="p">,</span> <span class="n">h10</span><span class="p">,</span> <span class="n">h09</span><span class="p">])</span>

     <span class="n">x</span> <span class="o">=</span> <span class="nc">GlobalAveragePooling1D</span><span class="p">()(</span><span class="n">concat_hidden</span><span class="p">)</span>

     <span class="n">x</span> <span class="o">=</span> <span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

     <span class="k">if</span> <span class="n">model_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">answers</span><span class="sh">'</span><span class="p">:</span>
       <span class="n">output</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
     <span class="k">elif</span> <span class="n">model_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">questions</span><span class="sh">'</span><span class="p">:</span>
       <span class="n">output</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
     <span class="k">else</span><span class="p">:</span>
       <span class="n">output</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

     <span class="nf">if </span><span class="p">(</span><span class="n">name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">xlnet-base-cased</span><span class="sh">'</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">name</span><span class="o">==</span><span class="sh">'</span><span class="s">roberta-base</span><span class="sh">'</span> <span class="ow">and</span> <span class="n">model_type</span><span class="o">!=</span><span class="sh">'</span><span class="s">questions</span><span class="sh">'</span><span class="p">):</span>
       <span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
     <span class="k">else</span><span class="p">:</span>
       <span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">input_segment</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>

     <span class="k">return</span> <span class="n">model</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now let’s create a function for calculating the evaluation metric <a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient">Spearman’s correlation coefficient</a>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre> <span class="c1"># Function to calculate the Spearman's rank correlation coefficient 'rhos' of actual and predicted data.
</span> <span class="k">def</span> <span class="nf">compute_spearmanr_ignore_nan</span><span class="p">(</span><span class="n">trues</span><span class="p">,</span> <span class="n">preds</span><span class="p">):</span>
     <span class="n">rhos</span> <span class="o">=</span> <span class="p">[]</span>
     <span class="k">for</span> <span class="n">tcol</span><span class="p">,</span> <span class="n">pcol</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">trues</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">preds</span><span class="p">)):</span>
         <span class="n">rhos</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">spearmanr</span><span class="p">(</span><span class="n">tcol</span><span class="p">,</span> <span class="n">pcol</span><span class="p">).</span><span class="n">correlation</span><span class="p">)</span>
     <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">nanmean</span><span class="p">(</span><span class="n">rhos</span><span class="p">)</span>

 <span class="c1"># Making the 'rhos' metric to tensorflow graph compatible.
</span> <span class="k">def</span> <span class="nf">rhos</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
     <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">py_function</span><span class="p">(</span><span class="n">compute_spearmanr_ignore_nan</span><span class="p">,</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">double</span><span class="p">)</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now we need a function that can collect the base learner model, data according to the base learner model, and train the model.
I’ve used K-Fold cross-validation with 5 folds for training.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="code"><pre> <span class="k">def</span> <span class="nf">fit_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">model_type</span><span class="p">,</span> <span class="n">data_gen</span><span class="p">,</span> <span class="n">file_path</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">use_saved_weights</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
   <span class="n">path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">../input/google-qna-predicted-data/</span><span class="sh">'</span>
   <span class="k">if</span> <span class="n">use_saved_weights</span><span class="p">:</span>
     <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">getting saved weights for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s">...</span><span class="sh">'</span><span class="p">)</span>
     <span class="n">model</span><span class="p">.</span><span class="nf">load_weights</span><span class="p">(</span><span class="n">path</span><span class="o">+</span><span class="n">file_path</span><span class="p">)</span>

   <span class="k">else</span><span class="p">:</span>
     <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">fitting data on </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s">...</span><span class="sh">'</span><span class="p">)</span>
     <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.00002</span><span class="p">)</span>
     <span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">binary_crossentropy</span><span class="sh">'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">rhos</span><span class="p">])</span>
     <span class="n">kf</span> <span class="o">=</span> <span class="nc">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
     <span class="k">for</span> <span class="n">tr</span><span class="p">,</span> <span class="n">cv</span> <span class="ow">in</span> <span class="n">kf</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
       <span class="n">tr_data</span><span class="p">,</span> <span class="n">cv_data</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_cv</span> <span class="o">=</span> <span class="n">data_gen</span><span class="p">.</span><span class="nf">generate_data</span><span class="p">(</span><span class="n">tr</span><span class="p">,</span> <span class="n">cv</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">model_type</span><span class="p">)</span>
       <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">tr_data</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">cv_data</span><span class="p">,</span> <span class="n">y_cv</span><span class="p">))</span>
       <span class="n">model</span><span class="p">.</span><span class="nf">save_weights</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>

   <span class="k">return</span> <span class="n">model</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now once we have trained the models and generated the predicted values, we need a function for calculating the weighted average. Here’s the code for that.
*The weight’s in the weighted average are all 1s.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre> <span class="k">def</span> <span class="nf">get_weighted_avg</span><span class="p">(</span><span class="n">model_predictions</span><span class="p">):</span>
   <span class="n">xlnet_q</span><span class="p">,</span> <span class="n">xlnet_a</span><span class="p">,</span> <span class="n">roberta_q</span><span class="p">,</span> <span class="n">roberta_a</span><span class="p">,</span> <span class="n">roberta_qa</span><span class="p">,</span> <span class="n">bert_q</span><span class="p">,</span> <span class="n">bert_a</span><span class="p">,</span> <span class="n">bert_qa</span> <span class="o">=</span> <span class="n">model_predictions</span>
   <span class="n">xlnet_concat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">xlnet_q</span><span class="p">,</span> <span class="n">xlnet_a</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
   <span class="n">bert_concat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">bert_q</span><span class="p">,</span> <span class="n">bert_a</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
   <span class="n">roberta_concat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">roberta_q</span><span class="p">,</span> <span class="n">roberta_a</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
   <span class="n">predict</span> <span class="o">=</span> <span class="p">(</span><span class="n">roberta_qa</span> <span class="o">+</span> <span class="n">bert_qa</span> <span class="o">+</span> <span class="n">xlnet_concat</span> <span class="o">+</span> <span class="n">bert_concat</span> <span class="o">+</span> <span class="n">roberta_concat</span><span class="p">)</span><span class="o">/</span><span class="mi">5</span>

   <span class="k">return</span> <span class="n">predict</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Before bringing everything together, there is one more function that I used for processing the final predicted values. Remember in the EDA section there was an analysis of the target values where we noticed that the target values were only 25 unique floats between 0 and 1. To make use of that information, I calculated 61 (a hyperparameter) uniformly distributed percentile values and mapped them to the 25 unique values. This created 61 bins uniformly spaced between the upper and lower range of the target values. Now to process the predicted data, I used those bins to collect the predicted values and put them in the right place/order. This trick helped in improving the score in the final submission to the leaderboard to some extent.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="code"><pre> <span class="c1"># https://www.kaggle.com/markpeng/ensemble-5models-v4-v7-magic/notebook?select=submission.csv#Do-Inference
</span> <span class="k">def</span> <span class="nf">get_exp_labels</span><span class="p">(</span><span class="n">train</span><span class="p">):</span>
     <span class="n">X</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">11</span><span class="p">:]</span>
     <span class="n">unique_labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>
     <span class="n">denominator</span> <span class="o">=</span> <span class="mi">60</span>
     <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">100</span> <span class="o">/</span> <span class="n">denominator</span><span class="p">)</span>
     <span class="n">exp_labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">unique_labels</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span> <span class="c1"># Generating the 60 bins.
</span>     <span class="k">return</span> <span class="n">exp_labels</span>

 <span class="k">def</span> <span class="nf">optimize_ranks</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">unique_labels</span><span class="p">):</span>
     <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">optimizing the predicted values...</span><span class="sh">'</span><span class="p">)</span>
     <span class="n">new_preds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">preds</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
     <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">preds</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
         <span class="n">interpolate_bins</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">digitize</span><span class="p">(</span><span class="n">preds</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">unique_labels</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
         <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">interpolate_bins</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
             <span class="n">new_preds</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
         <span class="k">else</span><span class="p">:</span>
             <span class="n">new_preds</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">unique_labels</span><span class="p">[</span><span class="n">interpolate_bins</span><span class="p">]</span>

     <span class="k">return</span> <span class="n">new_preds</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Finally, to bring the data-preprocessing, model training, and post-processing together, I created the <strong>get_predictions()</strong> function that-</p>
<ul>
  <li>Collects the data.</li>
  <li>Creates the 8 base_learners.</li>
  <li>Prepares the data for the base_learners.</li>
  <li>Trains the base learners and collects the predicted values from them.</li>
  <li>Calculates the weighted average of the predicted values.</li>
  <li>Processes the weighted average prediction.</li>
  <li>Converts the final predicted values into a dataframe format requested by Kaggle for submission and return it.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
</pre></td><td class="code"><pre> <span class="k">def</span> <span class="nf">get_predictions</span><span class="p">(</span><span class="n">predictions_present</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">model_saved_weights_present</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
   <span class="n">msw</span> <span class="o">=</span> <span class="n">model_saved_weights_present</span>
   <span class="n">X</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="nf">get_data</span><span class="p">()</span>
   <span class="n">path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">../input/google-qna-predicted-data/</span><span class="sh">'</span>
   <span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">xlnet-base-cased</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">roberta-base</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">bert-base-uncased</span><span class="sh">'</span><span class="p">]</span>
   <span class="n">model_types</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">questions</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">answers</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">questions_answers</span><span class="sh">'</span><span class="p">]</span>
   <span class="n">saved_weights_names</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">xlnet_q.h5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">xlnet_a.h5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">roberta_q.h5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">roberta_a.h5</span><span class="sh">'</span><span class="p">,</span>
                         <span class="sh">'</span><span class="s">roberta_qa.h5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">bert_q.h5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">bert_a.h5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">bert_qa.h5</span><span class="sh">'</span><span class="p">]</span>

   <span class="n">saved_model_predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">path</span><span class="o">+</span><span class="sh">'</span><span class="s">xlnet_q.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">path</span><span class="o">+</span><span class="sh">'</span><span class="s">xlnet_a.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">path</span><span class="o">+</span><span class="sh">'</span><span class="s">roberta_q.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">path</span><span class="o">+</span><span class="sh">'</span><span class="s">roberta_a.csv</span><span class="sh">'</span><span class="p">,</span>
                               <span class="n">path</span><span class="o">+</span><span class="sh">'</span><span class="s">roberta_qa.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">path</span><span class="o">+</span><span class="sh">'</span><span class="s">bert_q.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">path</span><span class="o">+</span><span class="sh">'</span><span class="s">bert_a.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">path</span><span class="o">+</span><span class="sh">'</span><span class="s">bert_qa.csv</span><span class="sh">'</span><span class="p">]</span>
   <span class="n">model_predictions</span> <span class="o">=</span> <span class="p">[]</span>

   <span class="k">if</span> <span class="n">predictions_present</span><span class="p">:</span>
     <span class="n">model_predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">file_name</span><span class="p">).</span><span class="n">values</span> <span class="k">for</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="n">saved_model_predictions</span><span class="p">]</span>

   <span class="k">else</span><span class="p">:</span>
     <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
     <span class="k">for</span> <span class="n">name_</span> <span class="ow">in</span> <span class="n">model_names</span><span class="p">:</span>
       <span class="k">for</span> <span class="n">type_</span> <span class="ow">in</span> <span class="n">model_types</span><span class="p">:</span>
         <span class="k">if</span> <span class="n">name_</span> <span class="o">==</span> <span class="sh">'</span><span class="s">xlnet-base-cased</span><span class="sh">'</span> <span class="ow">and</span> <span class="n">type_</span> <span class="o">==</span> <span class="sh">'</span><span class="s">questions_answers</span><span class="sh">'</span><span class="p">:</span>
           <span class="k">continue</span>
         <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
         <span class="n">model</span> <span class="o">=</span> <span class="nf">create_model</span><span class="p">(</span><span class="n">name_</span><span class="p">,</span> <span class="n">type_</span><span class="p">)</span>
         <span class="n">tokenizer</span> <span class="o">=</span> <span class="nf">get_tokenizer</span><span class="p">(</span><span class="n">name_</span><span class="p">)</span>
         <span class="n">data_gen</span> <span class="o">=</span> <span class="nf">data_generator</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">type_</span><span class="p">)</span>
         <span class="n">model</span> <span class="o">=</span> <span class="nf">fit_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">name_</span><span class="p">,</span> <span class="n">type_</span><span class="p">,</span> <span class="n">data_gen</span><span class="p">,</span> <span class="n">saved_weights_names</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">train</span><span class="p">,</span> <span class="n">msw</span><span class="p">)</span>
         <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">getting target predictions from </span><span class="si">{</span><span class="n">name_</span><span class="si">}</span><span class="s">...</span><span class="sh">'</span><span class="p">)</span>
         <span class="n">model_predictions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">data_gen</span><span class="p">.</span><span class="n">test_data</span><span class="p">))</span>
         <span class="n">i</span><span class="o">+=</span><span class="mi">1</span>

   <span class="n">predicted_labels</span> <span class="o">=</span> <span class="nf">get_weighted_avg</span><span class="p">(</span><span class="n">model_predictions</span><span class="p">)</span>
   <span class="n">exp_labels</span> <span class="o">=</span> <span class="nf">get_exp_labels</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
   <span class="n">optimized_predicted_labels</span> <span class="o">=</span> <span class="nf">optimize_ranks</span><span class="p">(</span><span class="n">predicted_labels</span><span class="p">,</span> <span class="n">exp_labels</span><span class="p">)</span>
   <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">test</span><span class="p">[</span><span class="sh">'</span><span class="s">qa_id</span><span class="sh">'</span><span class="p">],</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">optimized_predicted_labels</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">train</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">11</span><span class="p">:])],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
   <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">done...!</span><span class="sh">'</span><span class="p">)</span>

   <span class="k">return</span> <span class="n">df</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<blockquote>
  <p><strong>Getting the scores from Kaggle</strong></p>
</blockquote>

<p>Once the code compiles and runs successfully, it generates an output file that can be submitted to Kaggle for <strong>score</strong> calculation. The ranking of the code on the leaderboard is generated using the <strong>score.</strong>
The ensemble model got a public score of <strong>0.43658</strong> which makes it in the top 4.4% on the leaderboard.</p>

<p><img src="" alt="" /></p>
<h2 id="post-modeling-analysis">Post modeling Analysis</h2>

<p><strong><em>Check-out the notebook with complete post-modeling analysis (<a href="https://www.kaggle.com/sarthakvajpayee/top-4-4-post-modeling-analysis?scriptVersionId=40262842">Kaggle link</a>).</em></strong></p>

<p><img src="" alt="" /></p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4320/1*7gTtCiVIl_oCN6Wpq4R-9g.png" />
    </div>
</div>

<p><img src="" alt="" />
Its time for some post-modeling analysis!</p>

<p>In this section, we will go through an analysis of train data to figure out what parts of the data is the model doing well on and what parts of the data it’s not.
The main idea behind this step is to know the capability of the trained model and it works like a charm if applied properly for fine-tuning the model and data.
But we won’t get into the fine-tuning part in this section, we will just be performing some basic EDA on the train data using the predicted target values for the train data.
I’ll be covering the data feature by feature. Here are the top features we’ll be performing analysis on-</p>

<ul>
  <li>
    <p>question_title, question_body, and answer.</p>
  </li>
  <li>
    <p>Word lengths of question_title, question_body, and answer.</p>
  </li>
  <li>
    <p>Host</p>
  </li>
  <li>
    <p>Category</p>
  </li>
</ul>

<p>First, we will have to divide the data into a spectrum of good data and bad data. Good data will be the data points on which the model achieves a good score and bad data will be the data points on which the model achieves a bad score.
Now for scoring, we will be comparing the actual target values of the train data with the model’s predicted target values on train data. I used <strong>mean squared error (MSE)</strong> as a metric for scoring since it focuses on how close the actual and target values are. Remember the more the MSE-score is, the bad the data point will be.
Calculating the MSE-score is pretty simple. Here’s the code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Generating the MSE-score for each data point in train data.
from sklearn.metrics import mean_squared_error

train_score = [mean_squared_error(i,j) for i,j in zip(y_pred, y_true)]

# sorting the losses from minimum to maximum index wise.
train_score_args = np.argsort(train_score)
</code></pre></div></div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>question_title, question_body, and answer</strong></p>
</blockquote>

<p>Starting with the first set of features, which are all text type features, I’ll be plotting word clouds using them. The plan is to segment out these features from 5 data-points that have the least scores and from another 5 data-points that have the most scores.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="code"><pre> <span class="c1"># function for generating wordcloud
</span> <span class="kn">from</span> <span class="n">wordcloud</span> <span class="kn">import</span> <span class="n">WordCloud</span><span class="p">,</span> <span class="n">STOPWORDS</span>
 <span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
 <span class="n">sns</span><span class="p">.</span><span class="nf">set</span><span class="p">()</span>

 <span class="k">def</span> <span class="nf">generate_wordcloud</span><span class="p">(</span><span class="n">indexes</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">):</span>
   <span class="n">comment_words</span> <span class="o">=</span> <span class="sh">''</span>
   <span class="n">stopwords</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">STOPWORDS</span><span class="p">)</span>

   <span class="n">title_words</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">question_title</span><span class="sh">'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">indexes</span><span class="p">]</span>
   <span class="n">body_words</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">question_body</span><span class="sh">'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">indexes</span><span class="p">]</span>
   <span class="n">answer_words</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">indexes</span><span class="p">]</span>

   <span class="n">title_cloud</span> <span class="o">=</span> <span class="nc">WordCloud</span><span class="p">(</span><span class="n">width</span> <span class="o">=</span> <span class="mi">400</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">background_color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span>
                         <span class="n">stopwords</span> <span class="o">=</span> <span class="n">stopwords</span><span class="p">,</span> <span class="n">min_font_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">).</span><span class="nf">generate</span><span class="p">(</span><span class="n">title_words</span><span class="p">)</span>

   <span class="n">body_cloud</span> <span class="o">=</span> <span class="nc">WordCloud</span><span class="p">(</span><span class="n">width</span> <span class="o">=</span> <span class="mi">400</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">background_color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span>
                         <span class="n">stopwords</span> <span class="o">=</span> <span class="n">stopwords</span><span class="p">,</span> <span class="n">min_font_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">).</span><span class="nf">generate</span><span class="p">(</span><span class="n">body_words</span><span class="p">)</span>

   <span class="n">answer_cloud</span> <span class="o">=</span> <span class="nc">WordCloud</span><span class="p">(</span><span class="n">width</span> <span class="o">=</span> <span class="mi">400</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">background_color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span>
                         <span class="n">stopwords</span> <span class="o">=</span> <span class="n">stopwords</span><span class="p">,</span> <span class="n">min_font_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">).</span><span class="nf">generate</span><span class="p">(</span><span class="n">answer_words</span><span class="p">)</span>

   <span class="k">return</span> <span class="n">title_cloud</span><span class="p">,</span> <span class="n">body_cloud</span><span class="p">,</span> <span class="n">answer_cloud</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Let’s run the code and check what the results look like.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="code"><pre> <span class="c1"># I've picked the top 5 datapoints from train data with lowest loss and plotted the wordcloud of their question_title, question_body and answer.
</span> <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Top 5 data points from train data that give the </span><span class="sh">"</span><span class="s">lowest</span><span class="sh">"</span><span class="s"> loss.</span><span class="sh">'</span><span class="p">)</span>
 <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_score_args</span><span class="p">[:</span><span class="mi">5</span><span class="p">]):</span>
   <span class="n">title</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">answer</span> <span class="o">=</span> <span class="nf">generate_wordcloud</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
   <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span> <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">question_title</span><span class="sh">'</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">loss: </span><span class="si">{</span><span class="n">train_score</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">body</span><span class="p">)</span>
   <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span> <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">question_body</span><span class="sh">'</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>
   <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span> <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">setp</span><span class="p">(</span><span class="n">plt</span><span class="p">.</span><span class="nf">gcf</span><span class="p">().</span><span class="nf">get_axes</span><span class="p">(),</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[]);</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2572/1*5lOfSHKyTTV3YhsU4PxaDQ.png" />
    </div>
</div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="code"><pre> <span class="c1"># I've picked the top 5 datapoints from train data with 'highest' loss and plotted the wordcloud of their question_title, question_body and answer.
</span> <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Top 5 data points from Train data that give the </span><span class="sh">"</span><span class="s">highest</span><span class="sh">"</span><span class="s"> loss.</span><span class="sh">'</span><span class="p">)</span>
 <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_score_args</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:]):</span>
   <span class="n">title</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">answer</span> <span class="o">=</span> <span class="nf">generate_wordcloud</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">white</span><span class="sh">'</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
   <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span> <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">question_title</span><span class="sh">'</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">loss: </span><span class="si">{</span><span class="n">train_score</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">body</span><span class="p">)</span>
   <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span> <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">question_body</span><span class="sh">'</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>
   <span class="k">if</span> <span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span> <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="p">)</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">setp</span><span class="p">(</span><span class="n">plt</span><span class="p">.</span><span class="nf">gcf</span><span class="p">().</span><span class="nf">get_axes</span><span class="p">(),</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[]);</span>
   <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2676/1*pUtWG5KWy_c34dLGH2qJgA.png" />
    </div>
</div>
<blockquote>
  <p><strong>Word lengths of question_title, question_body, and answer</strong></p>
</blockquote>

<p>The next analysis is on the word lengths of question_title, question_body, and answer. For that, I’ll be picking 30 data-points that have the lowest MSE-scores and 30 data-points that have the highest MSE-scores for each of the 3 features question_title, question_body, and answer. Next, I’ll be calculating the word lengths of these 30 data-points for all the 3 features and plot them to see the trend.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="code"><pre> <span class="c1"># I've picked the top 30 datapoints from train and cv data with 'lowest' loss and plotted the word counts of their question_title, question_body and answer.
</span> <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">word counts of the question_title, question_body and answer of top 30 train and cv data with </span><span class="sh">'</span><span class="s">lowest</span><span class="sh">'</span><span class="s"> loss.</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">i</span> <span class="o">=</span> <span class="mi">30</span>
 <span class="n">title_train_len</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">))</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_score_args</span><span class="p">[:</span><span class="n">i</span><span class="p">]][</span><span class="sh">'</span><span class="s">question_title</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">]</span>
 <span class="n">body_train_len</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">))</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_score_args</span><span class="p">[:</span><span class="n">i</span><span class="p">]][</span><span class="sh">'</span><span class="s">question_body</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">]</span>
 <span class="n">answer_train_len</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">))</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_score_args</span><span class="p">[:</span><span class="n">i</span><span class="p">]][</span><span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">]</span>

 <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">title_train_len</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">question_title (train data)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">number of words</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">datapoint (loss: high --&gt; low)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">body_train_len</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">question_body (train data)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">datapoint (loss: low --&gt; high)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">answer_train_len</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">answer (train data)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">datapoint (loss: high --&gt; low)</span><span class="sh">'</span><span class="p">)</span>
 <span class="c1"># plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);
</span> <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3916/1*zfhNtTvxqVYPJYX1Emq7rA.png" />
    </div>
</div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="code"><pre> <span class="c1"># I've picked the top 30 datapoints from train data with 'highest' loss and plotted the word counts of their question_title, question_body and answer.
</span> <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">word counts of the question_title, question_body and answer of top 30 train and cv data with </span><span class="sh">'</span><span class="s">highest</span><span class="sh">'</span><span class="s"> loss.</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">i</span> <span class="o">=</span> <span class="o">-</span><span class="mi">30</span>
 <span class="n">title_train_len</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">))</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_score_args</span><span class="p">[</span><span class="n">i</span><span class="p">:]][</span><span class="sh">'</span><span class="s">question_title</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">]</span>
 <span class="n">body_train_len</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">))</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_score_args</span><span class="p">[</span><span class="n">i</span><span class="p">:]][</span><span class="sh">'</span><span class="s">question_body</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">]</span>
 <span class="n">answer_train_len</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">))</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_score_args</span><span class="p">[</span><span class="n">i</span><span class="p">:]][</span><span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">]</span>

 <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">title_train_len</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">question_title (train data)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">number of words</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">datapoint (loss: high --&gt; low)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">body_train_len</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">question_body (train data)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">datapoint (loss: high --&gt; low)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">answer_train_len</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">answer (train data)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">datapoint (loss: high --&gt; low)</span><span class="sh">'</span><span class="p">)</span>
 <span class="c1"># plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);
</span> <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3824/1*uF-kg7A_2geR4UBVJty0Nw.png" />
    </div>
</div>

<p><em>If we look at the number of words in question_title, question_body, and answer we can observe that the data that generates a high loss has a high number of words which means that the questions and answers are kind of thorough. So, the model does a good job when the questions and answers are concise.</em></p>
<blockquote>
  <p><strong>host</strong></p>
</blockquote>

<p>The next analysis is on the feature host. For this feature, I’ll be picking 100 data-points that have the lowest MSE-scores and 100 data-points that have the highest MSE-scores and select the values in the feature host. Then I’ll be plotting a histogram of this categorical feature to see the distributions.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre> <span class="c1"># I've picked the top 100 datapoints from train data with 'highest' loss and collected the values of domain names.
</span> <span class="n">top_url</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="sh">'</span><span class="s">host</span><span class="sh">'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_score_args</span><span class="p">[:</span><span class="mi">100</span><span class="p">]].</span><span class="nf">value_counts</span><span class="p">()</span>
 <span class="n">bottom_url</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="sh">'</span><span class="s">host</span><span class="sh">'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_score_args</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:]].</span><span class="nf">value_counts</span><span class="p">()</span>

 <span class="c1"># Top 10 frequently occuring domain names that lead to minimum loss
</span> <span class="n">top_url</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">10</span><span class="p">].</span><span class="n">plot</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">top 10 url domain that produce the minimum loss</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">frequency</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2556/1*JdLsXYflDINgWEOMhWryUw.png" />
    </div>
</div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre> <span class="c1"># Top 10 frequently occuring domain names that lead to maximum loss
</span> <span class="n">bottom_url</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">10</span><span class="p">].</span><span class="n">plot</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">top 10 url domain that produce the maximum loss</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">frequency</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2512/1*_-X1hQl1V8nchYotkUORpg.png" />
    </div>
</div>

<p><em>We can see that there are a lot of data points from the domain English, biology, sci-fi, physics that contribute to a lesser loss value whereas there are a lot of data points from drupal, programmers, tex that contribute to a higher loss.</em></p>

<p>Let’s also take a look at word-clouds of the unique host values that contribute to a low score and a high score. This analysis is again done using the top and bottom 100 data-points.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="code"><pre> <span class="c1"># finding the unique domain names that contribute to low and high losses
</span> <span class="n">best_url</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">top_url</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span> <span class="o">-</span> <span class="nf">set</span><span class="p">(</span><span class="n">bottom_url</span><span class="p">.</span><span class="nf">keys</span><span class="p">())))</span> <span class="c1"># set of urls that contribute solely to low loss
</span> <span class="n">worst_url</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">bottom_url</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span> <span class="o">-</span> <span class="nf">set</span><span class="p">(</span><span class="n">top_url</span><span class="p">.</span><span class="nf">keys</span><span class="p">())))</span> <span class="c1"># set of urls that contribute solely to high loss
</span>
 <span class="n">best_url_cloud</span> <span class="o">=</span> <span class="nc">WordCloud</span><span class="p">(</span><span class="n">width</span> <span class="o">=</span> <span class="mi">400</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">background_color</span> <span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">,</span>
                            <span class="n">stopwords</span> <span class="o">=</span> <span class="n">STOPWORDS</span><span class="p">,</span> <span class="n">min_font_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">).</span><span class="nf">generate</span><span class="p">(</span><span class="n">best_url</span><span class="p">)</span>

 <span class="n">worst_url_cloud</span> <span class="o">=</span> <span class="nc">WordCloud</span><span class="p">(</span><span class="n">width</span> <span class="o">=</span> <span class="mi">400</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">background_color</span> <span class="o">=</span><span class="sh">'</span><span class="s">cyan</span><span class="sh">'</span><span class="p">,</span>
                             <span class="n">stopwords</span> <span class="o">=</span> <span class="n">STOPWORDS</span><span class="p">,</span> <span class="n">min_font_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">).</span><span class="nf">generate</span><span class="p">(</span><span class="n">worst_url</span><span class="p">)</span>

 <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">best_url_cloud</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">url domain with well predicted labels (low loss)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">worst_url_cloud</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">url domain with bad predicted labels (high loss)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">setp</span><span class="p">(</span><span class="n">plt</span><span class="p">.</span><span class="nf">gcf</span><span class="p">().</span><span class="nf">get_axes</span><span class="p">(),</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[]);</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3660/1*eC0RkpXcA9jhzioVKIXTfQ.png" />
    </div>
</div>
<blockquote>
  <p><strong>Category</strong></p>
</blockquote>

<p>The final analysis is on the feature category. For this feature, I’ll be picking 100 data-points that have the lowest MSE-scores and 100 data-points that have the highest MSE-scores and select the values in the feature category. Then I’ll be plotting a pie-chart of this categorical feature to see the proportions.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre> <span class="c1"># for train data
</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
 <span class="n">X_train</span><span class="p">[</span><span class="sh">'</span><span class="s">category</span><span class="sh">'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_score_args</span><span class="p">[:</span><span class="mi">100</span><span class="p">]].</span><span class="nf">value_counts</span><span class="p">().</span><span class="n">plot</span><span class="p">.</span><span class="nf">pie</span><span class="p">(</span><span class="n">autopct</span><span class="o">=</span><span class="sh">'</span><span class="s">%1.1f%%</span><span class="sh">'</span><span class="p">,</span> <span class="n">explode</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.02</span><span class="p">,</span><span class="mf">0.04</span><span class="p">,</span><span class="mf">0.06</span><span class="p">,</span><span class="mf">0.08</span><span class="p">),</span> <span class="n">shadow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">categories of best fitted data points with minimum loss (on train data)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
 <span class="n">X_train</span><span class="p">[</span><span class="sh">'</span><span class="s">category</span><span class="sh">'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_score_args</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:]].</span><span class="nf">value_counts</span><span class="p">().</span><span class="n">plot</span><span class="p">.</span><span class="nf">pie</span><span class="p">(</span><span class="n">autopct</span><span class="o">=</span><span class="sh">'</span><span class="s">%1.1f%%</span><span class="sh">'</span><span class="p">,</span> <span class="n">explode</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.02</span><span class="p">,</span><span class="mf">0.04</span><span class="p">,</span><span class="mf">0.06</span><span class="p">,</span><span class="mf">0.08</span><span class="p">),</span> <span class="n">shadow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">categories of worst fitted data points with maximum loss (on train data)</span><span class="sh">'</span><span class="p">)</span>
 <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3660/1*cR2JuXG7-r4_Rq5-yjE8kA.png" />
    </div>
</div>

<p><img src="" alt="" />
We can notice that datapoints with category as technology make up 50% of the data that the model could not predict well whereas categories like LIFE_ARTS, SCIENCE, and CULTURE contribute much less to bad predictions.
For the good predictions, all the 5 categories contribute almost the same since there is no major difference in the proportion, still, we could say that the data-points with StackOverflow as the category contribute the least.</p>

<p><em>With this, we have come to the end of this blog and the 3 part series. Hope the read was pleasant.
You can check the complete notebook on Kaggle using [<strong>this link](https://www.kaggle.com/sarthakvajpayee/top-4-4-bert-roberta-xlnet)</strong> and leave an upvote if found my work useful.
I would like to thank all the creators for creating the awesome content I referred to for writing this blog.</em></p>

<p><em>Reference links:</em></p>

<ul>
  <li>
    <p><em>Applied AI Course: <a href="https://www.appliedaicourse.com/">https://www.appliedaicourse.com/</a></em></p>
  </li>
  <li>
    <p><a href="https://www.kaggle.com/c/google-quest-challenge/notebooks">https://www.kaggle.com/c/google-quest-challenge/notebooks</a></p>
  </li>
  <li>
    <p>[<em>http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)</em></p>
  </li>
  <li>
    <p>[<em>https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)</em></p>
  </li>
  <li>
    <p><a href="https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8">https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8</a></p>
    <blockquote>
      <p><strong>Final note</strong></p>
    </blockquote>
  </li>
</ul>

<p>Thank you for reading the blog. I hope it was useful for some of you aspiring to do projects or learn some new concepts in NLP.</p>

<p>In <a href="https://towardsdatascience.com/transformers-state-of-the-art-natural-language-processing-1d84c4c7462b?source=friends_link&amp;sk=4ba3eb424ff59ce765c749819c6b5892">part 1/3</a> we covered how Transformers became state-of-the-art in various modern natural language processing tasks and their working.</p>

<p>In <a href="https://towardsdatascience.com/understanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef?source=friends_link&amp;sk=f48ce58edfdf395fe5d86436d8102a61">part 2/3</a> we went through BERT (Bidirectional Encoder Representations from Transformers).</p>

<p>Kaggle in-depth EDA notebook link: <a href="https://www.kaggle.com/sarthakvajpayee/top-4-4-in-depth-eda-feature-scraping?scriptVersionId=40263047">https://www.kaggle.com/sarthakvajpayee/top-4-4-in-depth-eda-feature-scraping?scriptVersionId=40263047</a></p>

<p>Kaggle modeling notebook link: <a href="https://www.kaggle.com/sarthakvajpayee/top-4-4-bert-roberta-xlnet">https://www.kaggle.com/sarthakvajpayee/top-4-4-bert-roberta-xlnet</a></p>

<p>Kaggle post-modeling notebook link: <a href="https://www.kaggle.com/sarthakvajpayee/top-4-4-post-modeling-analysis?scriptVersionId=40262842">https://www.kaggle.com/sarthakvajpayee/top-4-4-post-modeling-analysis?scriptVersionId=40262842</a></p>

<p>Find me on LinkedIn: <a href="http://www.linkedin.com/in/sarthak-vajpayee">www.linkedin.com/in/sarthak-vajpayee</a></p>

<p>Find this project on Github: <a href="https://github.com/SarthakV7/Kaggle_google_quest_challenge">https://github.com/SarthakV7/Kaggle_google_quest_challenge</a></p>

<p>Peace! ☮</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Part 3/3 of Transformers vs Google QUEST Q&A Labeling (Kaggle top 5%).]]></summary></entry><entry><title type="html">How powerful can an ensemble of linear models be?</title><link href="https://sarthakv7.github.io/my_folio/blog/2020/linear_ensembles/" rel="alternate" type="text/html" title="How powerful can an ensemble of linear models be?" /><published>2020-05-29T11:10:16+00:00</published><updated>2020-05-29T11:10:16+00:00</updated><id>https://sarthakv7.github.io/my_folio/blog/2020/linear_ensembles</id><content type="html" xml:base="https://sarthakv7.github.io/my_folio/blog/2020/linear_ensembles/"><![CDATA[<h4 id="how-an-ensemble-of-linear-models-got-in-the-top-6-of-mercari-price-prediction-challenge-leaderboard-on-kaggle">How an ensemble of linear models got in the top 6% of Mercari price prediction challenge leaderboard on Kaggle.</h4>

<p><em>With the rapid growth of deep learning algorithms in recent years, today they have become a state of the art in AI. And this makes me wonder if the traditional and old school machine learning techniques like Linear Regression, Support Vector Machines, etc are still decent enough that they can go head to head with deep learning techniques?
To look over the capabilities of these often overlooked machine learning techniques I will be solving a Kaggle competition problem using only traditional machine learning techniques (no neural networks).</em></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3952/1*QLU58lo_X3qE_HLb70MTmg.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Note: I’ll be using python 3.7 for this project.</strong></p>
</blockquote>

<h3 id="birds-eye-view-of-the-blog-">Bird’s eye view of the blog-</h3>

<p>The project is divided into 6 major steps-</p>

<ul>
  <li>
    <p>Business problem and evaluation metrics</p>
  </li>
  <li>
    <p>About the data</p>
  </li>
  <li>
    <p>Exploratory Data Analysis</p>
  </li>
  <li>
    <p>Data preprocessing</p>
  </li>
  <li>
    <p>Modeling</p>
  </li>
  <li>
    <p>Obtaining scores from Kaggle leaderboard.</p>
  </li>
</ul>

<p><strong><em>Business problem and Evaluation metrics</em></strong></p>

<p>It can be hard to know how much something’s really worth. Small details can mean big differences in pricing. For example, one of these sweaters cost $335 and the other cost $9.99. Can you guess which one’s which?</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2808/1*KR9rv6UCu2XR90ldot_R0g.png" />
    </div>
</div>

<p><img src="" alt="" />
Product pricing gets even harder at scale, considering just how many products are sold online. Clothing has strong seasonal pricing trends and is heavily influenced by brand names, while electronics have fluctuating prices based on product specifications.
<a href="https://www.mercari.com/">Mercari</a>, Japan’s biggest community-powered shopping app, knows this problem deeply. They’d like to offer pricing suggestions to sellers, but this is tough because their sellers are enabled to put just about anything, or any bundle of things, on Mercari’s marketplace.
In this competition, we need to build an algorithm that automatically suggests the right product prices. We’ll be provided with text descriptions of products, and features including details like product category name, brand name, and item condition.</p>

<p>The evaluation metric for this competition is <a href="https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError">Root Mean Squared Logarithmic Error</a>. The RMSLE is calculated as:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2820/1*tZY-2x2IdDbbBoVIKxwQHA.png" />
    </div>
</div>

<p><img src="" alt="" />
Where:
<em>ϵ</em> is the RMSLE value (score)
<em>n</em> is the total number of observations in the (public/private) data set,
<em>pi</em> is the prediction of price,
<em>ai</em> is the actual sale price for <em>i</em>.
<em>log(x)</em> is the natural logarithm of x</p>

<p><strong><em>About the data</em></strong></p>

<p>The data we’ll be using is provided by Mercari and can be found on Kaggle using <a href="https://www.kaggle.com/c/mercari-price-suggestion-challenge/data">this</a> link. The data lists details about products from the Mercari website.
Let’s check out one of the products from the website and how it is described in the dataset.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4920/1*F_qskp-MUrFcMys310zPng.jpeg" />
    </div>
</div>

<p><img src="" alt="" />
<strong>The dataset has 8 features:</strong></p>

<ul>
  <li>
    <p><strong>Train_id/Test_id:</strong> Every item in the dataset has a unique item id. This will be used while submitting the predicted prices.</p>
  </li>
  <li>
    <p><strong>Name:</strong> Represents the name of the product, it is in string format. For the above product, the name is <em>‘Ayn Rand The Fountainhead’</em></p>
  </li>
  <li>
    <p><strong>Item condition:</strong> A number provided by the seller that denotes the condition of the item. It can take a value between 1 and 5. In our case, the condition of the product is ‘*good’ *so it’ll be denoted by 4 in the dataset.</p>
  </li>
  <li>
    <p><strong>Category name:</strong> Represents the category of the item. For the above item, the category mentioned in the dataset is *‘other/books/Literature &amp; Fiction’
*and this feature is also of datatype string.</p>
  </li>
  <li>
    <p><strong>Brand name:</strong> Represents the name of the brand the item belongs to. For the above product, the brand-name is <em>‘Penguin Random House’</em>.</p>
  </li>
  <li>
    <p><strong>Price:</strong> Represents the price of the item, in our case, this will be the target value that we need to predict. The unit is USD. For the above product, the price provided is <em>‘$9’</em>.</p>
  </li>
  <li>
    <p><strong>Shipping:</strong> A number that represents the type of shipping available on the product. Shipping will be 1 if the shipping fee is paid by the seller and 0 if the fee is paid by the buyer. For the above product, the shipping is free so in the dataset, this feature will be 1.</p>
  </li>
  <li>
    <p><strong>Item description:</strong> The full description of the item. For the above product, the description says, *“The Fountainhead” pocket paperback book — by Ayn Rand — Centennial Edition — classic literature — Book is in good condition with some wear on covers and corners (see pictures)”. *This feature comes already in a preprocessed form in the provided dataset.</p>
  </li>
</ul>

<p>Let’s import the data using pandas and check the first 5 entries.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">train.tsv</span><span class="sh">'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="sh">'</span><span class="se">\t</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">test.tsv</span><span class="sh">'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="sh">'</span><span class="se">\t</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">data</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4284/1*plSzaXxtFbxTSFD5LHTuhQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>

<p><strong><em>Exploratory Data Analysis (EDA)</em></strong></p>

<p>In this section, we’ll be exploring and analyzing the data in depth. We’ll be covering the data feature by feature.</p>
<blockquote>
  <p><strong>Price</strong></p>
</blockquote>

<p>This is the target feature that we need to predict using the information about the product in the form of other features.
Let’s check out the statistical summary of this feature using describe()</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">].</span><span class="nf">describe</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*P3GA8wWo1S1INdBR6q6Y4w.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>There are about 1.48 million products in the dataset. The costliest product is priced at $ 2009, the cheapest product is priced at $ 3 whereas the mean price is $ 26.75</li>
</ul>

<p>Now we’ll take a look at the histogram of the prices. Here, I’ve used the number of bins as 200.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">frequency</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">histogram of price</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2036/1*6U-7smRLqG654I2EchjKCw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>We can observe that the distribution follows a power-law distribution, to fix that, and to make it kind of Gaussian distribution, let’s convert the values to the log form i.e. we’ll be replacing the price values with log(price+1).</li>
</ul>

<p>We are converting the prices to Normal distribution as it is one of the most well-known distributions in statistics because it fits many natural phenomena and this makes it one of the most easily interpretable distributions that we can do analysis on. Another reason for transforming the data into a normal distribution is that the variance in price is reduced and most of the points are centered around the mean which makes the price prediction much easier for the model.</p>

<p>I’ve already converted the data into a log form. Here is the histogram of the log(price+1).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">price_log</span><span class="sh">'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">log(price + 1)</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">frequency</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">histogram of log of price</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*VKo8apbiIIDD5jnnDdcung.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can observe that the distribution is much more interpretable now and tries to follow a Normal distribution.</p>
  </li>
  <li>
    <p>Also, notice how most of the points are centered around the mean (the mean is somewhere near 3).</p>
    <blockquote>
      <p><strong>item_condition_id</strong></p>
    </blockquote>
  </li>
</ul>

<p>This is a categorical feature that denotes the condition of the item. Let’s check out more about it using value_counts()</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">item_condition_id</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*od3gkug_DbF-c0qDO3-mJg.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>The output tells us that this feature can take up 5 values between 1 and 5, and the number of items with that particular condition is mentioned next to it.</li>
</ul>

<p>Let’s look at the bar-graph of this feature</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre>    <span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">item_condition_id</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">().</span><span class="nf">keys</span><span class="p">(),</span>
                <span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">item_condition_id</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">())</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">item condition type</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">number of products</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">bar graph of </span><span class="sh">"</span><span class="s">item condition type</span><span class="sh">"'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*MQqGF0Uqj4rkEMAPez6yUA.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>We can see that a majority of items have a condition id of 1, and only very few items have a condition id of 5.</li>
</ul>

<p>Now let’s compare the price distribution of products with different item_condition_id</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3328/1*AOpvTg4KdoiYUkMpLVPQjA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2432/1*6uTtP9eKjbiS4vQmqaKxcA.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>We can see that the price distributions of items having different item_condition_id are very similar.</li>
</ul>

<p>Let’s check out the boxplot and violin plot of the price distribution of products with different item_condition_id.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre>    <span class="c1"># plotting box-plot
</span>    <span class="n">sns</span><span class="p">.</span><span class="nf">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">item_condition_id</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">price_log</span><span class="sh">'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

    <span class="c1"># plotting violin plot
</span>    <span class="n">sns</span><span class="p">.</span><span class="nf">violinplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">item_condition_id</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">price_log</span><span class="sh">'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2752/1*QLHsX6AoirpCRaUy3Hqnjw.png" />
    </div>
</div>

<p><img src="" alt="" />
The boxplot and violin plots also tell us that the price distributions of items with different item_condition_id are not so different, also the distributions are a bit right-skewed. Products with item_condition_id = 5 have the highest median price whereas products with item_condition_id = 4 have the lowest median price. Most of the products have a price in the range of 1.5 and 5.2</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Category name</strong></p>
</blockquote>

<p>This is a text type data that tells us about the category of the product.
Let’s check out the statistical summary of the feature category name-</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">category_name</span><span class="sh">'</span><span class="p">].</span><span class="nf">describe</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2036/1*jfUM2Vybwzar6wGW3QFADw.png" />
    </div>
</div>

<p><img src="" alt="" />
These are string type features that are actually 3 sub-categories joined into 1.
Let’s consider the most frequently occurring category name feature ‘Women/Athletic Apparel/Pants, Tights, Leggings’ as mentioned in the above description. It can be broken down into 3 sub-categories:</p>
<ul>
  <li>sub-category_1: ‘Women’</li>
  <li>sub-category_2: ‘Athletic Apparel’</li>
  <li>sub-category_3: ‘Pants, Tights, Leggings’
To make the visualization for this feature easy, I’ll consider this feature sub-category wise. Let’s divided the data sub-category wise.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="code"><pre>    <span class="c1"># this is to divide the category_name feature into 3 sub categories
</span>    <span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm_notebook</span>
    <span class="n">sub_category_1</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">sub_category_2</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">sub_category_3</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nf">tqdm_notebook</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">category_name</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">):</span>
      <span class="n">fs</span> <span class="o">=</span> <span class="n">feature</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">/</span><span class="sh">'</span><span class="p">)</span>
      <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="n">fs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">fs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">fs</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
      <span class="n">sub_category_1</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
      <span class="n">sub_category_2</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
      <span class="n">sub_category_3</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">sub_category_1</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_category_1</span>
    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">sub_category_2</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_category_2</span>
    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">sub_category_3</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_category_3</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Sub-category_1</strong></p>
</blockquote>

<p>Let’s check the statistical description:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">sub_category_1</span><span class="sh">'</span><span class="p">].</span><span class="nf">describe</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*CWomypnKmI1peXK8JMfQaQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>There are around 1.4M of these in our data, that can take 11 distinct values. The most frequent of these are Women.</li>
</ul>

<p>Let’s plot the bar graph of sub-category 1</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre>    <span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">sub_category_1</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">().</span><span class="nf">keys</span><span class="p">(),</span> <span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">sub_category_1</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">())</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">number of products</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">locs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">setp</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">bar-plot of sub_category_1</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*-A2A0l68XqYFIKf0dlVt_A.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can see that most of the items have sub_category_1 as ‘women’ and the least items have ‘Sports &amp; Outdoors’.</p>
  </li>
  <li>
    <p>Note that items with no sub_category_1 defined are denoted with ‘no label’.</p>
  </li>
</ul>

<p>Let’s check the distribution of sub_category_1 and log of price</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">sns</span><span class="p">.</span><span class="nc">FacetGrid</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="sh">"</span><span class="s">sub_category_1</span><span class="sh">"</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">5</span><span class="p">).</span><span class="nf">map</span><span class="p">(</span><span class="n">sns</span><span class="p">.</span><span class="n">distplot</span><span class="p">,</span> <span class="sh">'</span><span class="s">price_log</span><span class="sh">'</span><span class="p">).</span><span class="nf">add_legend</span><span class="p">();</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">comparing the log of price distribution of products with
               sub_category_1</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">PDF of log of price</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2404/1*2vj-JoEqTsgORuijWVnSkg.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can see that most of the distributions are right-skewed with a little difference.</p>
  </li>
  <li>
    <p>The sub-category ‘handmade’ is slightly distinguishable as we can see some products in this category with log(price) of less than 2</p>
  </li>
</ul>

<p>Now let’s take a look at the violin plots of sub_category_1</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2748/1*d7kLtI9HxuPd1h5G6mgqTA.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>Looking at the violin plot, we can say that the distribution of items with ‘men’ as sub_category_1 tends to be on the pricier end whereas items with ‘handmade’ as sub_category_1 tend to be on the economical end.</li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Sub_category_2</strong></p>
</blockquote>

<p>Let’s check the statistical description of sub_category_2:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">sub_category_2</span><span class="sh">'</span><span class="p">].</span><span class="nf">describe</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*GHKDffRHGzbx6gyGbyl0CQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>sub_category_2 has 114 distinct values, let’s analyze the top 20 categories of sub_category_2.</li>
</ul>

<p>Bar graph of the top 20 categories in sub_category_2</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">sub_category_2</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">().</span><span class="nf">keys</span><span class="p">()[:</span><span class="mi">20</span><span class="p">],</span>
                <span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">sub_category_2</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">()[:</span><span class="mi">20</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">number of products</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">locs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">setp</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">bar-plot of top 20 sub_category_2</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2656/1*-WiCIllod2FpJj2z-QV88Q.png" />
    </div>
</div>

<ul>
  <li>We can see that most of the items have sub_category_2 as ‘authentic apparel’ followed by ‘Makeup’ and then ‘Tops &amp; Blouses’.</li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Sub_category_3</strong></p>
</blockquote>

<p>Let’s check the statistical description of sub_category_3:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*G1wl1JPJgrjE1ZldJaKwRQ.png" />
    </div>
</div>

<ul>
  <li>sub_category_3 has 865 distinct values, let’s analyze the histogram of the top 20 categories of sub_category_3.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2608/1*eJAARMyWMKv7mNEky4jW0A.png" />
    </div>
</div>

<ul>
  <li>We can see that most of the items have sub_category_3 as ‘Pants, Tights, Leggings’ followed by ‘Other’ and ‘Face’.</li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Brand name</strong></p>
</blockquote>

<p>This is another text type feature that denotes the brand the product belongs to. Let’s check out the statistical summary of the feature brand_name.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*xts6ErQ_Ke0IivDCUHOThw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>Here, we can see that there are a total of 4089 distinct brand names.</li>
</ul>

<p>Let’s check the histogram of the top 20 brands</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">brand_name</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">().</span><span class="nf">keys</span><span class="p">()[:</span><span class="mi">20</span><span class="p">],</span>
                <span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">brand_name</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">()[:</span><span class="mi">20</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">number of products</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">locs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">setp</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">bar-plot of top 20 brands (including products with
               unknown brand)</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2656/1*lciLuLRNEM_kynJkkLsIOw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>Note that here, ‘unknown’ represents the item with no brand specified.</p>
  </li>
  <li>
    <p>PINK, Nike, and Victoria’s Secret are the top 3 brands with most items on the website.</p>
  </li>
</ul>

<p>Let’s see the bar-plot of the top 20 brands with their mean product price.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">brand_name</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">20</span><span class="p">],</span>
                <span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">average price of products</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">locs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">setp</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">bar-plot of top 20 brands with their mean product price</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2560/1*XkUZpI9qlRiH_WBCvAjFGw.png" />
    </div>
</div>

<p>Let’s see the bar-plot of the top 20 brands with maximum product price</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2544/1*MxZyU5j5VpiG_sLch804KQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Shipping</strong></p>
</blockquote>

<p>This is a numerical categorical data type that can take 2 values, 0s or 1s
Let’s check out its statistical description.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">shipping</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*cEWev-hX6tiBJfhvDP4kJQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>There are about 22% more items with shipping as 0 than 1.</li>
</ul>

<p>Let’s compare the log of price distribution of products with different shipping.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*SfOFnHY5SpS6jhiWBhTTPQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can see that the log of price distribution of items with different shipping has a slight variance.</p>
  </li>
  <li>
    <p>The products with shipping as 1 tend to have a lower price.</p>
  </li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>item_description (text)</strong></p>
</blockquote>

<p>This is a text type feature that describes the product. Let’s take a look at some of these.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">item_description</span><span class="sh">'</span><span class="p">]</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*Ho4OUDAoOby4WAyanoUqXQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>We can see that there are a total of 1482535 of these.</li>
</ul>

<p>We’ll be using this feature as is after performing some NLP techniques which will be discussed later in this blog.
Another thing that we can do with this feature is, calculate it’s word-length i.e. the number of words this feature contains for each product and do some analysis on that.
Let’s check the statistical summary of the word_length of the item description.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">item_description_word_length</span><span class="sh">'</span><span class="p">].</span><span class="nf">describe</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*YKmRGP0MxxMows0yZyQhKA.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>We can see that the longest description has 245 words and the shortest has no words. On average the words are around 25</li>
</ul>

<p>Let’s plot the histogram of item_description_word_length,</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">item_description_word_length</span><span class="sh">'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">item_description_word_length</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">frequency</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">histogram of item_description_word_length</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*fRm22uuLM42OFIh5iSz5ng.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can see that the histogram of word length follows a power-law distribution.</p>
  </li>
  <li>
    <p>I’ve used 200 bins for this histogram.</p>
  </li>
</ul>

<p>Let’s try to convert this into a Normal distribution by taking the log of the word length. Here is what the distribution looks like.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">log_item_description_word_length</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">log(item_description_word_length + 1)</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">frequency</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">histogram of log of item_description_word_length</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*zBS-98suZ_EvyBD7yuzqJw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can see that this feature tries to follow a Normal distribution.</p>
  </li>
  <li>
    <p>Most of the items have a word length between 5 and 20. (values obtained from antilog).</p>
  </li>
  <li>
    <p>We can use this as a feature for modeling.</p>
  </li>
</ul>

<p>Now let’s see how the log(item_word_length) affects the price of the item</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*9q81XPQaqbcThxuiMpna3Q.png" />
    </div>
</div>

<ul>
  <li>
    <p>We can see that the log of price increases as the item_word_length goes from 0 to 50 but then the prices tend to come down except the spike that we can observe near word length of around 190.</p>
  </li>
  <li>
    <p>Also, the prices are much more volatile for word length more than 100.</p>
  </li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Name of the product</strong></p>
</blockquote>

<p>Finally, let’s check out the last feature that is the name of the product. This is also a text type feature and we’ll be performing NLP on it later but first, let’s do some analysis on it by plotting the histogram of the number of words in the ‘name’ feature.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">name_length</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">name_length</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">frequency</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">histogram of name_length</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*a6FJNlDpETi4rWfojTozrQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>The distribution is visibly left-skewed and maximum items have a name length of about 25.</li>
</ul>

<p>Let’s see how the prices vary with the number of words in the product’s name.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre>    <span class="n">df</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="sh">'</span><span class="s">name_length</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">price_log</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">().</span><span class="nf">reset_index</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">relplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">name_length</span><span class="sh">"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">price_log</span><span class="sh">"</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="sh">"</span><span class="s">line</span><span class="sh">"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*IfkJ_h-YkQIsjapC9RyWMA.png" />
    </div>
</div>

<ul>
  <li>
    <p>Note that I’m using the log of prices instead of the actual prices.</p>
  </li>
  <li>
    <p>We can see that the distribution is much linear for name_length values between 10 and 38 and then there’s a sharp drop and rise.</p>
  </li>
</ul>

<p><img src="" alt="" />
<strong><em>Data preprocessing</em></strong></p>

<p>In this step, we’ll be cleaning the data and make it ready for modeling.
Remember that we have 6 features out of which, we have:</p>
<ul>
  <li>4 text features: Name, description, brand name, and category</li>
  <li>2 categorical features: shipping and the item_condition_id</li>
</ul>

<p>Let’s start by cleaning the text features and for that, we’ll define some functions-</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="code"><pre>  <span class="kn">import</span> <span class="n">re</span>
  <span class="k">def</span> <span class="nf">decontracted</span><span class="p">(</span><span class="n">phrase</span><span class="p">):</span>
      <span class="c1"># specific
</span>      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">won</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">will not</span><span class="sh">"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">can\'t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">can not</span><span class="sh">"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="c1"># general
</span>      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">n\'t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">not</span><span class="sh">"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'re</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> are</span><span class="sh">"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'s</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> is</span><span class="sh">"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'d</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> would</span><span class="sh">"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'ll</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> will</span><span class="sh">"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> not</span><span class="sh">"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'ve</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> have</span><span class="sh">"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'m</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> am</span><span class="sh">"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">phrase</span>
  
</pre></td></tr></tbody></table></code></pre></figure>

<p>The function works by decontracting words like “we’ll” to “we will”, “can’t” to “cannot”, “we’re” to “we are” etc. This step is necessary because we do not want our model to treat phrases like “we’re” and “we are” differently.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">stopwords</span><span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">i</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">me</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">my</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">myself</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">we</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">our</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ours</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ourselves</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">you</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">you</span><span class="sh">'</span><span class="s">re</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">you</span><span class="sh">'</span><span class="s">ve</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">you</span><span class="sh">'</span><span class="s">ll</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">you</span><span class="sh">'</span><span class="s">d</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">your</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">yours</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">yourself</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">yourselves</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">he</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">him</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">his</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">himself</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">she</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">she</span><span class="sh">'</span><span class="s">s</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">her</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">hers</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">herself</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">it</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">it</span><span class="sh">'</span><span class="s">s</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">its</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">itself</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">they</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">them</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">their</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">theirs</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">themselves</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">what</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">which</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">who</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">whom</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">this</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">that</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">that</span><span class="sh">'</span><span class="s">ll</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">these</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">those</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">am</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">is</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">are</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">was</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">were</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">be</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">been</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">being</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">have</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">has</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">had</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">having</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">do</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">does</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">did</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">doing</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">an</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">the</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">and</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">but</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">if</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">or</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">because</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">as</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">until</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">while</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">of</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">at</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">by</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">for</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">with</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">about</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">against</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">between</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">into</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">through</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">during</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">before</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">after</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">above</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">below</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">to</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">from</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">up</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">down</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">out</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">on</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">over</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">under</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">again</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">further</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">then</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">once</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">here</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">there</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">when</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">where</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">why</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">how</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">all</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">any</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">each</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">few</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">more</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">most</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">other</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">some</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">such</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">only</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">own</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">so</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">than</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">too</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">very</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">s</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">t</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">can</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">will</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">just</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">don</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">don</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span><span class="sh">'</span><span class="s">should</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">should</span><span class="sh">'</span><span class="s">ve</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">now</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">d</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ll</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">m</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">re</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">ve</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">ain</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">aren</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">aren</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span><span class="sh">'</span><span class="s">couldn</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">couldn</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span><span class="sh">'</span><span class="s">didn</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">didn</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">doesn</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">doesn</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">hadn</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">hadn</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">hasn</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">hasn</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">haven</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">haven</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">isn</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">isn</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span><span class="sh">'</span><span class="s">ma</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mightn</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">mightn</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">mustn</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">mustn</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">needn</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">needn</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span><span class="sh">'</span><span class="s">shan</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">shan</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span><span class="sh">'</span><span class="s">shouldn</span><span class="sh">'</span><span class="p">,</span><span class="sh">"</span><span class="s">shouldn</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">wasn</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">wasn</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">weren</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">weren</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">won</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">won</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">wouldn</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">wouldn</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">•</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">❤</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">✨</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">$</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">❌</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">♡</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">☆</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">✔</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">⭐</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">✅</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">⚡</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">‼</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">—</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">▪</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">❗</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">■</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">●</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">➡</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">⛔</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">♦</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">〰</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">×</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">⚠</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">°</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">♥</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">★</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">®</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">·</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">☺</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">–</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">➖</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">✴</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">❣</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">⚫</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">✳</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">➕</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">™</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ᴇ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">》</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">✖</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">▫</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">¤</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">⬆</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">⃣</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ᴀ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">❇</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ᴏ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">《</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">☞</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">❄</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">»</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ô</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">❎</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ɴ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">⭕</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ᴛ</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">◇</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ɪ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">½</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ʀ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">❥</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">⚜</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">⋆</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">⏺</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">❕</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ꕥ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">：</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">◆</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">✽</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">…</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">☑</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">︎</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">═</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">▶</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">⬇</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ʟ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">！</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">✈</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">�</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">☀</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ғ</span><span class="sh">'</span><span class="p">]</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>In the above code block, I’ve defined a list containing the stop words. Stop words are words that do not add much semantic or literal meaning to sentences. Most of these are contracted representations of words or not so important words like ‘a’, ‘at’, ‘for’ etc, and symbols.</p>

<p>Now we’ll define a function that takes the sentences, and uses the deconcatenated function and stopwords list to clean and return processed text.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre>      <span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm_notebook</span>
      <span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text_data</span><span class="p">):</span>
        <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># tqdm is for printing the status bar
</span>        <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nf">tqdm_notebook</span><span class="p">(</span><span class="n">text_data</span><span class="p">):</span>
          <span class="n">sent</span> <span class="o">=</span> <span class="nf">decontracted</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
          <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="se">\\</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
          <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="se">\\</span><span class="s">n</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
          <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="se">\\</span><span class="sh">"'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
          <span class="n">sent</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">[^A-Za-z0-9]+</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="n">sent</span><span class="p">)</span>
          <span class="n">sent</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span> <span class="k">if</span> <span class="n">e</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span>
                          <span class="n">stopwords</span><span class="p">)</span>
          <span class="n">preprocessed_text</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">sent</span><span class="p">.</span><span class="nf">lower</span><span class="p">().</span><span class="nf">strip</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">preprocessed_text</span>
      
</pre></td></tr></tbody></table></code></pre></figure>

<p>Time to clean our text data using preprocess_text() function.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre>   <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span> <span class="o">+</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span> <span class="o">+</span>
                          <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">brand_name</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span>
   <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">name</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>

   <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">item_description</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span><span class="o">+</span>
                          <span class="sh">'</span><span class="s"> </span><span class="sh">'</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">category_name</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="sh">''</span><span class="p">))</span>
   <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">text</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>

   <span class="n">df_test</span><span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span> <span class="o">+</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span>
                     <span class="o">+</span> <span class="n">df_test</span><span class="p">[</span><span class="sh">'</span><span class="s">brand_name</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span>
   <span class="n">df_test</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="sh">'</span><span class="s">item_description</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span> <span class="o">+</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span>
                      <span class="o">+</span> <span class="n">df_test</span><span class="p">[</span><span class="sh">'</span><span class="s">category_name</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="sh">''</span><span class="p">))</span>
   
</pre></td></tr></tbody></table></code></pre></figure>

<p>Note that the df[‘name’] column contains both ‘name’ and ‘brand_name’ features concatenated and preprocessed, similarly df[‘text’] feature contains ‘item_description’ and ‘category_name’ features concatenated and preprocessed.</p>

<p>Let’s proceed to the further processes but before that, we need to split the data into train and cross-validation sets. Also, we’ll be converting the target values i.e. the prices into log form so that they are normally distributed and the RMSLE(root mean squared log error) is easy to compute.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>    <span class="n">df</span> <span class="o">=</span>  <span class="n">df</span><span class="p">[[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">shipping</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">item_condition_id</span><span class="sh">'</span><span class="p">]]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">shipping</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">item_condition_id</span><span class="sh">'</span><span class="p">]]</span>

    <span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
    <span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

    <span class="n">y_scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_cv</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_cv</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                   <span class="n">test_size</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">y_train_std</span> <span class="o">=</span>  <span class="n">y_scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log1p</span><span class="p">(</span><span class="n">y_train</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now it’s time to convert these preprocessed text features into a numerical representation. I’ll be using TF-IDF vectorizer for this process. We’ll start with the feature ‘name’</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre>    <span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span> <span class="k">as</span> <span class="n">Tfidf</span>

    <span class="n">tfidf</span> <span class="o">=</span> <span class="nc">Tfidf</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">350000</span><span class="p">,</span> <span class="n">token_pattern</span><span class="o">=</span><span class="sh">'</span><span class="s">\w+</span><span class="sh">'</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># using only top 350000 tf-idf features (with bi-grams).
</span>    <span class="n">X_tr_name</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">X_cv_name</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_cv</span><span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">X_test_name</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">])</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>Next comes the feature ‘text’</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre>    <span class="n">tfidf</span> <span class="o">=</span> <span class="nc">Tfidf</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">350000</span><span class="p">,</span> <span class="n">token_pattern</span><span class="o">=</span><span class="sh">'</span><span class="s">\w+</span><span class="sh">'</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span> <span class="c1"># using only top 350000 tf-idf features (with tri-grams).
</span>    <span class="n">X_tr_text</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">X_cv_text</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_cv</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">X_test_text</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">])</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>Let’s also process the remaining categorical features starting with ‘shipping’
since this feature takes only 2 values 0 and 1, we do not need to perform some special kind of encoding for these, let’s keep them as they are.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">sparse</span>
    <span class="n">X_tr_ship</span> <span class="o">=</span>
             <span class="n">sparse</span><span class="p">.</span><span class="nf">csr_matrix</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="sh">'</span><span class="s">shipping</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">X_cv_ship</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="nf">csr_matrix</span><span class="p">(</span><span class="n">X_cv</span><span class="p">[</span><span class="sh">'</span><span class="s">shipping</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">X_test_ship</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="nf">csr_matrix</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="sh">'</span><span class="s">shipping</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>The second categorical feature that also happens to be an ordinal feature is ‘item_condition_id’. Remember these can take 5 integer values (1–5) so we’ll also keep these as they are.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>    <span class="n">X_tr_condition</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="nf">csr_matrix</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="sh">'</span><span class="s">item_condition_id</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span>
    <span class="n">X_cv_condition</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="nf">csr_matrix</span><span class="p">(</span><span class="n">X_cv</span><span class="p">[</span><span class="sh">'</span><span class="s">item_condition_id</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span>
    <span class="n">X_test_condition</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="nf">csr_matrix</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="sh">'</span><span class="s">item_condition_id</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>Notice that I’ve used -1 because this feature contains 5 types of values between (1–5) so -1 converts them to a range of (0–4). This will give us an advantage while converting to sparse data.</p>

<p>Now as the final step, we’ll be stacking these features column-wise.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3052/1*NaNK4CSTlS6OcNde3_Y7-A.png" />
    </div>
</div>

<p>I will now convert this preprocessed data into a binary form in which the values will only be either 1s or 0s.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>    <span class="n">X_tr_binary</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_tr</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">X_cv_binary</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_cv</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">X_test_binary</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_test</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>The advantage of this step is that now we’ll be having 2 datasets with a good variance to work on.</p>

<p><img src="" alt="" />
<strong><em>Modeling</em></strong></p>

<p>It’s time for testing some models on our data. The models that we’ll be trying are-</p>
<ul>
  <li>Ridge regressor</li>
  <li>Linear SVR</li>
  <li>SGD Regressor</li>
  <li>Random Forest Regressor</li>
  <li>Decision Tree Regressor</li>
  <li>XGBoost Regressor</li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Ridge Regressor on normal data</strong></p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*eeIvlwkMNG1wSmj3FR6M2g.gif" />
    </div>
</div>

<p>We use linear regression to find the optimal hyperplane (the red line in the above gif) such that the <strong>loss</strong> or square of the sum of distances of each point from the plane/line is minimum. We can notice that the loss will be minimum if we consider the line obtained at iterations=28.
Ridge regression is also known as Linear Regression with L2 Regularization which means it uses the sum of the square of weights as a penalty. The penalty term is added to restrict the model from overfitting (capturing noise).
The Ridge regression has just 1 hyperparameter <strong>λ</strong> that is multiplied with the penalty/regularization term and it decides the degree of underfitting the model undergoes. The greater the value of λ, the more we under-fit.
alpha is simply the regularization strength and it must be a positive float. So as alpha increases, the underfitting also increases.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2604/1*N1cZF0vMWfRxbVadoXRPtQ.png" />
    </div>
</div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">train loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">test loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha VS RMSLE-loss plot</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Hyperparameter: alpha</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">RMSLE loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xscale</span><span class="p">(</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*AOCULXcFu5oHAtQOuUbW0A.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can observe that as alpha decreases, the model starts overfitting.</p>
  </li>
  <li>
    <p>The test loss is minimum at alpha=1.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3488/1*-guTkXjJVsoTAUnWqAzXXA.png" />
    </div>
</div>

<p>Okay, so our Ridge returned a loss of 0.4232 on cv data.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Ridge Regressor on binary data</strong></p>
</blockquote>

<p>Now we’ll be using the Ridge regressor on the binary data</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">train loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">test loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha VS RMSLE-loss plot (on binary features)</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Hyperparameter: alpha</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">RMSLE loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xscale</span><span class="p">(</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*u9RLEGtoW6t8CNAdP7zy-Q.png" />
    </div>
</div>

<ul>
  <li>We can observe that the loss is minimum at alpha=100.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3256/1*jfkNabTLvD_u3bBkTzqA9w.png" />
    </div>
</div>

<p>Our Ridge regressor returned a loss of 0.4335 on cv data.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Let’s Try SGD-Regressor (as SVR) on Binary data</strong></p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2400/1*wsBakfF2Geh1zgY4HJbwFQ.gif" />
    </div>
</div>

<p><img src="" alt="" />
Let’s quickly refresh what SGD is and how it works. Remember the loss that I mentioned in Ridge regression? Well, there are different types of losses, let’s understand this geometrically. If a regression problem is all about finding the optimal hyperplane that best fits our data, a loss simply means how much our data differs from the hyperplane. So, a low loss means that the points don’t differ much from our hyperplane and the model performs well and vice-versa.
In the case of linear regression, the loss is a squared loss and it is obtained by taking the sum of squared distances of data points from the hyperplane divided by the number of terms.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*RRHnDdeeoCMg52HZ.png" />
    </div>
</div>

<p><img src="" alt="" />
Loss functions are important because they define what the hyperplane will be like. There are other algorithms called the Gradient Descent that make use of these loss-functions and update the parameters of the hyperplane such that it perfectly fits the data. The goal here is to minimize the loss. SGD is one optimized algorithm that updates the parameters of the hyperplane by reducing the loss step by step. It is done by calculating the gradient of the loss function with respect to the features and then using those gradients to descent towards the minima. In the above diagram (left part), we can see how the algorithm is reaching the minima of loss function by taking the right step downhill, and with each step in the correct direction, the parameters are getting updated which leads to a better fitting hyperplane (right part). To know more about the Stochastic Gradient Descent (SGD) algorithm you can check <a href="https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/">this wonderful blog</a>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*xSejSNMd5xAXxE5M.gif" />
    </div>
</div>

<p>Here are some other common losses but we’ll be using ‘Huber’, ‘epsilon_insensitive’, and ‘squared_epsilon_insensitive’ for the hyperparameter tuning of this model.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2480/1*I__GS6M_0shj0oEMEJfNBQ.png" />
    </div>
</div>

<p>The random search cross-validation tells us that ‘squared_epsilon_insensitive’ loss with L2 regularization works best for this data. By the way, ‘squared_epsilon_insensitive’ loss is one of the losses used by another well-known machine learning algorithm Support Vector Machine which uses margin maximization technique by making the use of support vectors to generate a better fitting hyperplane.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*6oNO9vFNwBL91iQL.jpeg" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>In this diagram, the dotted lines are called decision boundaries, and the points lying on the dotted lines are called support vectors and the objective of SVR is to maximize the distance between these decision boundaries.</li>
</ul>

<p>But why is margin maximization so important that it makes SVM one of the top ML algorithms? Let’s quickly understand this using a simple classification problem where we need to find an optimal hyperplane that separates the blue and red points.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*A_zxELRrMOraaHyV.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>Look at the two planes in the figure denoted by the names <em>Hyperplane</em> and <em>Optimal Hyperplane</em>. Well anyone can tell that the <em>Optimal Hyperplane</em> is much better at separating the blue and red points than the other plane and using SVM, this <em>optimal hyperplane</em> is almost guaranteed.</li>
</ul>

<p>One fun fact is that the flat bottom part in the ‘squared_epsilon_insensitive’ loss is because of this margin maximization trick. You can refer to <a href="https://medium.com/coinmonks/support-vector-regression-or-svr-8eb3acf6d0ff">this</a> and <a href="https://en.wikipedia.org/wiki/Support_vector_machine">this blog</a> to learn more about SVR.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3872/1*xOwShgO7BXHzmdVWyTrMLw.png" />
    </div>
</div>

<p>SGD Regressor (as SVR) returned a loss of 0.4325… on cv data.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Let’s try SGD regressor (as linear regressor) on binary data</strong></p>
</blockquote>

<p>Here we’ll be performing all the previous step procedures but on binary data.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2220/1*OpMLYa_iEokmFj8e4TGMaw.png" />
    </div>
</div>

<p>The random search cross-validation tells us that ‘squared_loss’ loss with L2 regularization works best for this data. By the way, this setup of squared_loss with L2 regularization sounds familiar right? This is exactly what we used in the Ridge regression model. Here we are approaching this from an optimization problem’s perspective because SGDRegressor gives us much more hyperparameters to play around with and fine-tune our model.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3636/1*fNDaUXTXcyFWSfKpOKnW9w.png" />
    </div>
</div>

<p>SGD Regressor (as linear regressor) returned a loss of 0.4362 on cv data.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Linear SVR on normal data</strong></p>
</blockquote>

<p>Let’s try Support Vector Regressor on normal data. The hyperparameter here is C that is also the reciprocal of alpha which we discussed in Ridge regression.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">train loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">test loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha VS RMSLE-loss plot (on binary features)</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Hyperparameter: C (1/alpha)</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">RMSLE loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xscale</span><span class="p">(</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*4-J5Ji1ADJ27YFz0u2lMlQ.png" />
    </div>
</div>

<ul>
  <li>We can see that 0.1 is the best hyperparameter value of hyperparameter C that gives us the minimum test loss.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3444/1*L35e-55NWoVP3OjtwJNSJg.png" />
    </div>
</div>

<p>Linear SVR returned a loss of 0.4326 on the CV of normal data.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Linear SVR on binary data</strong></p>
</blockquote>

<p>Now we’ll try Support Vector Regressor on binary data. The hyperparameter here is again C that is also the reciprocal of alpha which we discussed in Ridge regression.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">train loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">test loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha VS RMSLE-loss plot (on binary features)</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Hyperparameter: C (1/alpha)</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">RMSLE loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xscale</span><span class="p">(</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*2XsJL6XbIwDKWNpesnFvfg.png" />
    </div>
</div>

<ul>
  <li>We can see that 0.01 is the best hyperparameter value of hyperparameter C that gives us the minimum test loss.</li>
</ul>

<p>Linear SVR returned a loss of 0.4325 on the cv of binary data.</p>

<p><img src="" alt="" /></p>
<h3 id="tree-based-models">Tree-based models</h3>

<p><strong><em>The tree-based models below were taking too much time to fit (more than 60 mins) so I reduced the features using Ridge regressor on binary data.</em></strong></p>

<p>Note: Another dimensionality technique that I tried was truncated-SVD but it required a lot of RAM (more than 16 GB) for computation and since this is a kernel challenge, using the complete data didn’t make much sense.</p>

<p><strong><em>Selecting top features for tree-based models:</em></strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>    <span class="kn">from</span> <span class="n">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectFromModel</span>

    <span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>
    <span class="n">regressor</span> <span class="o">=</span> <span class="nc">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">selection</span> <span class="o">=</span> <span class="nc">SelectFromModel</span><span class="p">(</span><span class="n">regressor</span><span class="p">)</span>
    <span class="n">selection</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_tr_binary</span><span class="p">,</span> <span class="n">y_train_std</span><span class="p">.</span><span class="nf">ravel</span><span class="p">())</span>

    <span class="n">X_train_top</span> <span class="o">=</span> <span class="n">selection</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_tr_binary</span><span class="p">)</span>
    <span class="n">X_cv_top</span> <span class="o">=</span> <span class="n">selection</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_cv_binary</span><span class="p">)</span>
    <span class="n">X_test_top</span> <span class="o">=</span> <span class="n">selection</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test_binary</span><span class="p">)</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Decision Tree</strong></p>
</blockquote>

<p>Our first tree-based model is a Decision Tree, before using this on our dataset, let’s first quickly understand how it works.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://miro.medium.com/proxy/0*cant-HQdfMju-GxG" />
    </div>
</div>

<p><img src="" alt="" />
Decision Trees are made up of simple if-else statements and using these conditions they decide how to predict the price of a product given its name, conditioning, etc. Geometrically speaking, they fit on the data using several hyperplanes that are parallel to the axes.
While training a tree, the tree learns these if-else statements by using and verifying the train data. And when it is trained, it uses these learned if-else conditions to predict the value of test data.
But how does it decide how to split the data or what feature to consider while splitting the data and construct a complete tree?
Well, it uses something called entropy which is a measure of certainty to construct the tree.
Decision trees have several hyperparameters but we’ll be considering only the 2 important ones-</p>
<ul>
  <li><em>max_depth:</em> It denotes the maximum depth of a decision tree. So if the max_depth is supposed 4, while training, the tree constructed will not have a depth more than 4.</li>
  <li><em>min_samples_split:</em> It denotes the minimum number of data points that must be present to perform a split or consider an if-else condition on it. So if the min_samples_split is supposed 32, while training, the tree constructed will not apply an if-else condition if it sees less than 32 data points.</li>
</ul>

<p>Both the above hyperparameters restrict a decision tree from either underfitting or overfishing. A high max_depth and a low min_samples_split value makes decision trees more prone to overfitting and vice-versa.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*DzCdcC-xomLcLd1B.png" />
    </div>
</div>

<ul>
  <li>
    <p>In this figure, we can see how a trained decision tree algorithm tries to fit on the data, notice how the fitting lines are made up of axes parallel lines.</p>
  </li>
  <li>
    <p>We can also notice how a decision tree with a greater value of max_depth is prone to capture noisy points also.</p>
  </li>
</ul>

<p>I will not go into the internal working of decision trees in this blog since it will make it too long, to learn more about the internal working of a decision tree, you can check out this awesome <a href="https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8">blog</a>.</p>

<p>Let’s perform some hyperparameter tuning on our decision tree using RandomSearchCV and check what are the best hyperparameters for our tree.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2988/1*_ZinFL3ECLWXGNC6Lo768A.png" />
    </div>
</div>

<p>The best hyperparameter values returned are max_depth=64 and min_samples_split = 64. Now let’s check the loss obtained after training a decision tree on these hyperparameters.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3736/1*0cbEiS0XSgNC82nrDjfkaw.png" />
    </div>
</div>

<p>The loss values are not that great given that it took 14 mins to train. Our Linear models have outperformed the decision tree model by far.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Random forest — (max_depth=3, n_estimators=100)</strong></p>
</blockquote>

<p>Now let’s use another awesome tree-based model or I should say models to model our data.
Random forests are ensembles that are made up of multiple models. The idea is to use random parts of the data to train multiple models and then use the average predictions from these multiple models as the final value. This makes sense because of training several models using random parts of complete data creates models that are to some extent biased in different ways. Now by taking the average prediction from all these models, in the end, results in a better-predicted value.</p>

<p>The name Random Forest comes from Bootstrap sampling which we use in sampling data <em>randomly</em> from the training dataset and since we use multiple decision trees as our base models, it has the word <em>forest</em>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2612/0*N2HUznk6Rrtpg1iR.png" />
    </div>
</div>

<p>The above diagram denotes how Random Forest trains different base learners denoted as Tree 1, Tree 2, … using randomly sampled data and then collects and averages the predictions from these trees.</p>

<p>Random Forest has multiple hyperparameters but for our data, we’ll be using just 2:
<em>- n_estimator:</em> this denotes the number of base models that we want our random forest model to have.
<em>- max_depth:</em> This denotes the maximum depth of each base model i.e. the decision tree.</p>

<p>Let’s train a random forest model and perform some hyperparameter tuning on it.</p>

<p>The training time for this model was about 23 mins.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3136/1*8ehM9Y9BQGveU8-yBmny4A.png" />
    </div>
</div>

<p>We can see that this model does not perform well on the given dataset and the results are not at all good.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Random forest — (max_depth=4, n_estimators=200)</strong></p>
</blockquote>

<p>Here I’ve used the same model but with some changes in the architecture.
I’ve increased the max_depth to 4 and the number of base learners to 200.
Let’s see how the model performs.</p>

<p>The training time for this model was about 65 mins.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2988/1*7_d_JkciaKqEK7U04Zyzeg.png" />
    </div>
</div>

<p>The results are slightly better than the previous Random forest model but still not even close to our Linear models.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>XGBoost — (max_depth=4, n_estimators=200)</strong></p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*CVSyne5ZJ7MnDeAU.gif" />
    </div>
</div>

<p>This is the final tree-based model that we’ll be trying and it is called XGBoost.
XGBoost is a slightly enhanced version of GBDT which again is an ensemble modeling technique. In Gradient boosting, the purpose is to reduce the variance or reduce the underfitting behavior on a dataset. Let’s see how it works.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*ctuJT4dyWDY18QCm.png" />
    </div>
</div>

<p>In GBDT, we start by training our first base model which is typically a high bias decision tree using the train data, then we take the predicted values from this model and calculate the error which is defined by how much the predictions differ from the actual values. Now we train our second base learner but this time instead of using only the train data, we use also use the error obtained from our first base learner and again we take the predicted values from this model and calculate the error. This goes on till all the base learners are covered and as we train the base learners one by one, we notice that the error value slowly diminishes. You can read more about GBDT <a href="http://uc-r.github.io/gbm_regression">here</a>.
XGBoost is a slightly modified version of GBDT and it uses techniques like row sampling and column sampling which are techniques from Random Forest to construct the base-learners.</p>

<p>Let’s quickly check out the code for XGBoost, I’ll be using 2 hyperparameters:</p>
<ul>
  <li>n_estimators: which denotes the number of base-learners which are decision tree models.</li>
  <li>max_depth: which denotes the maximum depth of the base learner decision tree.</li>
</ul>

<p>The model took about 27 mins to train.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3012/1*GpBSeek8lu_plTioQc1r0Q.png" />
    </div>
</div>

<p>The results are not as bad as random forest but not as good as linear models also.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>XGBoost — (max_depth=6, n_estimators=500)</strong></p>
</blockquote>

<p>Let’s try XGBoost with max_depth=6 and n_estimators=500.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3400/1*Fw1yJ3iF2rCJAn1O5vz9_Q.png" />
    </div>
</div>

<p>We can see a decent amount of improvement from the previous model but it took the model 78 mins to train.</p>

<h3 id="lets-compare-the-different-models-and-their-performance">Let’s compare the different models and their performance:</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4636/1*P--evq3wbxVgY9Tj7787OQ.png" />
    </div>
</div>

<p><img src="" alt="" />
In the above table, we can see that the tree-based models are taking too much time to compute, in fact, the data I’m using for tree-based is much smaller, I’m using only the top selected binary features from Ridge regressor. So the new data has only around 236k features instead of the original 700k that other linear models are trained on. We can also observe that the minimum loss on cross-validation data that we were able to obtain is 0.4232… let’s try to reduce this further using ensemble modeling.</p>

<p><strong>The Linear models have outperformed other tree-based models so I’ll be using these to create an Ensemble.</strong></p>

<p>Let’s concatenate the results obtained from the top 6 linear models.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3776/1*eAS9FC7SP5MMTqR4eOnG_Q.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4440/1*QrZtzLNhER9TszchVkSq5w.png" />
    </div>
</div>

<p>Now let’s quickly test a simple ensemble that takes these features as input and computes the output as a mean of these values.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3148/1*dg9VBOEZjOnmC7TFwOpGXA.png" />
    </div>
</div>

<p>We can observe that the loss has increased slightly, which means that this method alone is not decent enough to produce good scores.</p>

<p>Now let’s check the correlation between these new features because all of them are from linear models and produce a similar loss. If they are heavily correlated, they will not improve the overall loss much.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
    <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">y_pred_ridge_binary_tr</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y_pred_ridge_normal_tr</span><span class="sh">'</span><span class="p">,</span>
               <span class="sh">'</span><span class="s">y_pred_svr_normal_tr</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">y_pred_svr_binary_tr</span><span class="sh">'</span><span class="p">,</span>
               <span class="sh">'</span><span class="s">y_pred_sgd_lr_binary_tr</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y_pred_sgd_svr_binary_tr</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">y_pred_tr_ensemble</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">Var_Corr</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">corr</span><span class="p">()</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">Var_Corr</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">Var_Corr</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span>
    <span class="n">yticklabels</span><span class="o">=</span><span class="n">Var_Corr</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Correlation between different features.</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2432/1*w4KjE92siYvJcKt61675KQ.png" />
    </div>
</div>

<ul>
  <li>We can see that the results from the underlying models are heavily correlated so there isn’t much scope of getting a marginally well score from building an ensemble on them.</li>
</ul>

<p>To tackle this, I increased the dimensionality of this data by adding the top features that were gathered from the Linear model on binary data that we used to train the tree-based models.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3372/1*DxQH9MNQ5TsMzAS50wiysw.png" />
    </div>
</div>

<p>It’s time to try different models on these newly generated features to see if we can improve the loss.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Let’s try SGD Regressor using different hyperparameters</strong></p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2684/1*YI8aWb3zJL_p67qa_FmJHw.png" />
    </div>
</div>

<p>The above code block represents the best hyperparameters returned by RandomSearchCV.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3444/1*s24NGK5ZOPD1mOvflcQA_w.png" />
    </div>
</div>

<p>The CV loss is not up to the mark since we already have a loss of 0.4232… and we are looking for a loss lower than that.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Let’s try Linear SVR and Ridge regressor on the new features</strong></p>
</blockquote>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">ridge_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">ridge_loss</span><span class="p">)</span>
    <span class="n">linearsvr_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">linearsvr_loss</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">ridge_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Ridge train</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">ridge_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Ridge test</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">linearsvr_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">linearsvr train</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">linearsvr_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">linearsvr test</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Hyperparameter: alpha or (1/C)</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xscale</span><span class="p">(</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Linear SVR and Ridge losses</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*yaalDojG9iH1xgmnkYI89A.png" />
    </div>
</div>

<ul>
  <li>We can see that at alpha=100000, the cv loss returned by Ridge regressor and Linear SVR are minimum. Let’s fit the models on that.</li>
</ul>

<p>Training a Ridge regressor with alpha = 100000</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3228/1*Tdz-S5tXH1WjRklGmA9xjA.png" />
    </div>
</div>

<p>Training a Linear SVR with C = 0.00001</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3268/1*-0KihDZg6yACp0SJoVs9zg.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3724/1*Jcy10PEYybzNB06Lmuo03Q.png" />
    </div>
</div>

<p><img src="" alt="" />
Okay, by looking at the above table we can tell that the Ridge and LinearSVR models yield the best results, so we’ll be using these to generate one more and the final layer of our ensemble.</p>

<p>Let’s quickly fit the data using these models and concatenate the output that we’ll feed as input to our final ensemble layer.</p>

<p>We’ll now create the final layer of our ensemble using the generated output from the previous layer models. We’ll be using some linear models but before that, let’s test the simple mean results.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3344/1*s2SWaJ2bbnKGvM6yMHX9zg.png" />
    </div>
</div>

<p>The results are better than the LinearSVR model alone but the Ridge still outperforms every model till now.</p>

<p>Let’s try some linear models now for the final layer:</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>SGD Regressor</strong></p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2432/1*UnARXx06nsf4Ipi8IljAMQ.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4188/1*rlb1u_EZ5kasq7Cg0gCvGw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Let’s try Ridge and Linear-SVR as the final layer model</strong></p>
</blockquote>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">ridge_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">ridge_loss</span><span class="p">)</span>
    <span class="n">linearsvr_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">linearsvr_loss</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">ridge_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Ridge train</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">ridge_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Ridge test</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">linearsvr_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">linearsvr train</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">linearsvr_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">linearsvr test</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Hyperparameter: alpha or (1/C)</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xscale</span><span class="p">(</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Linear SVR and Ridge losses</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*KiLPm2CRbWW-auX7ua3Vgg.png" />
    </div>
</div>

<ul>
  <li>The results are close but the Ridge regressor outperforms LinearSVR.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3360/1*CpaKlAOcl47fhvVkVGyGXg.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3720/1*uWD04L7O3hi423rS4sFc6Q.png" />
    </div>
</div>

<p><img src="" alt="" />
<strong>Here are all the models that have been used for the ensemble, compared in a tabular form.</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3736/1*CyA0T6y9RSgWS0a3JFp5ZQ.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3952/1*QLU58lo_X3qE_HLb70MTmg.png" />
    </div>
</div>

<p><img src="" alt="" />
<strong><em>Finally, let’s predict the prices of the test dataset and check how our ensemble performs on the Kaggle leaderboard.</em></strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3680/1*Q4xHaJNty0OCsrqN4f_ecQ.png" />
    </div>
</div>

<h3 id="after-submitting-the-predicted-results-i-was-able-to-obtain-a-score-of-042457-that-corresponds-to-the-top-6-on-the-kaggle-leaderboard">After submitting the predicted results, I was able to obtain a score of 0.42457 that corresponds to the top 6% on the Kaggle leaderboard.</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4076/1*m8PVQnO4QZhA6hev8Lfl2w.png" />
    </div>
</div>

<p><img src="" alt="" /></p>

<p><strong><em>Future work</em></strong></p>

<ul>
  <li>
    <p>The problem can be solved using deep learning algorithms like GRU, MLP, BERT as most of the features are from text descriptions. We can try these state of the art techniques on this data and try to improve the scores.</p>
  </li>
  <li>
    <p>The code has not been optimized for multiprocessing such that it uses all the 4 CPUs. I think that is worth a try because then even the Tree-based models could be added to the ensemble.</p>
  </li>
</ul>

<p><strong><em>References</em></strong></p>

<ul>
  <li>
    <p><a href="https://www.kaggle.com/c/mercari-price-suggestion-challenge/discussi">https://www.kaggle.com/c/mercari-price-suggestion-challenge/discussi</a> on/50256</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=QFR0IHbzA30">https://www.youtube.com/watch?v=QFR0IHbzA30</a></p>
  </li>
  <li>
    <p><a href="https://youtu.be/_PwhiWxHK8o">https://youtu.be/_PwhiWxHK8o</a></p>
  </li>
  <li>
    <p><a href="https://youtu.be/UHBmv7qCey4">https://youtu.be/UHBmv7qCey4</a></p>
  </li>
  <li>
    <p><a href="https://www.appliedaicourse.com/">https://www.appliedaicourse.com/</a></p>
  </li>
</ul>

<p><strong><em>Final note</em></strong></p>

<p>Thank you for reading the blog. I hope it was useful for some of you aspiring to do projects on machine-learning, ensemble modeling, data processing, data visualizing.</p>

<p>And if you have any doubts regarding this project, please leave a comment in the response section or in the GitHub repo of this project.</p>

<p>The full project is available on my Github:
<a href="https://github.com/SarthakV7/mercari_kaggle">https://github.com/SarthakV7/mercari_kaggle</a>
Find me on LinkedIn: <a href="http://www.linkedin.com/in/sarthak-vajpayee">www.linkedin.com/in/sarthak-vajpayee</a></p>

<p>Peace! ☮</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How an ensemble of linear models got in the top 6% of Mercari price prediction challenge leaderboard on Kaggle.]]></summary></entry><entry><title type="html">How I used machine learning to strategize my GRE preparation.</title><link href="https://sarthakv7.github.io/my_folio/blog/2020/GRE-ai/" rel="alternate" type="text/html" title="How I used machine learning to strategize my GRE preparation." /><published>2020-02-21T11:10:16+00:00</published><updated>2020-02-21T11:10:16+00:00</updated><id>https://sarthakv7.github.io/my_folio/blog/2020/GRE-ai</id><content type="html" xml:base="https://sarthakv7.github.io/my_folio/blog/2020/GRE-ai/"><![CDATA[<h4 id="the-most-challenging-part-of-gre-preparation-is-the-vocabulary-part-at-least-for-me-it-was-until-my-machine-learning-model-helped-me-out-with-it"><strong>The most challenging part of GRE preparation is the vocabulary part. At-least for me it was until my machine learning model helped me out with it.</strong></h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/5040/1*OStckVJzGb02CZt1RY8csw.png" />
    </div>
</div>

<p><img src="" alt="" />
When I started with my GRE preparation, after going through many resources (for the vocab section) I found that there are some words that pretty commonly appear in the exam and Barron’s high-frequency word list is one of the renowned resources that solve this problem. To begin with, I picked Barron’s 333 which is one such word list that contains 333 most frequently occurring words in GRE. The next challenge was learning these words so I came up with a plan. If I could somehow group similar words together it would make the learning process much easier. But how to do that? Manually grouping these words would be way more challenging than simply learning the words as they are. After pondering for some time, it occurred to me why not let the machine do all the hard work! I think with a capability of <em>above one million million floating-point operations per second</em> it is much better for these types of tasks than I am so let’s get started and see how to build a model from scratch that clusters similar words together.</p>

<p>I’ll be covering several machine learning concepts like Natural Language Processing (<strong>NLP</strong>), Term Frequency-Inverse Document Frequency (<strong>TF-IDF</strong>), Singular Value Decomposition (<strong>SVD</strong>), <strong>K-Means</strong>, t-Distributed Stochastic Neighbor Embedding (<strong>t</strong>-<strong>SNE</strong>) and many other techniques for data scraping, feature engineering and data visualization to demonstrate how we can cluster data from scratch.</p>

<blockquote>
  <p><strong>Note: I’ll be using python 3.7 for this project.</strong></p>
</blockquote>

<p>The blog will be divided into the following parts-</p>

<ul>
  <li>
    <p>Data collection: scraping websites to gather the data.</p>
  </li>
  <li>
    <p>Data cleaning</p>
  </li>
  <li>
    <p>Feature engineering</p>
  </li>
  <li>
    <p>Modeling</p>
  </li>
  <li>
    <p>Visualizing the results</p>
  </li>
</ul>

<p>Now that we know the problem statement and the data flow, let’s dive in.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Scraping the data</strong></p>
</blockquote>

<p>The first task is to collect the data i.e. Barron’s 333 high-frequency words. This can be done either by manually typing the words and creating a list or by automating the process. I used BeaulifulSoup and request to create a function that automatically scraped the data from different websites, let’s briefly understand the libraries and how to use them.</p>

<ul>
  <li>
    <p><strong><em>Numpy:</em></strong> A library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.</p>
  </li>
  <li>
    <p><strong><em>Pandas:</em></strong> A library written for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables.</p>
  </li>
  <li>
    <p><strong><em>BeautifulSoup:</em></strong> A library for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.</p>
  </li>
  <li>
    <p><strong><em>Requests:</em></strong> The requests module allows you to send HTTP requests using Python. The HTTP request returns a response object with all the response data (content, encoding, status, etc).</p>
  </li>
</ul>

<p>The code will use <strong>requests</strong> to get the response from the target websites, then using <strong>BeautifulSoup</strong> it’ll parse the html response and scrape out the required information from the page(s) and store the information in a tabular format using <strong>pandas.</strong>
To understand the format of an html page, you can check out <a href="https://www.w3schools.com/html/">this tutorial</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># importing necessary libraries

import requests
from bs4 import BeautifulSoup
import re
from functools import reduce
import numpy as np
import pandas as pd
</code></pre></div></div>

<p>Let’s scrape the Barron’s 333 words and their meanings from <a href="https://quizlet.com/2832581/barrons-333-high-frequency-words-flash-cards/">this website</a>-</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre><span class="n">URL</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://quizlet.com/2832581/barrons-333-high-frequency-words-flash-cards/</span><span class="sh">"</span> <span class="c1"># url of the data we want to scrape.
</span><span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">URL</span><span class="p">)</span> <span class="c1"># request object collects server's response to the http request.
</span><span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">content</span><span class="p">,</span> <span class="sh">'</span><span class="s">html5lib</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># BeautifulSoup creates a parser tree out of the html response that was collected using request.
</span><span class="n">rows</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">div</span><span class="sh">'</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span><span class="sh">'</span><span class="s">SetPageTerm-inner</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># Looking for elements with tag='div' and class_='SetPageTerm-inner'.
</span><span class="n">dic</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span> <span class="c1"># iterating over all the elements.
</span>    <span class="n">part</span> <span class="o">=</span> <span class="n">row</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">span</span><span class="sh">'</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span><span class="sh">'</span><span class="s">TermText notranslate lang-en</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># Looking for elements with tag='span' and class_='TermText notranslate lang-en'
</span>    <span class="n">word</span> <span class="o">=</span> <span class="n">part</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span> <span class="c1"># collecting the words.
</span>    <span class="n">meaning</span> <span class="o">=</span> <span class="n">part</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">text</span> <span class="c1"># collecting the meaning.
</span>    <span class="n">dic</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">meaning</span> <span class="c1"># adding the word, meaning to dictionary as key value pairs.
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">dic</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">word</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">meaning</span><span class="sh">'</span><span class="p">])</span> <span class="c1"># converting to dataframe
</span> 
</pre></td></tr></tbody></table></code></pre></figure>

<h1 id="the-html-code-of-a-webpage-in-chrome-can-be-accessed-using-shiftc-on-mac-or-ctrlshiftc-on-windowslinux">The HTML code of a webpage in chrome can be accessed using <strong>⌘+shift+c</strong> on mac or <strong>ctrl+shift+c</strong> on windows/Linux.</h1>

<p>Here I’m using ‘span’ as tag, class_= ‘TermText notranslate lang-en’ since the elements containing word and meaning have the same tag and class and there are only 2 such elements in every row element, 1st one corresponding to word and the 2nd one to the meaning.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3852/1*TlHPuTuo-vnLXl1Y__H-KQ.png" />
    </div>
</div>

<p><img src="" alt="" />
This is the scraped data in tabular form.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*B1evIg_8BvaDpMiHYwqLPw.png" />
    </div>
</div>

<p><img src="" alt="" />
This data is not enough so let’s add more data by scraping the synonyms of each word from <a href="https://www.thesaurus.com/browse/">this website</a>-</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">synonyms</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">th</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
  <span class="n">URL</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">https://www.thesaurus.com/browse/</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="sh">"</span> <span class="c1"># this url returns the page with 'word' described
</span>  <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">URL</span><span class="p">)</span> <span class="c1"># collecting the http response from the url
</span>  <span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">content</span><span class="p">,</span> <span class="sh">'</span><span class="s">html5lib</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># parsing the html page to extract items.
</span>  <span class="n">rows</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">span</span><span class="sh">'</span><span class="p">,</span> <span class="n">class_</span> <span class="o">=</span> <span class="sh">'</span><span class="s">css-133coio etbu2a32</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># stores all the element containing synonyms of the word.
</span>  <span class="n">syn</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="p">]</span>
  <span class="nf">if</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span><span class="o">&lt;</span><span class="n">th</span><span class="p">):</span> <span class="c1"># here, th denotes the number of synonyms that we need.
</span>    <span class="n">th</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span> <span class="c1"># in some cases only limited synonyms were available so I modified the code a bit.
</span>  <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">[:</span><span class="n">th</span><span class="p">]:</span> <span class="c1"># iterating over all the elements containing synonyms
</span>    <span class="k">try</span><span class="p">:</span>
      <span class="n">syn</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">a</span><span class="p">.</span><span class="n">text</span><span class="p">)</span> <span class="c1"># scrapping synonym string and storing in a list (most of them have a tag='a')
</span>    <span class="k">except</span><span class="p">:</span>
      <span class="n">syn</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">span</span><span class="p">.</span><span class="n">text</span><span class="p">)</span> <span class="c1"># scrapping synonym string with tag='span'
</span>  <span class="k">return</span> <span class="n">syn</span>

<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm_notebook</span>
<span class="n">mapping</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># stores the word and it's synonyms
</span><span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="nf">tqdm_notebook</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">word</span><span class="p">.</span><span class="n">values</span><span class="p">):</span> <span class="c1"># iterating over the words and scrapping synonyms
</span>    <span class="n">syn</span> <span class="o">=</span> <span class="nf">synonyms</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">th</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># I'll be collecting 5 synonyms
</span>    <span class="n">mapping</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">syn</span><span class="p">)</span> <span class="c1"># storing the synonyms.
</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">mapping</span><span class="p">)</span> <span class="c1"># Converting to dataframe
</span><span class="n">data</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">word</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">synonym_1</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">synonym_2</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">synonym_3</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">synonym_4</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">synonym_5</span><span class="sh">'</span><span class="p">]</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4976/1*3jfzEZeasCMd6uKm8GDe6g.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*PLr0hPzITQY5pY5JH_XO2g.png" />
    </div>
</div>

<p><img src="" alt="" />
Now let’s join the 2 data frames (meanings and synonyms):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>result = pd.merge(df.word, data, on='word')
result.fillna('', inplace=True)
print(result)
</code></pre></div></div>

<p><img src="" alt="" /></p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2840/1*iPHWY7srA9FlwLVrLIxUWg.png" />
    </div>
</div>

<p><img src="" alt="" />
We can see the data needs some cleaning since it contains stop-words like and, or, the and other elements like punctuation marks. Also, we must take care of the contractions like can’t, won’t, don’t these must be converted to can not, would not, do not respectively.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre></td><td class="code"><pre><span class="kn">from</span> <span class="n">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
  <span class="n">stop_words</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">ourselves</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">hers</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">between</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">yourself</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">but</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">again</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">there</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">about</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">once</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">during</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">out</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">very</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">having</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">with</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">they</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">own</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">an</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">be</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">some</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">for</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">do</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">its</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">yours</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">such</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">into</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">of</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">most</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">itself</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">other</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">is</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">s</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">am</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">or</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">who</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">as</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">from</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">him</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">each</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">the</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">themselves</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">until</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">below</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">are</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">we</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">these</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">your</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">his</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">through</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">don</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">nor</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">me</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">were</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">her</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">more</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">himself</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">this</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">down</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">should</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">our</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">their</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">while</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">above</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">up</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">to</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ours</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">had</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">she</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">all</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">no</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">when</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">at</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">any</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">before</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">them</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">and</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">been</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">have</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">will</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">on</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">does</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">yourselves</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">then</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">that</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">because</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">what</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">over</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">why</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">so</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">can</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">did</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">not</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">now</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">under</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">he</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">you</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">herself</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">has</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">just</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">where</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">too</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">only</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">myself</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">which</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">those</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">i</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">after</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">few</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">whom</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">t</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">being</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">if</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">theirs</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">my</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">against</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">by</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">doing</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">it</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">how</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">further</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">was</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">here</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">than</span><span class="sh">'</span><span class="p">]</span>
  <span class="n">sentence_clean</span> <span class="o">=</span> <span class="p">[</span><span class="n">words</span> <span class="k">if</span> <span class="n">words</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span> <span class="k">else</span> <span class="sh">''</span> <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">)]</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">sentence_clean</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">;</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">\(</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">\)</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">\d</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">  </span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">won</span><span class="sh">'</span><span class="s">t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">will not</span><span class="sh">"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">can\'t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">can not</span><span class="sh">"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">n\'t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> not</span><span class="sh">"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'re</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> are</span><span class="sh">"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'s</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> is</span><span class="sh">"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'d</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> would</span><span class="sh">"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'ll</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> will</span><span class="sh">"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'t</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> not</span><span class="sh">"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'ve</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> have</span><span class="sh">"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\'m</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> am</span><span class="sh">"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">sentence</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now since the data has been cleaned, let’s do some feature engineering.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Feature engineering</strong></p>
</blockquote>

<p>One thing that I noticed was that some of these words have synonyms that also belong to Barron’s 333 list. If I could somehow concatenate the synonyms of these words, it could increase the performance of our model.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*m7RGuamAlyh_gNf_ACvY-A.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>Eg. for <strong><em>Tortuous</em></strong>, the synonyms are <em>circuitous, convoluted, indirect.</em></p>
  </li>
  <li>
    <p>Out of the 3 synonyms, <strong>convoluted</strong> is present in Barron’s 333-word list and its synonyms are <em>intricate, labyrinthine, perplexing.</em></p>
  </li>
  <li>
    <p>So, the final synonyms of <strong>tortuous</strong> should be <em>circuitous, convoluted, indirect, intricate, labyrinthine, perplexing</em>.</p>
  </li>
</ul>

<p>After this step, for each word in Barron’s 333-word list we have, it’s direct synonyms, indirect synonyms (in the above example, the synonyms of <strong>convoluted</strong> are the indirect synonyms of <strong>tortuous</strong>) and meaning. I’ll use the notation <strong>set</strong> for this data further in this blog.</p>
<blockquote>
  <p><strong>Set:</strong> data about a word (here the word is from barron’s 333) like it’s direct synonyms, indirect synonyms and meaning. The set of a word includes the word itself.</p>
</blockquote>

<p>We have obtained clean sets of words that we need to cluster but remember we first need to convert these sets into some kind of numerical data because our model needs numbers to work on.</p>

<p>I’ll use <strong>TF-IDF</strong> to vectorize the data. Before diving in let’s understand what tf-idf is-</p>

<p><strong>TF-IDF</strong> short for term frequency-inverse document frequency is a numerical statistic that is intended to reflect how important a word is to a document in a corpus. Let’s understand this using <strong>Bag of Words</strong>.</p>

<p>The <strong>bag-of-words</strong> model is a simplifying representation used in natural language processing and information retrieval. In this model, a text is represented as the bag of its words.
In simple terms, Bag of Words is nothing but a basic numerical representation of documents, it is done by first creating a <strong>vocabulary</strong> of words that contains all the distinct words from all the documents. Now each document is represented using a vector that has ‘n’ elements (here, n is the number of words in the vocabulary so each element corresponds to a word in the vocabulary) and each element has a numerical value that tells us how many times that word was seen in that document.
let’s consider an example:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*EVu8iK6PijpfCxqjhFRpaQ.jpeg" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>The column <strong>word</strong> represents the vocabulary.</p>
  </li>
  <li>
    <p>In the table, column Document 1 and Document 2 represent the BOW of documents 1 and 2 respectively.</p>
  </li>
  <li>
    <p>The numbers represent how many times the corresponding word occurs in the document.</p>
  </li>
  <li>
    <p>Now comes TF-IDF, it is simply the product of term frequency and inverse document frequency.</p>
  </li>
</ul>

<p><em><strong>Term frequency</strong>:</em> It is the number that represents how often a word is present in the document.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*rQlC66HYBiVPYOXbUl1tvQ.png" />
    </div>
</div>

<p><img src="" alt="" />
<em><strong>Inverse document frequency</strong>:</em> It is the inverse of the log of the chance of finding a document that has the word in it.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*xtgXRlC-gSmoFOIG2vrSTQ.png" />
    </div>
</div>

<p><img src="" alt="" />
Think about it this way: If the word is used extensively in all documents, its existence within a specific document will not be able to provide as much specific information about the document itself. So the second term could be seen as a penalty term that penalizes common words such as “a”, “the”, “and”, etc. tf-idf can, therefore, be seen as a weighting scheme for words relevancy in a specific document.</p>

<p>Let’s check the TF-IDF of the two documents:
Document 1: “tf stands for term frequency”, terms: 5
Document 2: “and idf stands for inverse document frequency”, terms: 7</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3056/1*CzVrAaxzaLfs0bF8ZbkLSA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2252/1*bueWekM8K0tTHXRMI_0xSg.png" />
    </div>
</div>

<p><img src="" alt="" />
<strong><em>Bi-grams:</em></strong> A bigram is a sequence of two adjacent elements from a string of tokens, which are typical letters, syllables, or words. Here is an example of uni-grams and bi-grams generated from a document.
<strong><em>doc:</em></strong> “tf stands for term frequency”
<em>uni-grams:</em> [‘tf’, ‘stands’, ‘for’, ‘term’, ‘frequency’]
<em>bi-grams:</em> [‘tf stands’, ‘stands for’, ‘for term’, ‘term frequency’]</p>

<p><em>The advantage of n-grams is that they add information about the sequence of words in a document.</em></p>

<p>I’ve written a function for calculating the TF-IDF, the function uses both uni-grams and bi-grams.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">tfidf_vectorizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_grams</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tf</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">idf</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tfidf_</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
      <span class="sh">'''</span><span class="s">This function generates the vocabulary.
      Here I</span><span class="sh">'</span><span class="s">ve used both uni-grams and bi-grams.</span><span class="sh">'''</span>
        <span class="n">uni_grams</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>
        <span class="n">bi_grams</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">rows</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">rows</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">word_pair</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">words</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
                <span class="n">uni_grams</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">bi_grams</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">word_pair</span><span class="p">)</span>
            <span class="n">uni_grams</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_grams</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">uni_grams</span><span class="p">.</span><span class="nf">union</span><span class="p">(</span><span class="n">bi_grams</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
      <span class="sh">'''</span><span class="s">This function calculates the tf values for each document, idf values for each word
      and then finally returns the tfidf values</span><span class="sh">'''</span>
        <span class="n">tf_</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">n_grams</span><span class="p">])</span>
        <span class="n">idf_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">.</span><span class="nf">fromkeys</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_grams</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">idf_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_grams</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span><span class="n">rows</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">rows</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
            <span class="n">tf</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">.</span><span class="nf">fromkeys</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_grams</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">word_pair</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">words</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
                <span class="n">tf</span><span class="p">[</span><span class="n">word_pair</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">tf</span><span class="p">[</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">idf_</span><span class="p">[</span><span class="n">word_pair</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="n">idf_</span><span class="p">[</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">tf</span><span class="p">[</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">idf_</span><span class="p">[</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">idf_list</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">idf_</span><span class="p">.</span><span class="nf">values</span><span class="p">()))</span>
            <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">values</span><span class="p">()))</span>
            <span class="n">vector</span> <span class="o">=</span> <span class="n">vector</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
            <span class="n">tf_</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector</span>
        <span class="c1"># print(idf_list)
</span>        <span class="n">idf_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">/</span><span class="n">term</span><span class="p">)</span> <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">idf_list</span><span class="p">])</span>
        <span class="n">idf_</span> <span class="o">=</span> <span class="n">nz</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">idf_</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">tfidf_</span> <span class="o">=</span> <span class="n">tf_</span><span class="p">.</span><span class="n">values</span><span class="o">*</span><span class="n">idf_</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tf</span> <span class="o">=</span> <span class="n">tf_</span>
        <span class="n">self</span><span class="p">.</span><span class="n">idf</span> <span class="o">=</span> <span class="n">idf_</span>
        <span class="k">return</span> <span class="n">tfidf_</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
      <span class="sh">'''</span><span class="s">This function performs both fit and transform</span><span class="sh">'''</span>
        <span class="n">fit_</span> <span class="o">=</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="n">transform_</span> <span class="o">=</span> <span class="nf">transform</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">transform_</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now, that we have the TF-IDF embeddings for each of the sets, we can proceed to modeling.</p>

<p>Note: The data we have till now is in a tabular form containing <strong>m rows</strong> and <strong>n columns</strong> where <strong>‘m’ is the number of words in Barron’s 333-word list and ‘n’ is the size of Bag of Words vocabulary. This tabular data can also be represented as an array of dimension (m x n).</strong> Further in the blog, I’ll be using array to represent the data.</p>

<p>Before Modeling there is something that I would like to share that I found very helpful. Instead of using the TF-IDF values directly for modeling, how about bringing down the dimensions of the data?
That means, we have TF-IDF values corresponding to each set and since these TF-IDF values are represented as a vector of n elements, where ‘n’ also corresponds to the number of distinct words in all of our sets. If 2 sets have almost the same words, the distance between there corresponding points in an n-dimensional hyperplane is going to be very less and vice-versa. Similarly, if the 2 sets have very less words in common, the distance between there corresponding points in n-hyperplane is going to be much more and vice-versa. Now instead of using n-hyperplane to represent these points, I reduced the dimensionality of the points to 32-dimensions (Why 32? is discussed later in the blog). The dimensionality can be reduced by picking 32 random dimensions and ignoring the others but that would just be too stupid, so I tried using different dimensionality reduction techniques and found Truncated SVD to work miracles for the given data.</p>
<blockquote>
  <p>I used dimensionality reduction as a method to add some form of <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization</a> to the data.</p>
</blockquote>

<p><strong>Let’s understand Truncated SVD-</strong></p>

<p>SVD abbreviation for Singular Value Decomposition is a matrix factorization technique that factorizes any given matrix into the three matrices U, S, and V.
It goes by the equation-</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2900/0*ieoRDguEa0BVrLyO.jpeg" />
    </div>
</div>

<p><img src="" alt="" />
Let me explain what U, S, and V are.
<strong><em>U (aka left singular):</em></strong> is an orthogonal matrix whose columns are the eigenvectors of <strong>AᵀA</strong>.
<strong><em>S (singular):</em></strong> is a diagonal matrix whose diagonal elements are the square root of eigenvalues of <strong>AᵀA</strong> or <strong>AAᵀ</strong>(both have the same Eigenvalues) arranged in descending order i.e.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2792/1*rVknwpIrbPkFOIyemab5HA.png" />
    </div>
</div>

<p><strong><em>V (aka right singular):</em></strong> is an orthogonal matrix whose columns are the eigenvectors of <strong>AAᵀ</strong><em>.</em></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2712/0*mJ12be_KbhuS8Ta0.jpeg" />
    </div>
</div>

<p><img src="" alt="" />
<strong>Eigenvectors:</strong> An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it. Consider the image below in which three vectors are shown. The green square is only drawn to illustrate the linear transformation that is applied to each of these three vectors.
Note that the direction of these eigenvectors do not change but their length does change, and <strong>eigenvalue</strong> is the factor by which their length change.
source: <a href="https://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/">https://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/</a></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*7SkivPxZSwPO1NsQ.png" />
    </div>
</div>

<p><img src="" alt="" />
So now that we know how to factorize a matrix, we can reduce the dimension of a matrix using Truncated SVD which is a simple extension to SVD.</p>

<p><strong>Truncated SVD:</strong> Suppose we have an input matrix of dimensions (m x n) which we want to reduce to (m x r) where r&lt;n.
We simply compute the first <strong>‘r’</strong> eigenvectors of <strong>AᵀA</strong> and store it as the columns of U, then we compute the first <strong>‘r’</strong> eigenvectors of <strong>AAᵀ</strong> and store it as the columns of V and finally the root of first <strong>‘r’</strong> eigenvalues as diagonal elements of S.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2712/0*ajW4rgk6fhrYmNG_.gif" />
    </div>
</div>

<p><img src="" alt="" />
So in a nutshell, Truncated-SVD is a smart technique that reduces the dimensionality of given data in a smart way by preserving as much information (variance) as possible.</p>

<p>If you want to dive deeper into SVD, check out <a href="https://youtu.be/Nx0lRBaXoz4">this lecture</a> by Prof. W. Gilbert Strang, and <a href="https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491">this wonderful blog</a> on SVD.</p>

<p>Scikit-learn comes with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html">Truncated SVD</a> built in that can be imported and used directly.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Modeling</strong></p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2880/0*4gydrw9A3FqJfb7m.png" />
    </div>
</div>

<p>Now that we are done with the data pre-processing and feature engineering part, let’s see how to group similar words together using a clustering algorithm.</p>

<p><img src="" alt="" /></p>
<h2 id="k-means">K-Means</h2>

<p><strong>K</strong>-<strong>means</strong> clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable <strong>K</strong>.</p>

<p>Let’s see how K-means works.
Suppose there are some points in a 2-dimensional plane, and we want to cluster these points into <strong>K</strong> clusters.</p>

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-1">
         <img class="img-fluid rounded z-depth-0" src="https://lh3.googleusercontent.com/proxy/nEQUf8QVAdV7TSAW_kzeLOeKgpE0q1WRsL0BM-01OHsiGOOx8RPR4LFD6qm2Pw8KDlYxQkzthNlJBmLO4fGtSG41uRT2Oyw_i1jh" />
     </div>
 </div>

<p>The steps are simple-</p>

<ul>
  <li>
    <p>Defining <strong>K</strong> points randomly in the plane. Let’s call these points as cluster <strong>centroids</strong>.</p>
  </li>
  <li>
    <p>Iterating over each point in the data and check for the closest centroid and assign that point to its closest centroid.</p>
  </li>
  <li>
    <p>After the above step, each centroid must have some points that are closest to it, let’s call these sets of points <strong>clusters</strong>.</p>
  </li>
  <li>
    <p>Updating the centroid of each cluster by calculating the mean value of x and y coordinate of all the points in that cluster. The calculated mean values (x, y) are the coordinates of the updated centroid of that cluster.</p>
  </li>
  <li>
    <p>Repeating the last 3 steps until the coordinates of the centroids do not update much.</p>
  </li>
</ul>

<p><strong>But how to decide the right value for K (number of clusters)?</strong>
Let’s define a metric that can be used to measure the right value of <strong>K.</strong>
Distortion is one such metric that uses the sum of squares of the distance of points in a cluster from the cluster mean, for all the clusters summed up.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2048/1*aSeXkf9At7WSbGy5s_d4vw.png" />
    </div>
</div>

<p><img src="" alt="" />
Distortion can be used to check how efficient the clustering algorithm is for a given value of <strong>K</strong>.
The optimal value of K can be determined by calculating the distortion value for different values of K and then plotting them.
This plot is known as the <strong>Elbow plot</strong>. Just by looking at the elbow plot we can determine the optimal ‘K’ as the value where distortion stops decreasing rapidly.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*OIKw1kVoK0GVfDiB.png" />
    </div>
</div>

<p><img src="" alt="" />
Here is an Elbow plot and just by looking at it, we can say that the best hyperparameter is k=3.</p>

<p>It is called elbow plot because it looks like an arm (maybe a stick man’s arm) and the elbow of this arm represents the optimal K.</p>

<p>One more thing, I’ll be using cosine distance as a measure to compute the distance between points (including centroids). Let’s quickly understand what <strong>cosine distance</strong> is using <em>cosine similarity.</em></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*1Z6SiZT4cIUj05Hk" />
    </div>
</div>

<p><img src="" alt="" />
<strong>Cosine similarity</strong> is a measure to calculate how parallel 2 vectors are. It is calculated using the cosine of the angle between 2 vectors. It can easily be calculated using-</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*NJpAgmY9Xml5RPeY" />
    </div>
</div>

<p><img src="" alt="" />
So if 2 vectors a and b are parallel, the angle between them will be 0 and the cosine similarity will be cos(0) = 1. Similarly, if 2 vectors a and b are pointing in the opposite direction, the angle between them will be 𝛑 and the cosine similarity will be cos(𝛑) = -1.
In a nutshell, cosine similarity tells us about what extent 2 vectors are pointing in a similar direction. A value near 1 tells us that the vectors are pointing in a very similar direction whereas a value near -1 corresponds to pointing in opposite directions.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*GVHoeYeb-P8nySnEQo8JkQ.png" />
    </div>
</div>

<p><img src="" alt="" />
Now <strong>Cosine distance</strong> between 2 points is nothing but
<strong><em>1 - (cosine similarity of the vectors representing them in hyperspace)</em></strong>.
So it ranges from (0 to 2), where 0 corresponds to the points being very similar, and 2 corresponds to the points being very dissimilar.
Can you guess when will the cosine distance between 2 points be 1?</p>

<p>To know more check out <a href="https://www.machinelearningplus.com/nlp/cosine-similarity/">this blog</a>.</p>

<p>I’ll be implementing K-means from scratch since Scikit learns K-Means does not support cosine distance.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">KMeans</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_clusters</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="n">n_clusters</span> <span class="c1"># initializing the number of clusters
</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span> <span class="c1"># This function performs fit operation on the data
</span>        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">clusters</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># initializing the clusters as all zeros
</span>
        <span class="c1"># initializing centroids
</span>        <span class="n">rows</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span>
        <span class="n">rows</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">centroids</span> <span class="o">=</span> <span class="n">rows</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_clusters</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># Initialize old centroids as all zeros
</span>        <span class="n">self</span><span class="p">.</span><span class="n">old_centroids</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span>
                                          <span class="n">columns</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>

        <span class="c1"># check the distance of each data point to the centroid and assigning each point to the closest cluster.
</span>        <span class="k">while</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">old_centroids</span><span class="p">.</span><span class="nf">equals</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">):</span>
            <span class="c1"># Stash old centroids
</span>            <span class="n">self</span><span class="p">.</span><span class="n">old_centroids</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">.</span><span class="nf">copy</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

            <span class="c1"># Iterate through each data point/set
</span>            <span class="k">for</span> <span class="n">row_i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">)):</span>
                <span class="n">distances</span> <span class="o">=</span> <span class="nf">list</span><span class="p">()</span>
                <span class="n">point</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">row_i</span><span class="p">]</span>

                <span class="c1"># Calculate the distance between the point and centroid
</span>                <span class="k">for</span> <span class="n">row_c</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">)):</span>
                    <span class="n">centroid</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">row_c</span><span class="p">]</span>
                    <span class="n">point_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">point</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">centroid_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">centroid</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">distances</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">cosine_distances</span><span class="p">(</span><span class="n">point_array</span><span class="p">,</span> <span class="n">centroid_array</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

                <span class="c1"># Assign this data point to a cluster
</span>                <span class="n">self</span><span class="p">.</span><span class="n">clusters</span><span class="p">[</span><span class="n">row_i</span><span class="p">]</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">))</span>

            <span class="c1"># For each cluster extract the values which now belong to each cluster and calculate new k-means
</span>            <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_clusters</span><span class="p">):</span>

                <span class="n">label_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">clusters</span> <span class="o">==</span> <span class="n">label</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

                <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">label_idx</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">old_centroids</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Set the new centroid to the mean value of the data points within this cluster
</span>                    <span class="n">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">label_idx</span><span class="p">].</span><span class="nf">mean</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>It’s time to run the algorithm on the pre-processed data and check for the right hyperparameters.
Hyperparameter tuning is a process of determining the right hyperparameters that make the model work phenomenally well for the given data.
In this case, there are 2 hyperparameters-</p>

<ul>
  <li>
    <p>1 from Truncated-SVD: n_components (reduced dimension)</p>
  </li>
  <li>
    <p>1 from K-means: ‘K’ (number of clusters).</p>
  </li>
</ul>

<p>I’ve already plotted the elbow plots for different n_component values.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2304/1*pMWankiqTj-wyeWiCftndQ.png" />
    </div>
</div>

<p><img src="" alt="" />
By looking at the plots, we can say that the best hyperparameters are-</p>
<ul>
  <li>n_components: 32</li>
  <li>K (number of clusters): 50</li>
</ul>

<p>Finally, It’s time to initialize Truncated-SVD and K-Means using the best hyperparameters and cluster the data.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.decomposition import TruncatedSVD

&gt; trans = TruncatedSVD(n_components=32)
&gt; data_updated = trans.fit_transform(words_tfidf.toarray())
&gt; model = custom_KMeans(n_clusters=50)
&gt; model.train(data_updated)
</code></pre></div></div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Visualizing the results</strong></p>
</blockquote>

<p>These are the results obtained after clustering the data.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3200/1*UockM02CJAXyktGngP2EyA.png" />
    </div>
</div>

<p><img src="" alt="" />
Let’s check out some of the clusters:
I’ll be using the <a href="https://networkx.github.io/documentation/stable/">networkx library</a> to create the clusters.
In each cluster, the red nodes correspond to the words from Barron’s 333-word list and how they are linked with each other and their synonyms.
You can check out the documentation for networkx <a href="https://networkx.github.io/documentation/stable/">here</a>. Also, I’ll demonstrate how wonderful it is with an example in the end.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/6022/1*qzIVJO_m0UUOemKXinmGgQ.png" />
    </div>
</div>

<p>Finally, let’s visualize the data in 3d using t-SNE but first,</p>

<p><img src="" alt="" />
<strong>Let’s talk about t-SNE:</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2560/0*erGNe7ChcAcpJzdX.jpg" />
    </div>
</div>

<p><img src="" alt="" />
<strong>t-SNE</strong> (t-Distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions such that each embedding in the lower dimension represents the value in higher dimension. Also, these embeddings are placed in the lower dimension in such a manner that the distance between neighborhood points is preserved. So, t-SNE preserves the local structure of the data as well.
I’ll try to explain how it does what it does.
For a given point in n-dimensional hyperspace, it calculates the distance of that point from all the other points and converts these distributions of distances to <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">student’s t-distribution</a>. This is done for all the points such that in the end, each point has its own t-distribution of distances from all the other points.
Now the points are randomly scattered in the lower dimensional space and each point is displaced by some distance such that after the displacement of all the points is done if we recalculate the t-distribution of distances of each point from the remaining points (this time this is done in the lower dimensional space), the distribution would be the same as what we obtained in n-dimensional hyperspace.
There are 2 main hyperparameters in t-SNE-
<strong><em>Perplexity:</em></strong> Instead of calculating the distance from all the other points, we can use only ‘k’ nearest points. This value of ‘k’ is called the perplexity value.
<strong><em>Iterations:</em></strong> The number of iterations for which we want t-SNE to update the points in lower-dimensional space.
Due to stochasticity, the algorithm may perform differently for different perplexity values so as a good practice, it is preferred to run t-SNE for different perplexity values and different numbers of iterations.
To know more about t-SNE, check out <a href="https://distill.pub/2016/misread-tsne/">this awesome blog</a>, it has t-SNE very well explained with interactive visualization.</p>

<p>Below is the plot using t-SNE in two dimensions for different perplexity and iteration values. We can see t-SNE working well with perplexity 20 and 2000 iterations.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td><td class="code"><pre><span class="c1"># importing necessary libraries
</span><span class="kn">from</span> <span class="n">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm_notebook</span>
<span class="c1"># initializing 16 subplots
</span><span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="c1"># transorforming the data to lower dimensions using TruncatedSVD
</span><span class="n">trans</span> <span class="o">=</span> <span class="nc">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">svd_dim</span> <span class="o">=</span> <span class="n">trans</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">words_tfidf</span><span class="p">.</span><span class="nf">toarray</span><span class="p">())</span>
<span class="c1"># iterating over different perplexity and iteration values and plotting.
</span><span class="n">perplexity</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">60</span><span class="p">]</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="p">[</span><span class="mi">500</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">1500</span><span class="p">,</span><span class="mi">2000</span><span class="p">]</span>
<span class="n">p_</span><span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nf">tqdm_notebook</span><span class="p">(</span><span class="n">perplexity</span><span class="p">):</span>
  <span class="n">i_</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm_notebook</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">trans_</span> <span class="o">=</span> <span class="nc">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">node_embeddings_2d</span> <span class="o">=</span> <span class="n">trans_</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">svd_dim</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">p_</span><span class="p">,</span><span class="n">i_</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">node_embeddings_2d</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="n">node_embeddings_2d</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
                      <span class="n">c</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="n">clusters</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">p_</span><span class="p">,</span> <span class="n">i_</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">perplexity:</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s">; iterations:</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i_</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
      <span class="n">ax</span><span class="p">[</span><span class="n">p_</span><span class="p">,</span> <span class="n">i_</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">1st dimension</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p_</span><span class="o">==</span><span class="mi">3</span><span class="p">:</span>
      <span class="n">ax</span><span class="p">[</span><span class="n">p_</span><span class="p">,</span> <span class="n">i_</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">2nd dimension</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">i_</span><span class="o">+=</span><span class="mi">1</span>
  <span class="n">p_</span><span class="o">+=</span><span class="mi">1</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">tsne_plot.png</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2880/1*RQ1ZtC9P8ZhcQPbHsUhsEg.png" />
    </div>
</div>

<p>Finally, Here is the complete graph of all the words and their synonyms (I used 4 synonyms for each word) using networkx.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="code"><pre><span class="c1"># importing the necessary libraries
</span><span class="kn">import</span> <span class="n">networkx</span> <span class="k">as</span> <span class="n">nx</span>
<span class="kn">from</span> <span class="n">networkx.algorithms</span> <span class="kn">import</span> <span class="n">bipartite</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="c1"># importing the data
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">barrons333_4.csv</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># contains word-synonym pair
# The below list contains word-synonym pair as tuples
</span><span class="n">edges</span> <span class="o">=</span> <span class="p">[</span><span class="nf">tuple</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()]</span>
<span class="c1"># defining graph object, adding words as nodes and links to synonyms
</span><span class="n">B</span> <span class="o">=</span> <span class="n">nx</span><span class="p">.</span><span class="nc">Graph</span><span class="p">()</span>
<span class="n">B</span><span class="p">.</span><span class="nf">add_nodes_from</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">word</span><span class="sh">'</span><span class="p">].</span><span class="nf">unique</span><span class="p">(),</span> <span class="n">bipartite</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">word</span><span class="sh">'</span><span class="p">)</span>
<span class="n">B</span><span class="p">.</span><span class="nf">add_edges_from</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">links</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># defining the type of layout for the graph
</span><span class="n">pos_</span> <span class="o">=</span> <span class="n">nx</span><span class="p">.</span><span class="nf">spring_layout</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="c1"># using pyplot to plot the graph.
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">45</span><span class="p">,</span><span class="mi">45</span><span class="p">))</span>
<span class="n">nx</span><span class="p">.</span><span class="nf">draw</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">pos_</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">,</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">word_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">word</span><span class="sh">'</span><span class="p">].</span><span class="nf">unique</span><span class="p">())</span>
<span class="n">nx</span><span class="p">.</span><span class="nf">draw_networkx_nodes</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">pos_</span><span class="p">,</span> <span class="n">nodelist</span><span class="o">=</span><span class="n">word_list</span><span class="p">,</span>
                     <span class="p">...</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="sh">'</span><span class="s">g</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/5788/1*5MjLkhDoPNcU-eVi2T-J3A.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Final note</strong></p>
</blockquote>

<p>Thank you for reading the blog. I hope it was useful for some of you aspiring to do projects on NLP, unsupervised machine-learning, data processing, data visualizing.</p>

<p>And if you have any doubts regarding this project, please leave a comment in the response section or in the GitHub repo of this project.</p>

<p>The full project is available on my Github:
<a href="https://github.com/SarthakV7/Clustering-Barron-s-333-word-list-using-unsupervised-machine-learning">https://github.com/SarthakV7/Clustering-Barron-s-333-word-list-using-unsupervised-machine-learning</a></p>

<p>Find me on LinkedIn: <a href="http://www.linkedin.com/in/sarthak-vajpayee">www.linkedin.com/in/sarthak-vajpayee</a></p>

<p>Peace! ☮</p>]]></content><author><name></name></author><summary type="html"><![CDATA[The most challenging part of GRE preparation is the vocabulary part. At-least for me it was until my machine learning model helped me out with it.]]></summary></entry><entry><title type="html">How to solve sudoku using artificial intelligence.</title><link href="https://sarthakv7.github.io/my_folio/blog/2019/sudoku/" rel="alternate" type="text/html" title="How to solve sudoku using artificial intelligence." /><published>2019-11-21T11:10:16+00:00</published><updated>2019-11-21T11:10:16+00:00</updated><id>https://sarthakv7.github.io/my_folio/blog/2019/sudoku</id><content type="html" xml:base="https://sarthakv7.github.io/my_folio/blog/2019/sudoku/"><![CDATA[<h4 id="solving-sudoku-is-so-much-fun-but-how-about-training-a-machine-that-can-solve-sudoku-at-a-rate-of-150-sudokus-per-minute-">Solving Sudoku is so much fun, but how about training a machine that can solve sudoku at a rate of 150+ sudokus per minute? 😮</h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2400/1*TKm-NpEhRkmGLIQtx5IgBg.gif" />
    </div>
</div>

<p><img src="" alt="" />
In this tutorial, I will show how easy it is to code your own sudoku solver using deep learning and some image processing.</p>

<h3 id="prerequisites">Prerequisites:</h3>

<ul>
  <li>
    <p><strong>OpenCV</strong>: OpenCV is a library of programming functions mainly aimed at real-time computer vision plus its open-source, fun to work with and my personal favorite. I have used version 4.1.0 for this project.</p>
  </li>
  <li>
    <p><strong>Python</strong>: aka swiss army knife of coding. I have used version 3.6.7 here.</p>
  </li>
  <li>
    <p><strong>IDE:</strong> I will be using Jupyter notebooks for this project.</p>
  </li>
  <li>
    <p><strong>Keras:</strong> Easy to use and widely supported, Keras makes deep learning about as simple as deep learning can be.</p>
  </li>
  <li>
    <p><strong>Scikit-Learn:</strong> It is a free software machine learning library for the Python programming language.</p>
  </li>
  <li>
    <p>And of course, do not forget the <strong>coffee!</strong></p>
  </li>
</ul>

<p><strong><em>Let’s begin by creating a workspace.</em></strong></p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Creating a workspace.</strong></p>
</blockquote>

<p>I recommend making a conda environment because it makes project management much easier. Please follow the instructions in this <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/">link</a> for installing mini-conda. Once installed open cmd/terminal and create an environment using-</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>    conda create <span class="nt">-n</span> <span class="s1">'name_of_the_environment'</span> <span class="nv">python</span><span class="o">=</span>3.6 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now let’s activate the environment:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>    conda activate <span class="s1">'name_of_the_environment'</span> 
</pre></td></tr></tbody></table></code></pre></figure>

<p>This should set us inside our virtual environment. Time to install some libraries-</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre>    <span class="c"># installing OpenCV</span>
    pip <span class="nb">install </span>opencv-python<span class="o">==</span>4.1.0# Installing Keras
    pip <span class="nb">install </span>keras# Installing Jupyter
    pip <span class="nb">install </span>jupyter#Installing Scikit-Learn
    pip <span class="nb">install </span>scikit-learn 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Finally, let’s open jupyter notebook using the below command and start coding.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>    jupyter notebook 
</pre></td></tr></tbody></table></code></pre></figure>

<h3 id="approach-">Approach-</h3>

<p><strong>To make this simple, I have divided the approach into 3 parts-</strong></p>
<ol>
  <li>Looking for sudoku in the image.</li>
  <li>Extracting numbers from sudoku.</li>
  <li>Solving the Sudoku using backtracking.</li>
</ol>

<h3 id="part-1-looking-for-sudoku-in-the-image-"><strong>Part 1 (Looking for sudoku in the image)-</strong></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*VpVKSlgn5RBo-vmbEIlakg.png" />
    </div>
</div>

<p>In this part, we’ll be focusing on how to extract the sudoku grid i.e. our Region of Interest (ROI) from the input image.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2092/1*ZRxJNwJV2QiP8ACbGxPAhQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<h5 id="steps-"><strong>Steps</strong>-</h5>

<p><strong>1. Converting the input image to a binary image</strong>: This step involves converting the input image to a greyscale image, applying gaussian blur to the grey-scale image, followed by thresholding, bitwise_not and dilating the image.</p>

<p><strong>Grey-scale:</strong> Colored digital images are nothing but 3-dimensional arrays containing pixel data in the form of numbers. Ideally there are 3 channels: RGB (Red, Green, &amp; Blue) and when we convert a colored image to greyscale, we just keep a single channel instead of 3 and this channel has a value ranging from 0–255 where 0 corresponds to a black pixel and 255corresponds to a white pixel and remaining values decide how gray the pixel is (the closer to 0, the darker it is).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*HdStwcs07a0a5EuVTPx5vw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>

<p><strong>Gaussian Blur:</strong> An image blurring technique in which the image is convolved with a <a href="https://en.wikipedia.org/wiki/Gaussian_filter">Gaussian kernel</a>. I’ll be using it to smoothen the input image by reducing noise.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*EdbST8knhBIZrx70F9PZsg.png" />
    </div>
</div>

<p><img src="" alt="" /></p>

<p><strong>Thresholding:</strong> In grayscaled images, each pixel has a value between 0–255, to convert such an image to a binary image we apply thresholding. To do this, we choose a threshold value between 0–255 and check each pixel’s value in the grayscale image. If the value is less than the threshold, it is given a value of 0 else 1.</p>

<p><img src="" alt="" />
<strong><em>In binary images, the pixels have just 2 values (0: Black, 1:White) and hence, it makes edge detection much easier.</em></strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*AK_0UaE534NT_VwSW70urw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="code"><pre> <span class="k">def</span> <span class="nf">pre_process_image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">skip_dilate</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
     <span class="sh">"""</span><span class="s">Uses a blurring function, adaptive thresholding and dilation to expose the main features of an image.</span><span class="sh">"""</span>

     <span class="c1"># Gaussian blur with a kernal size (height, width) of 9.
</span>     <span class="c1"># Note that kernal sizes must be positive and odd and the kernel must be square.
</span>     <span class="n">proc</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">GaussianBlur</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="nf">copy</span><span class="p">(),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>

     <span class="c1"># Adaptive threshold using 11 nearest neighbour pixels
</span>     <span class="n">proc</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">adaptiveThreshold</span><span class="p">(</span><span class="n">proc</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">ADAPTIVE_THRESH_GAUSSIAN_C</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">THRESH_BINARY</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

     <span class="c1"># Invert colours, so gridlines have non-zero pixel values.
</span>     <span class="c1"># Necessary to dilate the image, otherwise will look like erosion instead.
</span>     <span class="n">proc</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">bitwise_not</span><span class="p">(</span><span class="n">proc</span><span class="p">)</span>

     <span class="k">if</span> <span class="ow">not</span> <span class="n">skip_dilate</span><span class="p">:</span>
         <span class="c1"># Dilate the image to increase the size of the grid lines.
</span>         <span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]],</span> <span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>
         <span class="n">proc</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">dilate</span><span class="p">(</span><span class="n">proc</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>

     <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">proc</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">)</span>
     <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">pre_process_image</span><span class="sh">'</span><span class="p">)</span>
     <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
     <span class="k">return</span> <span class="n">proc</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p><img src="" alt="" /></p>

<p><strong>2. Detecting the largest polygon in the image:</strong> This step involves finding the contours from the previous image and selecting the largest contour (corresponds to the largest grid). Now from the largest contour, selecting the extreme most points and that would be the 4 corners.
The function cv2.findContours returns all the contours it finds in the image.
After sorting the returned contours from the image by area, the largest contour can easily be selected. And once we have the largest polygon, we can easily select the 4 corners (displayed as the 4 green points in the third image in figure 1).</p>

<ul>
  <li><strong>Contour detection:</strong> Contours can be explained simply as a curve joining all the continuous points (along the boundary), having the same color or intensity.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*7xbCSgN25_qq5irJ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">find_corners_of_largest_polygon</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Finds the 4 extreme corners of the largest contour in the image.</span><span class="sh">"""</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">contours</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">findContours</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="nf">copy</span><span class="p">(),</span> <span class="n">cv2</span><span class="p">.</span><span class="n">RETR_EXTERNAL</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">CHAIN_APPROX_SIMPLE</span><span class="p">)</span>  <span class="c1"># Find contours
</span>    <span class="n">contours</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">contours</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">cv2</span><span class="p">.</span><span class="n">contourArea</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># Sort by area, descending
</span>    <span class="n">polygon</span> <span class="o">=</span> <span class="n">contours</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Largest image
</span>
    <span class="c1"># Use of `operator.itemgetter` with `max` and `min` allows us to get the index of the point
</span>    <span class="c1"># Each point is an array of 1 coordinate, hence the [0] getter, then [0] or [1] used to get x and y respectively.
</span>
    <span class="c1"># Bottom-right point has the largest (x + y) value
</span>    <span class="c1"># Top-left has point smallest (x + y) value
</span>    <span class="c1"># Bottom-left point has smallest (x - y) value
</span>    <span class="c1"># Top-right point has largest (x - y) value
</span>    <span class="n">bottom_right</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="nf">enumerate</span><span class="p">([</span><span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">pt</span> <span class="ow">in</span> <span class="n">polygon</span><span class="p">]),</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="p">.</span><span class="nf">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">top_left</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="nf">enumerate</span><span class="p">([</span><span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">pt</span> <span class="ow">in</span> <span class="n">polygon</span><span class="p">]),</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="p">.</span><span class="nf">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">bottom_left</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="nf">enumerate</span><span class="p">([</span><span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">pt</span> <span class="ow">in</span> <span class="n">polygon</span><span class="p">]),</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="p">.</span><span class="nf">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">top_right</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="nf">enumerate</span><span class="p">([</span><span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">pt</span> <span class="ow">in</span> <span class="n">polygon</span><span class="p">]),</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="p">.</span><span class="nf">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Return an array of all 4 points using the indices
</span>    <span class="c1"># Each point is in its own array of one coordinate
</span>    <span class="k">return</span> <span class="p">[</span><span class="n">polygon</span><span class="p">[</span><span class="n">top_left</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">polygon</span><span class="p">[</span><span class="n">top_right</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">polygon</span><span class="p">[</span><span class="n">bottom_right</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">polygon</span><span class="p">[</span><span class="n">bottom_left</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p><img src="" alt="" /></p>

<p><strong>3. Cropping and warping the detected polygon:</strong>
The approach is simple since we have the coordinates of the 4 corners, we can use this to crop and wrap the original image resulting in the final image in figure 1.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*SyK4ZRyqdM2gYDLoANJOZA.png" />
    </div>
</div>

<p>See the two images in the figure above? Both of them are cropped but still, the one on the right is much better than the one on the left. That’s because of warping, it gives a better perspective of the image. I’ll explain how we can achieve warping using cv2.warpPerspective()
cv2.warpPerspective function takes 3 arguments <strong>source image (src), transformation matrix (m), and size of the destination image (size, size)</strong>.
transformation matrix. Now transformation matrix is a 3x3 matrix that helps us calculate the perspective transform. We can get the transformation matrix using cv2.<strong>getPerspectiveTransform</strong>(source_image, destination_image).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*vtyYf7q3UDKGpdpaipNppA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*cydEJHPYQMjG3yZZgsN4ew.png" />
    </div>
</div>

<p><img src="" alt="" /></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">crop_and_warp</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">crop_rect</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Crops and warps a rectangular section from an image into a square of similar size.</span><span class="sh">"""</span>

    <span class="c1"># Rectangle described by top left, top right, bottom right and bottom left points
</span>    <span class="n">top_left</span><span class="p">,</span> <span class="n">top_right</span><span class="p">,</span> <span class="n">bottom_right</span><span class="p">,</span> <span class="n">bottom_left</span> <span class="o">=</span> <span class="n">crop_rect</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">crop_rect</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">crop_rect</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">crop_rect</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

    <span class="c1"># Explicitly set the data type to float32 or `getPerspectiveTransform` will throw an error
</span>    <span class="n">src</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">top_left</span><span class="p">,</span> <span class="n">top_right</span><span class="p">,</span> <span class="n">bottom_right</span><span class="p">,</span> <span class="n">bottom_left</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Get the longest side in the rectangle
</span>    <span class="n">side</span> <span class="o">=</span> <span class="nf">max</span><span class="p">([</span>
        <span class="nf">distance_between</span><span class="p">(</span><span class="n">bottom_right</span><span class="p">,</span> <span class="n">top_right</span><span class="p">),</span>
        <span class="nf">distance_between</span><span class="p">(</span><span class="n">top_left</span><span class="p">,</span> <span class="n">bottom_left</span><span class="p">),</span>
        <span class="nf">distance_between</span><span class="p">(</span><span class="n">bottom_right</span><span class="p">,</span> <span class="n">bottom_left</span><span class="p">),</span>
        <span class="nf">distance_between</span><span class="p">(</span><span class="n">top_left</span><span class="p">,</span> <span class="n">top_right</span><span class="p">)</span>
        <span class="p">])</span>

    <span class="c1"># Describe a square with side of the calculated length, this is the new perspective we want to warp to
</span>    <span class="n">dst</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">side</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">side</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">side</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">side</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Gets the transformation matrix for skewing the image to fit a square by comparing the 4 before and after points
</span>    <span class="n">m</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">getPerspectiveTransform</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>

    <span class="c1"># Performs the transformation on the original image
</span>    <span class="n">warp</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">warpPerspective</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">side</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="n">side</span><span class="p">)))</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">warp</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">warp_image</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">warp</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p><img src="" alt="" /></p>

<h3 id="part-2-extracting-numbers-from-sudoku-">Part 2 (Extracting numbers from sudoku)-</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*SqNs8ziy_z1d2ZUVmth_jQ.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2564/1*B3oZp0a54NBoOk6MrUVhnQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<h5 id="step-1-">Step 1-</h5>

<ol>
  <li><strong>Eliminating noise from the binary image:</strong> We’ll use a gaussian blur to remove the noise from the warped image resulting in a better image.</li>
  <li><strong>Extracting the smaller grids:</strong> Since we already know that a sudoku grid has 81 similar dimensional cells (9 rows and 9 columns). We can simply iterate over the row length and column length of the image, check for points every 1/9th of the length of row/column distance apart and stored the set of pixels that were in between in an array. Some of the sample pixels extracted are shown in the last image in figure 2.</li>
</ol>

<h5 id="step-2-">Step 2-</h5>

<ol>
  <li>Since we now have the individual grids that contain the digit information, we need to build a neural network that is capable of recognize the digits.</li>
  <li><strong>Data:</strong> We should expect 10 types of characters, 1–9 and null i.e. blank space that is needed to be filled. I’ve created the data using the above-mentioned grid extraction method from different sudoku images. 30 samples for each digit.</li>
  <li><strong>CNN:</strong> The convolutional neural network that I’ve made is 6 layers deep (4 hidden layers) sequential model.
To keep the model simple, we’ll start by creating a sequential object.</li>
</ol>

<ul>
  <li>The first layer will be a convolutional layer with 32 output filters, a convolution window of size (16, 16), and ‘ReLU’ as the activation function.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*pl1n5jXISUPldfxW.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*Y060k1TEJsWdIKm1.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>Next, we’ll be adding a max-pooling layer with a window size of (2,2).
<strong>Max pooling</strong> is a sample-based discretization process. The objective is to down-sample an input representation (image, hidden-<strong>layer</strong> output matrix, etc.), reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*YZdtu5_gSytx9x0H" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2800/0*3DmQcQvfa-RXuXfZ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>Now, we will be adding some dropout rate to take care of overfitting.
<strong>Dropout</strong> is a regularization hyperparameter initialized to prevent Neural Networks from Overfitting. Dropout is a technique where randomly selected neurons are ignored during training. They are “<strong>dropped</strong>-<strong>out</strong>” randomly. We have chosen a dropout rate of 0.5 which means 50% of the nodes will be retained.</p>
  </li>
  <li>
    <p>Now it’s time to flatten the node data so we add a flatten layer for that. The flatten layer takes data from the previous layer and represents it in a single dimension.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*Bw6THVBMaOGx0CKP" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*bHOWX-n-1tgSHsIK.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>Finally, we will be adding 2 dense layers, one with the dimensionality of the output space as 128, activation function=’relu’ and other, our final layer with 10 output classes for categorizing the 10 digits (0–9) and activation function=’ softmax’.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
</pre></td><td class="code"><pre><span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="n">keras.layers.convolutional</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">keras.utils</span> <span class="kn">import</span> <span class="n">np_utils</span>

<span class="c1"># load dataset
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">image_data.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># split into input and output variables
</span><span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">]</span>
<span class="k">del</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">flat_pixels</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">flat_pixels</span><span class="p">,</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    <span class="n">X</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

<span class="c1"># split the data into training (50%) and testing (50%)
</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

<span class="c1"># one hot encode outputs
</span><span class="n">Y_train</span> <span class="o">=</span> <span class="n">np_utils</span><span class="p">.</span><span class="nf">to_categorical</span><span class="p">(</span><span class="n">Y_train</span><span class="p">)</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">np_utils</span><span class="p">.</span><span class="nf">to_categorical</span><span class="p">(</span><span class="n">Y_test</span><span class="p">)</span>

<span class="c1">#reshaping data
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X_test</span>  <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">11</span>

<span class="c1"># print('done!!!')
</span>
<span class="c1"># create model
</span><span class="n">model_</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">()</span>
<span class="n">model_</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">16</span><span class="p">),</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">))</span>
<span class="n">model_</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">model_</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model_</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Flatten</span><span class="p">())</span>
<span class="n">model_</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">))</span>
<span class="n">model_</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">))</span>


<span class="c1"># Compile model
</span><span class="n">model_</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>
<span class="n">model_</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Final evaluation of the model
</span><span class="n">scores</span> <span class="o">=</span> <span class="n">model_</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">CNN Error: %.2f%%</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="mi">100</span><span class="o">-</span><span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p><img src="" alt="" /></p>
<h5 id="training-and-testing"><strong>Training and Testing.</strong></h5>

<p>The dataset I have created is small but works well. It contains 30 sample images of size 28x28 for each of the digits (0–9). Here 0 corresponds to the blank space in sudoku.
Now we need to split the data into 2 sets-</p>
<ol>
  <li>Training data set: We use this data to train the model.</li>
  <li>Test data set: This data is unseen for the model so we will use this to test the performance of the created model on unseen data.
The train test split is 70–30 i.e 30% of points are selected from the whole dataset randomly and assigned as the test set. The remaining 70% points are assigned as the train set.</li>
</ol>

<p><img src="" alt="" /></p>
<h5 id="results-">Results-</h5>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*fy-5w07mG15wHi152thBjg.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*jdX7m-nBymIRpAmKr6RpyQ.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*rG65lP_s852EdeSjw_91pA.png" />
    </div>
</div>

<p><img src="" alt="" />
These are the prediction results from some of the sample grids that I extracted and as we can see the predictions are pretty accurate, we can proceed to the next step:
Detecting all the characters and storing them in an array of dimensions 9x9.</p>

<p>We traverse through all the grids that we extracted from the sudoku and get the predicted character from our model we’ll end up with an array-</p>
<ul>
  <li>Note: here 0 corresponds to the blank space.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*6unKN-WMQVMWHX2A7yreug.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<h3 id="part-3-solving-the-sudoku-using-backtracking">Part 3 (Solving the Sudoku using backtracking):</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*Agl6KYLlG82XsYeUoId9fQ.png" />
    </div>
</div>

<p>Now that we have converted the image into an array, we just need to make a function that can fill up the blank spaces effectively given the rules of sudoku.
I found backtracking very efficient at this so let’s talk more about it.</p>

<h5 id="backtracking">Backtracking:</h5>

<p>Backtracking is an algorithmic-technique for solving problems recursively by trying to build a solution incrementally, one piece at a time, removing those solutions that fail to satisfy the constraints of the problem at any point.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*1aj4-PcXpXs_llBbxanWIg.gif" />
    </div>
</div>

<p><img src="" alt="" />
Let’s understand how backtracking works through an example using a 3x3 sudoku. Suppose we’ve to solve the grid given the constraints-</p>

<ol>
  <li>
    <p>Each column has unique numbers from 1 to ’n’ or empty spaces.</p>
  </li>
  <li>
    <p>Each column has unique numbers from 1 to ’n’ or empty spaces.</p>
  </li>
</ol>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*nViO2YFMkaMlLPpMHqn0-A.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*vnC_w6fLl4vnfaEu9d0jwA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*jMmLY1d0qKqFQ0D3Vunn7Q.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*2nqeTzxTmqUtRNOtlMC0rA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*AIv6X-A-Mdc__I3jKeqykg.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*5j0PCkrqFAjj7hKpkF4gfw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>

<p><strong>1.</strong> We need to fill 5 spaces, let’s start by iterating over the blank spaces from left to right and top to bottom.</p>

<p><strong>2.</strong> Let’s put 1 in the first encountered space and check if all the constraints are satisfied.
Seems 1 is good here as it is unique in its row and column.</p>

<p><strong>3.</strong> Moving on to the next grid, we again start by keeping 1 but as we can see that it is not unique in the column so we check for 2 and 2 satisfies all the constraints</p>

<p><strong>4.</strong> For the 3rd empty grid, we can see none of the 3 numbers 1,2,3 satisfy the constraints.</p>

<p><strong>5.</strong> To fix this, we take a step back and increment the number in the last grid we filled and make sure that the constraints are satisfied. Now we proceed with the next grig again and we can see that 2 fits in it.</p>

<p><strong>6.</strong> Similarly, we fill the remaining grids and reach the optimal solution.</p>

<p>Now that we’ve understood how backtracking works, let’s make a function using the constraints used while solving a 9x9 sudoku and test it on the array we obtained from the previous step.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">find_empty_grid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="c1"># checks for empty spaces '0'.
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
            <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nf">return </span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>  <span class="c1">#return row, col
</span>
    <span class="k">return</span> <span class="bp">None</span>

<span class="k">def</span> <span class="nf">solve_array</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">find</span> <span class="o">=</span> <span class="nf">find_empty_grid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#find the empty spaces
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">find</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">find</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">valid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">)):</span> <span class="c1">#check the constraints
</span>            <span class="n">x</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>

            <span class="k">if</span> <span class="nf">solve_array</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="k">return</span> <span class="bp">True</span>

            <span class="n">x</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">return</span> <span class="bp">False</span>

<span class="k">def</span> <span class="nf">valid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span> <span class="c1">#this function checks the constraints
</span>    <span class="c1"># Check row
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
        <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]][</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">num</span> <span class="ow">and</span> <span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">False</span>

    <span class="c1"># Check column
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">==</span> <span class="n">num</span> <span class="ow">and</span> <span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">i</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">False</span>

    <span class="c1"># Check box
</span>    <span class="n">box_x</span> <span class="o">=</span> <span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>
    <span class="n">box_y</span> <span class="o">=</span> <span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">box_y</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="n">box_y</span><span class="o">*</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">3</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">box_x</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">box_x</span><span class="o">*</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">3</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">num</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="o">!=</span> <span class="n">pos</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">False</span>

    <span class="k">return</span> <span class="bp">True</span>

<span class="k">def</span> <span class="nf">print_board</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="c1">#this function is for printing the array with a better look
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">- - - - - - - - - - - - - </span><span class="sh">"</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">j</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="sh">""</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
                <span class="n">k</span> <span class="o">=</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span> <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="nf">str</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>
                <span class="nf">print</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">k</span> <span class="o">=</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span> <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="nf">str</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>
                <span class="nf">print</span><span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="sh">""</span><span class="p">)</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*tQ8j2E6zIw4iEPKF0ThtLQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>

<blockquote>
  <p><strong>Final comment</strong></p>
</blockquote>

<p>Thank you for reading the blog, hope this project is useful for some of you aspiring to do projects on OCR, image processing, Machine Learning, IoT.</p>

<p>And if you have any doubts regarding this project, please leave a comment in the response section.</p>

<p>Kaggle notebook link: <a href="https://www.kaggle.com/sarthakvajpayee/simple-ai-sudoku-solver?scriptVersionId=40346804">https://www.kaggle.com/sarthakvajpayee/simple-ai-sudoku-solver?scriptVersionId=40346804</a></p>

<p>Github repo link: <a href="https://github.com/SarthakV7/ai_sudoku_solver">https://github.com/SarthakV7/ai_sudoku_solver</a></p>

<p>Find me on LinkedIn: <a href="http://www.linkedin.com/in/sarthak-vajpayee">www.linkedin.com/in/sarthak-vajpayee</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Solving Sudoku is so much fun, but how about training a machine that can solve sudoku at a rate of 150+ sudokus per minute? 😮]]></summary></entry><entry><title type="html">AI-powered Indian license plate detector.</title><link href="https://sarthakv7.github.io/my_folio/blog/2019/license_plate/" rel="alternate" type="text/html" title="AI-powered Indian license plate detector." /><published>2019-09-07T11:10:16+00:00</published><updated>2019-09-07T11:10:16+00:00</updated><id>https://sarthakv7.github.io/my_folio/blog/2019/license_plate</id><content type="html" xml:base="https://sarthakv7.github.io/my_folio/blog/2019/license_plate/"><![CDATA[<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3200/1*Fh9BMaEwSRsCFOFvQnC2ww.jpeg" />
    </div>
</div>

<p><img src="" alt="" /></p>

<h4 id="inspiration"><strong>Inspiration:</strong></h4>
<p>The guy who hit my car and got away with it!</p>

<p><img src="" alt="" /></p>

<h4 id="backstory"><strong>Backstory:</strong></h4>
<p>After a memorable evening with friends as we were about to leave for our home there was something that made that evening even more memorable, a huge dent in my car’s front bumper, seemed it was hit by another vehicle, but who to blame? There was no one around who would have witnessed that. And what could I do about it?  I’ll tell you exactly what I did about it. I decided to use my machine learning and data skills and make an AI-based Indian License plate detector that was capable enough to keep a watch on a vehicle by detecting the number plates of vehicles around it and in this blog I’ll be taking you guys through my journey of how I did it! First things first: There is always a scope of improvising, so if you come up with some better ideas or doubts regarding this project please do use the response section below.</p>

<p><img src="" alt="" /></p>

<h3 id="approach">Approach:</h3>

<p><strong>We need to build a system that is capable of -</strong></p>

<ul>
  <li>
    <p>Taking in the image/video (series of images) from surrounding:
at the hardware end, we need a pc (or raspberry pi) along with a camera and at the software end, we need a library to capture and process the data (image). I’ve used OpenCV (4.1.0) and Python (3.6.7) for this project.</p>
  </li>
  <li>
    <p>Looking for a license plate in the image:
To detect an object(license plate) from an image we need another tool that can recognize an Indian license plate so for that I’ve used Haar cascade, pre-trained on Indian license plates (will be updating soon to YOLO v3).</p>
  </li>
  <li>
    <p>Analyzing and performing some image processing on the License plate:
Using OpenCV’s grayscale, threshold, erode, dilate, contour detection and by some parameter tuning, we may easily be able to generate enough information about the plate to decide if the data is useful enough to be passed on to further processes or not (sometime if the image is very distorted or not proper, we may only get suppose 8 out of 10 characters, then there’s no point passing the data down the pipeline but to ignore it and look at the next frame for the plate), also before passing the image to the next process we need to make sure that it is noise-free and processed.</p>
  </li>
  <li>
    <p>Segmenting the alphanumeric characters from the license plate:
if everything in the above steps works fine, we should be ready to extract the characters from the plate, this can be done by thresholding, eroding, dilating and blurring the image skillfully such that at the end the image we have is almost noise-free and easy for further functions to work on. We now again use contour detection and some parameter tuning to extract the characters.</p>
  </li>
  <li>
    <p>Considering the characters one by one, recognizing the characters, concatenating the results and giving out the plate number as a string:
Now comes the fun part! Since we have all the characters, we need to pass the characters one by one into our trained model, and it should recognize the characters and voilà! We’ll be using Keras for our Convolutional Neural Network model.</p>
  </li>
</ul>

<p><img src="" alt="" /></p>

<h3 id="prerequisites">Prerequisites:</h3>

<ul>
  <li>
    <p><strong>OpenCV</strong>: OpenCV is a library of programming functions mainly aimed at real-time computer vision plus its open-source, fun to work with and my personal favorite. I have used version 4.1.0 for this project.</p>
  </li>
  <li>
    <p><strong>Python</strong>: aka swiss army knife of coding. I have used version 3.6.7 here.</p>
  </li>
  <li>
    <p><strong>IDE:</strong> I’ll be using Jupyter here.</p>
  </li>
  <li>
    <p><strong>Haar cascade</strong>: It is a machine learning object detection algorithm used to identify objects in an image or video and based on the concept of <strong>​​</strong> features proposed by Paul Viola and Michael Jones in their paper “Rapid Object Detection using a Boosted Cascade of Simple Features” in 2001. <a href="https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework">More info</a></p>
  </li>
  <li>
    <p><strong>Keras</strong>: Easy to use and widely supported, Keras makes deep learning about as simple as deep learning can be.</p>
  </li>
  <li>
    <p><strong>Scikit-Learn:</strong> It is a free software machine learning library for the Python programming language.</p>
  </li>
  <li>
    <p>And of course, do not forget the <strong>coffee</strong>!</p>
  </li>
</ul>

<p><img src="" alt="" /></p>

<h5 id="step-1"><strong>Step 1</strong></h5>

<blockquote>
  <p><strong>Creating a workspace.</strong></p>
</blockquote>

<p>I recommend making a conda environment because it makes project management much easier. Please follow the instructions in this <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/">link</a> for installing miniconda. Once installed open cmd/terminal and create an environment using-</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>    conda create <span class="nt">-n</span> <span class="s1">'name_of_the_environment'</span> <span class="nv">python</span><span class="o">=</span>3.6.7 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now let’s activate the environment:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>    conda activate <span class="s1">'name_of_the_environment'</span> 
</pre></td></tr></tbody></table></code></pre></figure>

<p>This should set us inside our virtual environment. Time to install some libraries-</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>    <span class="c"># installing OpenCV</span>
    pip <span class="nb">install </span>opencv-python<span class="o">==</span>4.1.0

    <span class="c"># Installing Keras</span>
    pip <span class="nb">install </span>keras

    <span class="c"># Installing Jupyter</span>
    pip <span class="nb">install </span>jupyter

    <span class="c">#Installing Scikit-Learn</span>
    pip <span class="nb">install </span>scikit-learn 
</pre></td></tr></tbody></table></code></pre></figure>

<p><img src="" alt="" /></p>

<h5 id="step-2"><strong>Step 2</strong></h5>
<blockquote>
  <p><strong>Setting up the environment!</strong></p>
</blockquote>

<p>We’ll start with running jupyter notebook and then importing necessary libraries in our case OpenCV, Keras and sklearn.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># in your conda environment run
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre>    jupyter notebook 
</pre></td></tr></tbody></table></code></pre></figure>

<p>This should open Jupyter notebook in the default web browser.
Once open, let’s import the libraries</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="code"><pre>    <span class="c">#importing openCV</span>
    import cv2

    <span class="c">#importing numpy</span>
    import numpy as np

    <span class="c">#importing pandas to read the CSV file containing our data</span>
    import pandas as pd

    <span class="c">#importing keras and sub-libraries</span>
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.layers import Dropout
    from keras.layers import Flatten, MaxPool2D
    from keras.layers.convolutional import Conv2D
    from keras.layers.convolutional import MaxPooling2D
    from keras import backend as K
    from keras.utils import np_utils
    from sklearn.model_selection import train_test_split 
</pre></td></tr></tbody></table></code></pre></figure>

<p><img src="" alt="" /></p>
<h5 id="step-3"><strong>Step 3</strong></h5>

<blockquote>
  <p><strong>Number plate detection:</strong></p>
</blockquote>

<p>Let’s start simple by importing a sample image of a car with a license plate and define some functions:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">extract_plate</span><span class="p">(</span><span class="n">img</span><span class="p">):</span> <span class="c1"># the function detects and perfors blurring on the number plate.
</span>	<span class="n">plate_img</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

	<span class="c1">#Loads the data required for detecting the license plates from cascade classifier.
</span>	<span class="n">plate_cascade</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">CascadeClassifier</span><span class="p">(</span><span class="sh">'</span><span class="s">./indian_license_plate.xml</span><span class="sh">'</span><span class="p">)</span>

	<span class="c1"># detects numberplates and returns the coordinates and dimensions of detected license plate's contours.
</span>	<span class="n">plate_rect</span> <span class="o">=</span> <span class="n">plate_cascade</span><span class="p">.</span><span class="nf">detectMultiScale</span><span class="p">(</span><span class="n">plate_img</span><span class="p">,</span> <span class="n">scaleFactor</span> <span class="o">=</span> <span class="mf">1.3</span><span class="p">,</span> <span class="n">minNeighbors</span> <span class="o">=</span> <span class="mi">7</span><span class="p">)</span>

	<span class="nf">for </span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span><span class="p">)</span> <span class="ow">in</span> <span class="n">plate_rect</span><span class="p">:</span>
		<span class="n">a</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.02</span><span class="o">*</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nf">int</span><span class="p">(</span><span class="mf">0.025</span><span class="o">*</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1">#parameter tuning
</span>		<span class="n">plate</span> <span class="o">=</span> <span class="n">plate_img</span><span class="p">[</span><span class="n">y</span><span class="o">+</span><span class="n">a</span><span class="p">:</span><span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="o">-</span><span class="n">a</span><span class="p">,</span> <span class="n">x</span><span class="o">+</span><span class="n">b</span><span class="p">:</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="o">-</span><span class="n">b</span><span class="p">,</span> <span class="p">:]</span>
		<span class="c1"># finally representing the detected contours by drawing rectangles around the edges.
</span>		<span class="n">cv2</span><span class="p">.</span><span class="nf">rectangle</span><span class="p">(</span><span class="n">plate_img</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">,</span> <span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">),</span> <span class="p">(</span><span class="mi">51</span><span class="p">,</span><span class="mi">51</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span>

	<span class="k">return</span> <span class="n">plate_img</span><span class="p">,</span> <span class="n">plate</span> <span class="c1"># returning the processed image
</span> 
</pre></td></tr></tbody></table></code></pre></figure>

<p>The above function works by taking image as input, then applying ‘haar cascade’ that is pre-trained to detect Indian license plates, here the parameter scaleFactor stands for a value by which input image can be scaled for better detection of license plate (<a href="https://sites.google.com/site/5kk73gpu2012/assignment/viola-jones-face-detection#TOC-Image-Pyramid">know more</a>). minNeighbors is just a parameter to reduce false positives, if this value is low, the algorithm may be more prone to giving a misrecognized outputs. (you can download the haar cascade file as ‘indian_license_plate.xml’ file from my <a href="https://github.com/SarthakV7/AI-based-indian-license-plate-detection">github</a> profile.)</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*RFqmJj0alAKWAqyBuihosw.jpeg" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*S8bTK6q1LUuChQ2Fet6yfQ.jpeg" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*w_LVI7pA6CehL0P5t6LhGQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<h5 id="step-4"><strong>Step 4</strong></h5>

<blockquote>
  <p><strong>Performing some image processing on the License plate.</strong></p>
</blockquote>

<p>Now let’s process this image further to make the character extraction process easy. We’ll start by defining some more functions for that.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td><td class="code"><pre><span class="c1"># Find characters in the resulting images
</span><span class="k">def</span> <span class="nf">segment_characters</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> <span class="p">:</span>

    <span class="c1"># Preprocess cropped license plate image
</span>    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="mi">333</span><span class="p">,</span> <span class="mi">75</span><span class="p">))</span>
    <span class="n">img_gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">img_binary</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">threshold</span><span class="p">(</span><span class="n">img_gray</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">THRESH_BINARY</span><span class="o">+</span><span class="n">cv2</span><span class="p">.</span><span class="n">THRESH_OTSU</span><span class="p">)</span>
    <span class="n">img_erode</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">erode</span><span class="p">(</span><span class="n">img_binary</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">img_dilate</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">dilate</span><span class="p">(</span><span class="n">img_erode</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

    <span class="n">LP_WIDTH</span> <span class="o">=</span> <span class="n">img_dilate</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">LP_HEIGHT</span> <span class="o">=</span> <span class="n">img_dilate</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Make borders white
</span>    <span class="n">img_dilate</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">,:]</span> <span class="o">=</span> <span class="mi">255</span>
    <span class="n">img_dilate</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">255</span>
    <span class="n">img_dilate</span><span class="p">[</span><span class="mi">72</span><span class="p">:</span><span class="mi">75</span><span class="p">,:]</span> <span class="o">=</span> <span class="mi">255</span>
    <span class="n">img_dilate</span><span class="p">[:,</span><span class="mi">330</span><span class="p">:</span><span class="mi">333</span><span class="p">]</span> <span class="o">=</span> <span class="mi">255</span>

    <span class="c1"># Estimations of character contours sizes of cropped license plates
</span>    <span class="n">dimensions</span> <span class="o">=</span> <span class="p">[</span><span class="n">LP_WIDTH</span><span class="o">/</span><span class="mi">6</span><span class="p">,</span> <span class="n">LP_WIDTH</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">LP_HEIGHT</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">LP_HEIGHT</span><span class="o">/</span><span class="mi">3</span><span class="p">]</span>

    <span class="c1"># Get contours within cropped license plate
</span>    <span class="n">char_list</span> <span class="o">=</span> <span class="nf">find_contours</span><span class="p">(</span><span class="n">dimensions</span><span class="p">,</span> <span class="n">img_dilate</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">char_list</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>The above function takes in the image as input and performs the following operation on it-</p>

<ul>
  <li>
    <p>resizes it to a dimension such that all characters seem distinct and clear</p>
  </li>
  <li>
    <p>convert the colored image to a grey scaled image i.e instead of 3 channels (BGR), the image only has a single 8-bit channel with values ranging from 0–255 where 0 corresponds to black and 255 corresponds to white. We do this to prepare the image for the next process.</p>
  </li>
  <li>
    <p>now the threshold function converts the grey scaled image to a binary image i.e each pixel will now have a value of 0 or 1 where 0 corresponds to black and 1 corresponds to white. It is done by applying a threshold that has a value between 0 and 255, here the value is 200 which means in the grayscaled image for pixels having a value above 200, in the new binary image that pixel will be given a value of 1. And for pixels having value below 200, in the new binary image that pixel will be given a value of 0.</p>
  </li>
  <li>
    <p>The image is now in binary form and ready for the next process Eroding.
Eroding is a simple process used for removing unwanted pixels from the object’s boundary meaning pixels that should have a value of 0 but are having a value of 1. It works by considering each pixel in the image one by one and then considering the pixel’s neighbor (the number of neighbors depends on the kernel size), the pixel is given a value 1 only if all its neighboring pixels are 1, otherwise it is given a value of 0.</p>
  </li>
  <li>
    <p>The image is now clean and free of boundary noise, we will now dilate the image to fill up the absent pixels meaning pixels that should have a value of 1 but are having value 0. The function works similar to eroding but with a little catch, it works by considering each pixel in the image one by one and then considering the pixel’s neighbor (the number of neighbors depends on the kernel size), the pixel is given a value 1 if at least one of its neighboring pixels is 1.</p>
  </li>
  <li>
    <p>The next step now is to make the boundaries of the image white. This is to remove any out of the frame pixel in case it is present.</p>
  </li>
  <li>
    <p>Next, we define a list of dimensions that contains 4 values with which we’ll be comparing the character’s dimensions for filtering out the required characters.</p>
  </li>
  <li>
    <p>Through the above processes, we have reduced our image to a processed binary image and we are ready to pass this image for character extraction.</p>
  </li>
</ul>

<p><img src="" alt="" /></p>
<h5 id="step-5"><strong>Step 5</strong></h5>

<blockquote>
  <p><strong>Segmenting the alphanumeric characters from the license plate.</strong></p>
</blockquote>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">cv2</span>

<span class="c1"># Match contours to license plate or character template
</span><span class="k">def</span> <span class="nf">find_contours</span><span class="p">(</span><span class="n">dimensions</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span> <span class="p">:</span>

    <span class="c1"># Find all contours in the image
</span>    <span class="n">cntrs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">findContours</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="nf">copy</span><span class="p">(),</span> <span class="n">cv2</span><span class="p">.</span><span class="n">RETR_TREE</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">CHAIN_APPROX_SIMPLE</span><span class="p">)</span>

    <span class="c1"># Retrieve potential dimensions
</span>    <span class="n">lower_width</span> <span class="o">=</span> <span class="n">dimensions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">upper_width</span> <span class="o">=</span> <span class="n">dimensions</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">lower_height</span> <span class="o">=</span> <span class="n">dimensions</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">upper_height</span> <span class="o">=</span> <span class="n">dimensions</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>


    <span class="c1"># Check largest 5 or  15 contours for license plate or character respectively
</span>    <span class="n">cntrs</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">cntrs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">cv2</span><span class="p">.</span><span class="n">contourArea</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">15</span><span class="p">]</span>

    <span class="n">x_cntr_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">target_contours</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">img_res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">cntr</span> <span class="ow">in</span> <span class="n">cntrs</span> <span class="p">:</span>
        <span class="c1">#detects contour in binary image and returns the coordinates of rectangle enclosing it
</span>        <span class="n">intX</span><span class="p">,</span> <span class="n">intY</span><span class="p">,</span> <span class="n">intWidth</span><span class="p">,</span> <span class="n">intHeight</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">boundingRect</span><span class="p">(</span><span class="n">cntr</span><span class="p">)</span>

        <span class="c1">#checking the dimensions of the contour to filter out the characters by contour's size
</span>        <span class="k">if</span> <span class="n">intWidth</span> <span class="o">&gt;</span> <span class="n">lower_width</span> <span class="ow">and</span> <span class="n">intWidth</span> <span class="o">&lt;</span> <span class="n">upper_width</span> <span class="ow">and</span> <span class="n">intHeight</span> <span class="o">&gt;</span> <span class="n">lower_height</span> <span class="ow">and</span> <span class="n">intHeight</span> <span class="o">&lt;</span> <span class="n">upper_height</span> <span class="p">:</span>
            <span class="n">x_cntr_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">intX</span><span class="p">)</span> <span class="c1">#stores the x coordinate of the character's contour, to used later for indexing the contours
</span>
            <span class="n">char_copy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">44</span><span class="p">,</span><span class="mi">24</span><span class="p">))</span>
            <span class="c1">#extracting each character using the enclosing rectangle's coordinates.
</span>            <span class="n">char</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="n">intY</span><span class="p">:</span><span class="n">intY</span><span class="o">+</span><span class="n">intHeight</span><span class="p">,</span> <span class="n">intX</span><span class="p">:</span><span class="n">intX</span><span class="o">+</span><span class="n">intWidth</span><span class="p">]</span>
            <span class="n">char</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="n">char</span><span class="p">,</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>

            <span class="c1"># Make result formatted for classification: invert colors
</span>            <span class="n">char</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">subtract</span><span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="n">char</span><span class="p">)</span>

            <span class="c1"># Resize the image to 24x44 with black border
</span>            <span class="n">char_copy</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">42</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">22</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
            <span class="n">char_copy</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">char_copy</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">char_copy</span><span class="p">[</span><span class="mi">42</span><span class="p">:</span><span class="mi">44</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">char_copy</span><span class="p">[:,</span> <span class="mi">22</span><span class="p">:</span><span class="mi">24</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">img_res</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">char_copy</span><span class="p">)</span> <span class="c1">#List that stores the character's binary image (unsorted)
</span>
    <span class="c1">#Return characters on ascending order with respect to the x-coordinate (most-left character first)
</span>
    <span class="c1">#arbitrary function that stores sorted list of character indeces
</span>    <span class="n">indices</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x_cntr_list</span><span class="p">)),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">k</span><span class="p">:</span> <span class="n">x_cntr_list</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
    <span class="n">img_res_copy</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
        <span class="n">img_res_copy</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">img_res</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span><span class="c1"># stores character images according to their index
</span>    <span class="n">img_res</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">img_res_copy</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">img_res</span>
<span class="n">view</span> <span class="n">rawcharacter_detection</span><span class="p">.</span><span class="n">py</span> <span class="n">hosted</span> <span class="k">with</span> <span class="err">❤</span> <span class="n">by</span> <span class="n">GitHub</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>After step 4 we should have a clean binary image to work on. In this step, we will be applying some more image processing to extract the individual characters from the license plate. The steps involved will be-</p>

<ul>
  <li>Finding all the contours in the input image. The function cv2.findContours returns all the contours it finds in the image. Contours can be explained simply as a curve joining all the continuous points (along the boundary), having the same color or intensity.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*I1-aZ-szf-SqrxueEtB-Tg.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*8blZEtiXo9vxriC3yp1EDA.png" />
    </div>
</div>

<ul>
  <li>After finding all the contours we consider them one by one and calculate the dimension of their respective bounding rectangle. Now consider bounding rectangle is the smallest rectangle possible that contains the contour. Let me illustrate the bounding rectangle by drawing them for each character here.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*0l5qoklROE2bdIq4JkXfuA.png" />
    </div>
</div>

<ul>
  <li>Since we have the dimensions of these bounding rectangle, all we need to do is do some parameter tuning and filter out the required rectangle containing required characters. For this, we will be performing some dimension comparison by accepting only those rectangle that have:
    <ol>
      <li>Width in the range  0, (length of the pic)/(number of characters) and,</li>
      <li>Length in a range of (width of the pic)/2, 4*(width of the pic)/5.
After this step, we should have all the characters extracted as binary images.</li>
    </ol>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*PVdInfmQoYIxyYyp81Kx1w.png" />
    </div>
</div>

<ul>
  <li>The characters may be unsorted but don’t worry, the last few lines of the code take care of that. It sorts the character according to the position of their bounding rectangle from the left boundary of the plate.</li>
</ul>

<p><img src="" alt="" /></p>
<h5 id="step-6"><strong>Step 6</strong></h5>

<blockquote>
  <p><strong>Creating a Machine Learning model and training it for the characters.</strong></p>
</blockquote>

<ul>
  <li>The data is all clean and ready, now it’s time do create a Neural Network that will be intelligent enough to recognize the characters after training.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2048/1*KhUiEJdZy42JfkCfwm7jjg.jpeg" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>For modeling, we will be using a Convolutional Neural Network with 3 layers.</li>
</ul>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre><span class="c"># create model</span>
model <span class="o">=</span> Sequential<span class="o">()</span>
model.add<span class="o">(</span>Conv2D<span class="o">(</span><span class="nv">filters</span><span class="o">=</span>32, <span class="nv">kernel_size</span><span class="o">=(</span>5,5<span class="o">)</span>, <span class="nv">input_shape</span><span class="o">=(</span>28, 28, 1<span class="o">)</span>, <span class="nv">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="o">))</span>
model.add<span class="o">(</span>MaxPooling2D<span class="o">(</span><span class="nv">pool_size</span><span class="o">=(</span>2, 2<span class="o">)))</span>
model.add<span class="o">(</span>Dropout<span class="o">(</span><span class="nv">rate</span><span class="o">=</span>0.4<span class="o">))</span>
model.add<span class="o">(</span>Flatten<span class="o">())</span>
model.add<span class="o">(</span>Dense<span class="o">(</span><span class="nv">units</span><span class="o">=</span>128, <span class="nv">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="o">))</span>
model.add<span class="o">(</span>Dense<span class="o">(</span><span class="nv">units</span><span class="o">=</span>36, <span class="nv">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="o">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2248/1*FAMyA1skMbdXYQlmUM6Kfw.png" />
    </div>
</div>

<ul>
  <li>
    <p>To keep the model simple, we’ll start by creating a sequential object.</p>
  </li>
  <li>
    <p>The first layer will be a convolutional layer with 32 output filters, a convolution window of size (5,5), and ‘Relu’ as activation function.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*njuH4XVXf-l9pR_RorUOrA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*iUxZ6ZNaizs2DzhDvTWDgg.png" />
    </div>
</div>

<ul>
  <li>Next, we’ll be adding a max-pooling layer with a window size of (2,2).
<strong>Max pooling</strong> is a sample-based discretization process. The objective is to down-sample an input representation (image, hidden-<strong>layer</strong> output matrix, etc.), reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/10140/1*cEpnL1pqYe45cBZIfOxASw.png" />
    </div>
</div>

<ul>
  <li>
    <p>Now, we will be adding some dropout rate to take care of overfitting.
<strong>Dropout</strong> is a regularization hyperparameter initialized to prevent Neural Networks from Overfitting. Dropout is a technique where randomly selected neurons are ignored during training. They are “<strong>dropped</strong>-<strong>out</strong>” randomly. We have chosen a dropout rate of 0.4 meaning 60% of the node will be retained.</p>
  </li>
  <li>
    <p>Now it’s time to flatten the node data so we add a flatten layer for that. The flatten layer takes data from the previous layer and represents it in a single dimension.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*BLP5zEDWc6kwBpThM5jFjQ.png" />
    </div>
</div>

<ul>
  <li>Finally, we will be adding 2 dense layers, one with the dimensionality of the output space as 128, activation function=’ReLU’ and other, our final layer with 36 outputs for categorizing the 26 alphabets (A-Z) + 10 digits (0–9) and activation function= ‘softmax’</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*_uXiq8n5QvQzlJLNjZRLSg.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<h5 id="step-7"><strong>Step 7</strong></h5>

<blockquote>
  <p><strong>Training our CNN model.</strong></p>
</blockquote>

<ul>
  <li>
    <p>The data we will be using contains images of alphabets (A-Z) and digits (0–9) of size 28x28, also the data is balanced so we won’t have to do any kind of data tuning here.</p>
  </li>
  <li>
    <p>I’ve created a <a href="https://github.com/SarthakV7/AI-based-indian-license-plate-detection/blob/master/data.zip">zip file</a> that contains data as per the directory structure below, with a train test split of 80:20</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*uXcMknHKArBw3f5J.jpeg" />
    </div>
</div>

<ul>
  <li>
    <p>We’ll be using ImageDataGenerator class available in keras to generate some more data using image augmentation techniques like width shift, height shift. To know more about ImageDataGenerator, please check out <a href="https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720">this</a> nice blog.</p>
  </li>
  <li>
    <p>Width shift: Accepts a float value denoting by what fraction the image will be shifted left and right.
Height shift: Accepts a float value denoting by what fraction the image will be shifted up and down.</p>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre> <span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">ImageDataGenerator</span>
<span class="n">train_datagen</span> <span class="o">=</span> <span class="nc">ImageDataGenerator</span><span class="p">(</span><span class="n">rescale</span><span class="o">=</span><span class="mf">1.</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span> <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="n">train_generator</span> <span class="o">=</span> <span class="n">train_datagen</span><span class="p">.</span><span class="nf">flow_from_directory</span><span class="p">(</span>
        <span class="sh">'</span><span class="s">data/train</span><span class="sh">'</span><span class="p">,</span>  <span class="c1"># this is the target directory
</span>        <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">),</span>  <span class="c1"># all images will be resized to 28x28
</span>        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">class_mode</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical</span><span class="sh">'</span><span class="p">)</span>

<span class="n">validation_generator</span> <span class="o">=</span> <span class="n">train_datagen</span><span class="p">.</span><span class="nf">flow_from_directory</span><span class="p">(</span>
        <span class="sh">'</span><span class="s">data/val</span><span class="sh">'</span><span class="p">,</span>  <span class="c1"># this is the target directory
</span>        <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">),</span>  <span class="c1"># all images will be resized to 28x28
</span>        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">class_mode</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical</span><span class="sh">'</span><span class="p">)</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<ul>
  <li>It’s time to train our model now!
we will use ‘categorical_crossentropy’ as loss function, ‘Adam’ as optimization function and ‘Accuracy’ as our error matrix.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="code"><pre> <span class="kn">import</span> <span class="n">datetime</span>
<span class="k">class</span> <span class="nc">stop_training_callback</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">Callback</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="p">{}):</span>
    <span class="nf">if</span><span class="p">(</span><span class="n">logs</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">val_acc</span><span class="sh">'</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.992</span><span class="p">):</span>
      <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">stop_training</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">log_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">logs/fit/</span><span class="sh">"</span> <span class="o">+</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y%m%d-%H%M%S</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tensorboard_callback</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="nc">TensorBoard</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">histogram_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensorboard_callback</span><span class="p">,</span> <span class="nf">stop_training_callback</span><span class="p">()]</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit_generator</span><span class="p">(</span><span class="n">train_generator</span><span class="p">,</span>
      <span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="n">train_generator</span><span class="p">.</span><span class="n">samples</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">,</span>
      <span class="n">validation_data</span> <span class="o">=</span> <span class="n">validation_generator</span><span class="p">,</span>
      <span class="n">validation_steps</span> <span class="o">=</span> <span class="n">validation_generator</span><span class="p">.</span><span class="n">samples</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">,</span>
      <span class="n">epochs</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<ul>
  <li>After training for 23 epochs, the model achieved an accuracy of 99.54%.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4332/1*HAA_UPvZRihw3i17aYLvVA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2692/1*o9wFBmX69Nm44RbMngw6dA.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<h5 id="step-8"><strong>Step 8</strong></h5>

<blockquote>
  <p><strong>The output.</strong></p>
</blockquote>

<p>Finally, its time to test our model, remember the binary images of extracted characters from number plate? Let’s feed the images to our model!</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3540/1*esXDwU6Brah9mL42BY0M-A.png" />
    </div>
</div>

<p>The output-</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*9OWCIVp8wZvYbgajn_mC-w.png" />
    </div>
</div>

<p><img src="" alt="" /></p>

<blockquote>
  <p><strong>Final comment</strong></p>
</blockquote>

<p>Thank you guys for reading the blog, hope this project is useful for some of you aspiring to do projects on OCR, image processing, Machine Learning, IoT.</p>

<p>And if you have any doubts regarding this project, please leave a comment in the response section.</p>

<p>The full project is available on my Github:
<a href="https://github.com/SarthakV7/AI-based-indian-license-plate-detection">https://github.com/SarthakV7/AI-based-indian-license-plate-detection</a></p>

<p>Find me on LinkedIn: <a href="http://www.linkedin.com/in/sarthak-vajpayee">www.linkedin.com/in/sarthak-vajpayee</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[The guy who hit my car and got away with it!]]></summary></entry></feed>